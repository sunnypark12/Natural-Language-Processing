{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqxpid8J3_xt"
      },
      "source": [
        "# Part of Speech Tagging with LSTM, Fine-tuned BERT\n",
        "**CS 4650 \"Natural Language Processing\" Project 2**  \n",
        "Georgia Tech, Fall 2024 (Instructor: Alan Ritter)\n",
        "\n",
        "**To start, first make a copy of this notebook to your local drive, so you can edit it.**\n",
        "\n",
        "If you want GPUs (which will improve training speed), you can always change your instance type to GPU by going to Runtime -> Change runtime type -> Hardware accelerator.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Basic POS Tagger  [15 points]\n",
        "\n",
        "In this assignment, we will train LSTM-based POS-taggers, and evaluate their performance. We will use English text from the Wall Street Journal, marked with POS tags such as `NNP` (proper noun) and `DT` (determiner)."
      ],
      "metadata": {
        "id": "S_DVctvT4zlV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X367eCR3_x0"
      },
      "source": [
        "### 1.1 Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -so train.txt https://dl.dropboxusercontent.com/scl/fi/ixd4izjvd3e3zdijj8p08/train.txt?rlkey=nglgb600e17tg8n0o20ufjve4"
      ],
      "metadata": {
        "id": "2hVu_ia9Ottg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtnGNDoA3_x3"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Run some setup code for this notebook. Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# ===========================================================================\n",
        "# A quick note on CUDA functionality (and `.to(model.device)`):\n",
        "# CUDA is a parallel GPU platform produced by NVIDIA and is used by most GPU\n",
        "# libraries in PyTorch. CUDA organizes GPUs into device IDs (i.e., \"cuda:X\" for GPU #X).\n",
        "# \"device\" will tell PyTorch which GPU (or CPU) to place an object in. Since\n",
        "# collab only uses one GPU, we will use 'cuda' as the device if a GPU is available\n",
        "# and the CPU if not. You will run into problems if your tensors are on different devices.\n",
        "# ===========================================================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check to make sure a GPU is available using the following code block.\n",
        "\n",
        "\n",
        "```py\n",
        "# If the below message is shown, it means you are using a CPU.\n",
        "/bin/bash: nvidia-smi: command not found\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xuEywStkc3M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "ounnp0ASc58O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42fee25f-19a6-4a41-d24e-7c197e6536fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct 30 00:27:37 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwA2y6OR3_yE"
      },
      "source": [
        "### 1.2 Preparing Data\n",
        "\n",
        "`train.txt`: The training data is present in this file. This file contains sequences of words and their respective tags. The data is split into 80% training and 20% development to train the model and tune the hyperparameters, respectively. See `load_tag_data` for details on how to read the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFpH2P1A3_yG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f640e61-a116-495f-c7f2-615e4ffd6aea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data:  7148\n",
            "Val Data:  1788\n",
            "Total tags:  44\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Run some preprocessing code for our dataset. Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "def load_tag_data(tag_file):\n",
        "    all_sentences = []\n",
        "    all_tags = []\n",
        "    sent = []\n",
        "    tags = []\n",
        "    with open(tag_file, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.strip() == \"\":\n",
        "                all_sentences.append(sent)\n",
        "                all_tags.append(tags)\n",
        "                sent = []\n",
        "                tags = []\n",
        "            else:\n",
        "                word, tag, _ = line.strip().split()\n",
        "                sent.append(word)\n",
        "                tags.append(tag)\n",
        "    return all_sentences, all_tags\n",
        "\n",
        "train_sentences, train_tags = load_tag_data('train.txt')\n",
        "\n",
        "unique_tags = set([tag for tag_seq in train_tags for tag in tag_seq])\n",
        "\n",
        "# Create train-val split from train data\n",
        "train_val_data = list(zip(train_sentences, train_tags))\n",
        "random.shuffle(train_val_data)\n",
        "split = int(0.8 * len(train_val_data))\n",
        "training_data = train_val_data[:split]\n",
        "val_data = train_val_data[split:]\n",
        "\n",
        "print(\"Train Data: \", len(training_data))\n",
        "print(\"Val Data: \", len(val_data))\n",
        "print(\"Total tags: \", len(unique_tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlfliN0J-RzV"
      },
      "source": [
        "### 1.3 Word-to-Index and Tag-to-Index mapping\n",
        "In order to work with text in Tensor format, we need to map each word to an index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uojEDun83_yP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb851e0-027a-4264-af57-97158680121e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tags 44\n",
            "Vocab size 19122\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "word_to_idx = {}\n",
        "for sent in train_sentences:\n",
        "    for word in sent:\n",
        "        if word not in word_to_idx:\n",
        "            word_to_idx[word] = len(word_to_idx)\n",
        "\n",
        "tag_to_idx = {}\n",
        "for tag in unique_tags:\n",
        "    if tag not in tag_to_idx:\n",
        "        tag_to_idx[tag] = len(tag_to_idx)\n",
        "\n",
        "idx_to_tag = {}\n",
        "for tag in tag_to_idx:\n",
        "    idx_to_tag[tag_to_idx[tag]] = tag\n",
        "\n",
        "print(\"Total tags\", len(tag_to_idx))\n",
        "print(\"Vocab size\", len(word_to_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H26dqorp3_yX"
      },
      "outputs": [],
      "source": [
        "def prepare_sequence(sent, idx_mapping):\n",
        "    idxs = [idx_mapping[word] for word in sent]\n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRnBTCwD3_yc"
      },
      "source": [
        "### 1.4 Set up model\n",
        "We will build and train a Basic POS Tagger which is an LSTM model to tag the parts of speech in a given sentence. Here we define a few default hyperparameters for your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P5SHabu3_yf"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 4\n",
        "HIDDEN_DIM = 8\n",
        "LEARNING_RATE = 0.1\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkkS4oEb3_yk"
      },
      "source": [
        "### 1.5 Define Model [5 points]\n",
        "\n",
        "The model takes as input a sentence as a tensor in the index space. This sentence is then converted to embedding space where each word maps to its word embedding. The word embeddings is learned as part of the model training process. These word embeddings act as input to the LSTM which produces a representation for each word. Then the representations of words are passed to a Linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCa30HQb3_ym"
      },
      "outputs": [],
      "source": [
        "class BasicPOSTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        \"\"\"\n",
        "        Define and initialize anything needed for the forward pass.\n",
        "\n",
        "        You are required to create a model with:\n",
        "          an embedding layer: that maps words to the embedding space\n",
        "          an LSTM layer: that takes word embeddings as input and outputs hidden states\n",
        "          a linear layer: maps from hidden state space to tag space\n",
        "        \"\"\"\n",
        "        super(BasicPOSTagger, self).__init__()\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        \"\"\"\n",
        "        Implement the forward pass.\n",
        "\n",
        "        Given a tokenized index-mapped sentence as the argument,\n",
        "        compute the corresponding raw scores for tags (without softmax)\n",
        "\n",
        "        returns:: tag_scores (Tensor)\n",
        "        \"\"\"\n",
        "        tag_scores = None\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "        embeddings = self.embedding(sentence)\n",
        "        lstm_out, _ = self.lstm(embeddings.view(len(sentence), 1, -1))\n",
        "        lstm_out = lstm_out.view(len(sentence), -1)\n",
        "        tag_scores = self.hidden2tag(lstm_out)\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot9J3MrB3_ys"
      },
      "source": [
        "### 1.6 Training [5 points]\n",
        "\n",
        "We define train and evaluate procedures that allow us to train our model using our created train-val split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWMGxh4Z3_yv"
      },
      "outputs": [],
      "source": [
        "def train(epoch, model, loss_function, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_examples = 0\n",
        "    for sentence, tags in training_data:\n",
        "        \"\"\"\n",
        "        Implement the training method\n",
        "\n",
        "        Hint: you can use the prepare_sequence method for creating index mappings\n",
        "        for sentences. Find the gradient with respect to the loss and update the\n",
        "        model parameters using the optimizer.\n",
        "        \"\"\"\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "        # Zero out the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Prepare input data (sentences and gold labels)\n",
        "        input_sentence = prepare_sequence(sentence, word_to_idx)\n",
        "        gold_labels = prepare_sequence(tags, tag_to_idx)\n",
        "\n",
        "        # Do forward pass with current batch of input\n",
        "        tag_scores = model(input_sentence)\n",
        "\n",
        "        # Get loss with model predictions and true labels\n",
        "        loss = loss_function(tag_scores, gold_labels)\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Increase running total loss and the number of past training samples\n",
        "        train_loss += loss.item()\n",
        "        train_examples += len(sentence)\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "    avg_train_loss = train_loss / train_examples\n",
        "    avg_val_loss, val_accuracy = evaluate(model, loss_function)\n",
        "\n",
        "    print(f\"Epoch: {epoch}/{EPOCHS}\\tAvg Train Loss: {avg_train_loss:.4f}\\tAvg Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.0f}\")\n",
        "\n",
        "def evaluate(model, loss_function):\n",
        "    \"\"\"\n",
        "    returns:: avg_val_loss (float)\n",
        "    returns:: val_accuracy (float)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    val_loss = 0\n",
        "    val_examples = 0\n",
        "    with torch.no_grad():\n",
        "        for sentence, tags in val_data:\n",
        "            \"\"\"\n",
        "            Implement the evaluate method\n",
        "\n",
        "            Find the average validation loss along with the validation accuracy.\n",
        "            Hint: To find the accuracy, argmax of tag predictions can be used.s\n",
        "            \"\"\"\n",
        "            ### BEGIN YOUR CODE ###\n",
        "\n",
        "            # Prepare input data (sentences and gold labels)\n",
        "            input_sentence = prepare_sequence(sentence, word_to_idx)\n",
        "            gold_labels = prepare_sequence(tags, tag_to_idx)\n",
        "\n",
        "            # Do forward pass with current batch of input\n",
        "            tag_scores = model(input_sentence)\n",
        "\n",
        "            # Get loss with model predictions and true labels\n",
        "            loss = loss_function(tag_scores, gold_labels)\n",
        "\n",
        "            # Get the predicted labels\n",
        "            _, predicted = torch.max(tag_scores, 1)\n",
        "\n",
        "            # Get number of correct prediction\n",
        "            correct += (predicted == gold_labels).sum().item()\n",
        "\n",
        "            # Increase running total loss and the number of past valid samples\n",
        "            val_loss += loss.item()\n",
        "            val_examples += len(sentence)\n",
        "\n",
        "            ### END YOUR CODE ###\n",
        "    val_accuracy = 100. * correct / val_examples\n",
        "    avg_val_loss = val_loss / val_examples\n",
        "    return avg_val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsuHjjH1rQeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad1e8fdc-4dda-4309-c17b-06b5d281bedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\tAvg Train Loss: 1.3198\tAvg Val Loss: 1.0410\t Val Accuracy: 70\n",
            "Epoch: 2/10\tAvg Train Loss: 0.9397\tAvg Val Loss: 0.9255\t Val Accuracy: 74\n",
            "Epoch: 3/10\tAvg Train Loss: 0.7999\tAvg Val Loss: 0.8361\t Val Accuracy: 77\n",
            "Epoch: 4/10\tAvg Train Loss: 0.7733\tAvg Val Loss: 0.8510\t Val Accuracy: 76\n",
            "Epoch: 5/10\tAvg Train Loss: 0.7681\tAvg Val Loss: 0.8557\t Val Accuracy: 77\n",
            "Epoch: 6/10\tAvg Train Loss: 0.7551\tAvg Val Loss: 0.8870\t Val Accuracy: 75\n",
            "Epoch: 7/10\tAvg Train Loss: 0.7788\tAvg Val Loss: 0.9052\t Val Accuracy: 75\n",
            "Epoch: 8/10\tAvg Train Loss: 0.7987\tAvg Val Loss: 0.9024\t Val Accuracy: 75\n",
            "Epoch: 9/10\tAvg Train Loss: 0.7330\tAvg Val Loss: 0.9033\t Val Accuracy: 75\n",
            "Epoch: 10/10\tAvg Train Loss: 0.8148\tAvg Val Loss: 0.9699\t Val Accuracy: 75\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Initialize the model, optimizer and the loss function\n",
        "\"\"\"\n",
        "### BEGIN YOUR CODE ###\n",
        "model = BasicPOSTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_idx), len(tag_to_idx))\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
        "### END YOUR CODE ###\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(epoch, model, loss_function, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK6mT_k8NRvB"
      },
      "source": [
        "*Hint: Under the default hyperparameter setting, after 5 epochs you should be able to get at least `0.75` accuracy on the validation set.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP64WDReBuDr"
      },
      "source": [
        "### 1.7 Error analysis [5 points]\n",
        "\n",
        "In this step, we will analyze what kind of errors it was making on the validation set.\n",
        "\n",
        "Step 1, write a method to generate predictions from the validation set. For every sentence, get its words, predicted tags (model_tags), and the ground truth tags (gt_tags). To make the next step easier, you may want to concatenate words from all sentences into a very long list, and same for model_tags and gt_tags.\n",
        "\n",
        "\n",
        "Step 2, analyze what kind of errors the model was making. For example, it may frequently label NN as VB. Let's get the top-10 most frequent types of errors, each of their frequency, and some example words. One example is at below. It is interpreted as the model predicts NNP as VBG for 626 times, five random example words are shown.\n",
        "\n",
        "```\n",
        "['VBG', 'NNP', 626, ['Rowe', 'Livermore', 'Parker', 'F-16', 'HEYNOW']]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QgMHr7HCn1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a9a1d93-225f-4e59-ed6d-351c2d695162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('NNS', 'JJ', 1032, ['labor-management', 'few', 'bleak', 'last', 'longstanding'])\n",
            "('NNS', 'NN', 621, ['tax', 'loophole', 'notion', 'offering', 'opposition'])\n",
            "('IN', 'RB', 476, ['only', 'then', 'there', 'probably', 'then'])\n",
            "('NNS', 'NNP', 444, ['Cleveland', 'Hoechst', 'Advanced', 'Fisher', 'German'])\n",
            "('VBN', 'VBD', 394, ['added', 'thought', 'watched', 'felt', 'provided'])\n",
            "('IN', 'VBD', 341, ['brought', 'had', 'closed', 'did', 'met'])\n",
            "('NNP', 'JJ', 266, ['imminent', 'practical', 'troubled', 'Northern', 'flamboyant'])\n",
            "('NNS', 'VBZ', 264, ['manufactures', 'does', 'markets', 'makes', 'shares'])\n",
            "('PRP', 'CC', 260, ['or', '&', 'or', 'but', 'or'])\n",
            "('NN', 'JJ', 221, ['far-flung', 'secondary', 'House-passed', 'youthful', 'average'])\n"
          ]
        }
      ],
      "source": [
        "def generate_predictions(model, val_data):\n",
        "    \"\"\"\n",
        "    Generate predictions for val_data\n",
        "\n",
        "    Create lists of words, tags predicted by the model and ground truth tags.\n",
        "    Hint: It should look very similar to the evaluate function.\n",
        "\n",
        "    returns:: word_list (str list)\n",
        "    returns:: model_tags (str list)\n",
        "    returns:: gt_tags (str list)\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE ###\n",
        "    word_list = []\n",
        "    model_tags = []\n",
        "    gt_tags = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for sentence, tags in val_data:\n",
        "            input_sentence = prepare_sequence(sentence, word_to_idx)\n",
        "            gold_labels = prepare_sequence(tags, tag_to_idx)\n",
        "\n",
        "            tag_scores = model(input_sentence)\n",
        "            pred_idx = torch.argmax(tag_scores, dim=1)\n",
        "\n",
        "            pred_tags = [idx_to_tag[idx.item()] for idx in pred_idx]\n",
        "            true_tags = tags\n",
        "\n",
        "            word_list.extend(sentence)\n",
        "            model_tags.extend(pred_tags)\n",
        "            gt_tags.extend(true_tags)\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return word_list, model_tags, gt_tags\n",
        "\n",
        "def error_analysis(word_list, model_tags, gt_tags):\n",
        "    \"\"\"\"\n",
        "    Carry out error analysis\n",
        "\n",
        "    From those lists collected from the above method, find the\n",
        "    top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
        "    sorted by frequency\n",
        "\n",
        "    returns: errors (list of tuples)\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE ###\n",
        "    from collections import defaultdict, Counter\n",
        "\n",
        "    error_dict = defaultdict(lambda: {'count': 0, 'words': []})\n",
        "\n",
        "    for word, pred_tag, true_tag in zip(word_list, model_tags, gt_tags):\n",
        "        if pred_tag != true_tag:\n",
        "            key = (pred_tag, true_tag)\n",
        "            error_dict[key]['count'] += 1\n",
        "            if len(error_dict[key]['words']) < 5:\n",
        "                error_dict[key]['words'].append(word)\n",
        "\n",
        "    errors = [ (model_tag, true_tag, info['count'], info['words'])\n",
        "               for (model_tag, true_tag), info in error_dict.items() ]\n",
        "    errors.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return errors\n",
        "\n",
        "word_list, model_tags, gt_tags = generate_predictions(model, val_data)\n",
        "errors = error_analysis(word_list, model_tags, gt_tags)\n",
        "\n",
        "for i in errors[:10]:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRNjFRDcD2h7"
      },
      "source": [
        "**Report your findings here.**  \n",
        "What kinds of errors did the model make and why do you think it made them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLcI9BUZIo04"
      },
      "source": [
        "## 2. Hyper-parameter Tuning [10 points]\n",
        "\n",
        "In order to improve your model performance, try making some modifications on `EMBEDDING_DIM`, `HIDDEN_DIM`, and `LEARNING_RATE`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RekmpLxzIo04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f34184f-c157-48c7-b7f2-aa46308e18b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\tAvg Train Loss: 0.7689\tAvg Val Loss: 0.4666\t Val Accuracy: 87\n",
            "Epoch: 2/10\tAvg Train Loss: 0.3454\tAvg Val Loss: 0.3794\t Val Accuracy: 89\n",
            "Epoch: 3/10\tAvg Train Loss: 0.2306\tAvg Val Loss: 0.3543\t Val Accuracy: 90\n",
            "Epoch: 4/10\tAvg Train Loss: 0.1588\tAvg Val Loss: 0.3544\t Val Accuracy: 91\n",
            "Epoch: 5/10\tAvg Train Loss: 0.1067\tAvg Val Loss: 0.3681\t Val Accuracy: 91\n",
            "Epoch: 6/10\tAvg Train Loss: 0.0697\tAvg Val Loss: 0.3797\t Val Accuracy: 91\n",
            "Epoch: 7/10\tAvg Train Loss: 0.0463\tAvg Val Loss: 0.3936\t Val Accuracy: 91\n",
            "Epoch: 8/10\tAvg Train Loss: 0.0317\tAvg Val Loss: 0.4077\t Val Accuracy: 91\n",
            "Epoch: 9/10\tAvg Train Loss: 0.0231\tAvg Val Loss: 0.4209\t Val Accuracy: 91\n",
            "Epoch: 10/10\tAvg Train Loss: 0.0175\tAvg Val Loss: 0.4324\t Val Accuracy: 91\n"
          ]
        }
      ],
      "source": [
        "YOUR_EMBEDDING_DIM = 128\n",
        "YOUR_HIDDEN_DIM = 128\n",
        "YOUR_LEARNING_RATE = 0.01\n",
        "\n",
        "# Set three hyper-parameters. Initialize the model, optimizer and the loss function\n",
        "# Hint, you may want to use reduction='sum' in the CrossEntropyLoss function\n",
        "\n",
        "### BEGIN YOUR CODE ###\n",
        "EMBEDDING_DIM = YOUR_EMBEDDING_DIM\n",
        "HIDDEN_DIM = YOUR_HIDDEN_DIM\n",
        "LEARNING_RATE = YOUR_LEARNING_RATE\n",
        "\n",
        "model = BasicPOSTagger(\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    vocab_size=len(word_to_idx),\n",
        "    tagset_size=len(tag_to_idx)\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "### END YOUR CODE ###\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(epoch, model, loss_function, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svXyUssdXZ4r"
      },
      "source": [
        "## 3. Character-level POS Tagger  [15 points]\n",
        "\n",
        "Use the character-level information to augment word embeddings. For example, words that end with -ing or -ly give quite a bit of information about their POS tags. To incorporate this information, run a character-level LSTM on every word to create a character-level representation of the word. Take the last hidden state from the character-level LSTM as the representation and concatenate with the word embedding (as in the `BasicPOSTagger`) to create a new word representation that captures more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX4-3AoxSJeY"
      },
      "outputs": [],
      "source": [
        "# Create char to index mapping\n",
        "char_to_idx = {}\n",
        "unique_chars = set()\n",
        "MAX_WORD_LEN = 0\n",
        "\n",
        "for sent in train_sentences:\n",
        "    for word in sent:\n",
        "        for c in word:\n",
        "            unique_chars.add(c)\n",
        "        if len(word) > MAX_WORD_LEN:\n",
        "            MAX_WORD_LEN = len(word)\n",
        "\n",
        "for c in unique_chars:\n",
        "    char_to_idx[c] = len(char_to_idx)\n",
        "char_to_idx[' '] = len(char_to_idx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An Aside on Padding\n",
        "\n",
        "#### How to do padding correctly for the characters?\n",
        "\n",
        "\n",
        "Assume we have got a sentence [\"We\", \"love\", \"NLP\"]. You are supposed to first prepend a certain number of blank characters to each of the words in this sentence.\n",
        "\n",
        "How to determine the number of blank characters we need? The calculation of MAX_WORD_LEN is here for help (which we already provide in the starter code). For the given sentence, MAX_WORD_LEN equals 4. Therefore we prepend two blank characters to \"We\", zero blank character to \"love\", and one blank character to \"NLP\". So the resultant padded sentence we get should be [\"  We\", \"love\", \" NLP\"].\n",
        "\n",
        "Then, we feed all characters in [\"  We\", \"love\", \" NLP\"] into a char-embedding layer, and get a tensor of shape (3, 4, char_embedding_dim). To make this tensor's shape proper for the char-level LSTM (nn.LSTM), we need to transpose this tensor, i.e. swap the first and the second dimension. So we get a tensor of shape (4, 3, char_embedding_dim), where 4 corresponds to seq_len and 3 corresponds to batch_size.\n",
        "\n",
        "The last thing you need to do is to obtain the last hidden state from the char-level LSTM, and concatenate it with the word embedding, so that you can get an augmented representation of that word.\n",
        "\n",
        "![padding](https://raw.githubusercontent.com/chaojiang06/chaojiang06.github.io/master/TA/spring2022_CS4650/char_padding.png)\n",
        "  *An illustration for left padding characters*\n",
        "\n",
        "#### Why doing the padding?\n",
        "Someone may ask why we want to do such a kind of padding, instead of directly passing each of the character sequences of each word one by one through an LSTM, to get the last hidden state. The reason is that if you don't do padding, then that means you can only implement this process using \"for loop\". For CharPOSTagger, if you implement it using \"for loop\", the training time would be approximately 150s (GPU) / 250s (CPU) per epoch, while it would be around 30s (GPU) / 150s (CPU) per epoch if you do the padding and feed your data in batches. Therefore, we strongly recommend you learn how to do the padding and transform your data into batches. In fact, those are quite important concepts which you should get yourself familar with, although it might take you some time.\n",
        "\n",
        "#### Why doing *left* padding?\n",
        "Our hypothesis is that the suffixes of English words (e.g., -ly, -ing, etc) are more indicative than prefixes for the part-of-speech (POS). Though LSTM is supposed to be able to handle long sequences, it still lose information along the way and the information closer to the last state (which you use as char-level representations) will be retained better.\n",
        "\n",
        "#### How to understand the dimention change?\n",
        "Assume we have got a sentence with 3 words [\"We\", \"love\", \"NLP\"], and assume the dimension of character embedding is 2, the dimension of word embedding is 4, the dimension of word-level LSTM's hidden layer is 5, the dimension of character-level LSTM's hidden layer is 6.\n",
        "\n",
        "In `BasicPOSTagger`, the dimension change would be:\n",
        "\n",
        "- ------ input ------> $(3\\times 1\\times 4)$\n",
        "- -- word-level LSTM --> $(3\\times 1\\times 5)$\n",
        "- ----- linear layer -----> $(3\\times 1\\times 44)$\n",
        "\n",
        "In `CharPOSTagger`, after padding, character embedding, and swapping, the dimension change would be:\n",
        "\n",
        "- ------ input ------> $(\\text{MAX_WORD_LEN}\\times 3\\times 2)$\n",
        "-  -- character-level LSTM --> $(\\text{MAX_WORD_LEN}\\times 3\\times 6)$\n",
        "- -- Take the last hidden state --> $(3\\times 6)$\n",
        "- -- concatenate with word embedings --> $(3\\times 1\\times 10)$\n",
        "- -- word-level LSTM --> $(3\\times 1\\times 5)$\n",
        "- -- linear layer --> $(3\\times 1\\times 44)$."
      ],
      "metadata": {
        "id": "8xXPsL3nDjAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 4\n",
        "HIDDEN_DIM = 8\n",
        "LEARNING_RATE = 0.1\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0\n",
        "EPOCHS = 10\n",
        "CHAR_EMBEDDING_DIM = 4\n",
        "CHAR_HIDDEN_DIM = 4"
      ],
      "metadata": {
        "id": "OMsoXAMDO9-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Define Model [5 points]"
      ],
      "metadata": {
        "id": "D8q2EmxmCNfn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U0wb4OeOsde"
      },
      "outputs": [],
      "source": [
        "class CharPOSTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, char_embedding_dim,\n",
        "                 char_hidden_dim, char_size, vocab_size, tagset_size):\n",
        "        \"\"\"\n",
        "        Define and initialize anything needed for the forward pass.\n",
        "\n",
        "        You are required to create a model with:\n",
        "          an embedding layer for word: that maps words to their embedding space\n",
        "          an embedding layer for character: that maps characters to their embedding space\n",
        "          a character-level LSTM layer: that finds the character-level embedding for a word\n",
        "          a word-level LSTM layer: that takes the concatenated representation per word (word embedding + char-lstm) as input and outputs hidden states\n",
        "          a linear layer: maps from hidden state space to tag space\n",
        "        \"\"\"\n",
        "        super(CharPOSTagger, self).__init__()\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.char_embedding = nn.Embedding(char_size, char_embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.char_lstm = nn.LSTM(\n",
        "            input_size=char_embedding_dim,\n",
        "            hidden_size=char_hidden_dim,\n",
        "            num_layers=1,\n",
        "            bidirectional=False\n",
        "        )\n",
        "\n",
        "        self.word_lstm = nn.LSTM(\n",
        "            input_size=embedding_dim + char_hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=1,\n",
        "            bidirectional=False\n",
        "        )\n",
        "\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "    def forward(self, sentence, chars):\n",
        "        tag_scores = None\n",
        "        \"\"\"\n",
        "        Implement the forward pass.\n",
        "\n",
        "        Given a tokenized index-mapped sentence and a character sequence as the arguments,\n",
        "        find the corresponding raw scores for tags (without softmax)\n",
        "\n",
        "        returns:: tag_scores (Tensor)\n",
        "        \"\"\"\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "        seq_len = sentence.size(0)\n",
        "        word_embeddings = self.word_embedding(sentence)\n",
        "\n",
        "        # character embeddings\n",
        "        char_embeddings = self.char_embedding(chars)\n",
        "        char_embeddings = char_embeddings.permute(1, 0, 2)\n",
        "\n",
        "        char_lstm_output, _ = self.char_lstm(char_embeddings)\n",
        "\n",
        "\n",
        "        lastchar_lstm_output = char_lstm_output[-1]\n",
        "\n",
        "        # concatenated embeddings\n",
        "        full_embeds = torch.cat((word_embeddings, lastchar_lstm_output), dim=1)\n",
        "        full_embeds = full_embeds.unsqueeze(1)\n",
        "\n",
        "        # word embeddings\n",
        "        word_lstm_output, _ = self.word_lstm(full_embeds)\n",
        "        word_lstm_output = word_lstm_output.squeeze(1)\n",
        "\n",
        "        tag_scores = self.hidden2tag(word_lstm_output)\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Training [5 points]"
      ],
      "metadata": {
        "id": "IXke-HReCdkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_char(epoch, model, loss_function, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_examples = 0\n",
        "    for sentence, tags in training_data:\n",
        "        \"\"\"\n",
        "        Implement the training method\n",
        "\n",
        "        Hint: you can use the prepare_sequence method for creating index mappings\n",
        "          for sentences. For constructing character input, you may want to left pad\n",
        "          each word to MAX_WORD_LEN first, then use prepare_sequence method to create\n",
        "          index  mappings.\n",
        "        \"\"\"\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "        # Zero out the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Prepare input data (sentences, characters, and gold labels)\n",
        "        input_sentence = prepare_sequence(sentence, word_to_idx)\n",
        "        gold_label = prepare_sequence(tags, tag_to_idx)\n",
        "\n",
        "        # char padding\n",
        "        chars_padded = []\n",
        "        for word in sentence:\n",
        "            # left pad the word with spaces\n",
        "            padded_word = ' ' * (MAX_WORD_LEN - len(word)) + word\n",
        "            char_indices = [char_to_idx.get(char, 0) for char in padded_word]\n",
        "            chars_padded.append(char_indices)\n",
        "        chars_in = torch.tensor(chars_padded, dtype=torch.long)\n",
        "\n",
        "        # move tensors to GPU\n",
        "        if torch.cuda.is_available():\n",
        "            input_sentence = input_sentence.cuda()\n",
        "            chars_in = chars_in.cuda()\n",
        "            gold_label = gold_label.cuda()\n",
        "\n",
        "        # Do forward pass with current batch of input\n",
        "        tag_scores = model(input_sentence, chars_in)\n",
        "\n",
        "        # Get loss with model predictions and true labels\n",
        "        loss = loss_function(tag_scores, gold_label)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Increase running total loss and the number of past training samples\n",
        "        train_loss += loss.item()\n",
        "        train_examples += len(sentence)\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "    avg_train_loss = train_loss / train_examples\n",
        "    avg_val_loss, val_accuracy = evaluate_char(model, loss_function)\n",
        "\n",
        "    print(f\"Epoch: {epoch}/{EPOCHS}\\tAvg Train Loss: {avg_train_loss:.4f}\\tAvg Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.0f}\")\n",
        "\n",
        "def evaluate_char(model, loss_function):\n",
        "    \"\"\"\n",
        "    returns:: avg_val_loss (float)\n",
        "    returns:: val_accuracy (float)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    val_loss = 0\n",
        "    val_examples = 0\n",
        "    with torch.no_grad():\n",
        "        for sentence, tags in val_data:\n",
        "            \"\"\"\n",
        "            Implement the evaluate method. Find the average validation loss\n",
        "            along with the validation accuracy.\n",
        "\n",
        "            Hint: To find the accuracy, argmax of tag predictions can be used.\n",
        "            \"\"\"\n",
        "\n",
        "            ### BEGIN YOUR CODE ###\n",
        "\n",
        "            # Prepare input data (sentences, characters, and gold labels)\n",
        "            input_sentence = prepare_sequence(sentence, word_to_idx)  # [seq_len]\n",
        "            gold_labels = prepare_sequence(tags, tag_to_idx)  # [seq_len]\n",
        "\n",
        "            # char padding\n",
        "            chars_padded = []\n",
        "            for word in sentence:\n",
        "                # left pad\n",
        "                padded_word = ' ' * (MAX_WORD_LEN - len(word)) + word\n",
        "                char_indices = [char_to_idx.get(char, 0) for char in padded_word]\n",
        "                chars_padded.append(char_indices)\n",
        "\n",
        "            chars_in = torch.tensor(chars_padded, dtype=torch.long)\n",
        "\n",
        "            # move tensors to GPU\n",
        "            if torch.cuda.is_available():\n",
        "                input_sentence = input_sentence.cuda()\n",
        "                chars_in = chars_in.cuda()\n",
        "                gold_labels = gold_labels.cuda()\n",
        "\n",
        "\n",
        "            # Do forward pass with current batch of input\n",
        "            tag_scores = model(input_sentence, chars_in)\n",
        "\n",
        "            # Get loss with model predictions and true labels\n",
        "            loss = loss_function(tag_scores, gold_labels)\n",
        "\n",
        "            # Get the predicted labels\n",
        "            predicted = torch.argmax(tag_scores, dim=1)\n",
        "\n",
        "            # Get number of correct prediction\n",
        "            correct += (predicted == gold_labels).sum().item()\n",
        "\n",
        "            # Increase running total loss and the number of past valid samples\n",
        "            val_loss += loss.item()\n",
        "            val_examples += len(sentence)\n",
        "\n",
        "            ### END YOUR CODE ###\n",
        "    val_accuracy = 100. * correct / val_examples\n",
        "    avg_val_loss = val_loss / val_examples\n",
        "    return avg_val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "ll3IHzmiSxf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-QttCw6Otf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb235a10-5927-4584-9f38-da6e0088019b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\tAvg Train Loss: 1.0257\tAvg Val Loss: 0.7023\t Val Accuracy: 80\n",
            "Epoch: 2/10\tAvg Train Loss: 0.6375\tAvg Val Loss: 0.6456\t Val Accuracy: 82\n",
            "Epoch: 3/10\tAvg Train Loss: 0.5955\tAvg Val Loss: 0.6216\t Val Accuracy: 84\n",
            "Epoch: 4/10\tAvg Train Loss: 0.5420\tAvg Val Loss: 0.5796\t Val Accuracy: 85\n",
            "Epoch: 5/10\tAvg Train Loss: 0.5783\tAvg Val Loss: 0.6265\t Val Accuracy: 84\n",
            "Epoch: 6/10\tAvg Train Loss: 0.5514\tAvg Val Loss: 0.6541\t Val Accuracy: 83\n",
            "Epoch: 7/10\tAvg Train Loss: 0.5338\tAvg Val Loss: 0.6282\t Val Accuracy: 84\n",
            "Epoch: 8/10\tAvg Train Loss: 0.5006\tAvg Val Loss: 0.6201\t Val Accuracy: 83\n",
            "Epoch: 9/10\tAvg Train Loss: 0.4598\tAvg Val Loss: 0.5982\t Val Accuracy: 84\n",
            "Epoch: 10/10\tAvg Train Loss: 0.4422\tAvg Val Loss: 0.6015\t Val Accuracy: 86\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model, optimizer and the loss function\n",
        "# Hint, you may want to use reduction='sum' in the CrossEntropyLoss function\n",
        "\n",
        "### BEGIN YOUR CODE ###\n",
        "model = CharPOSTagger(\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    char_embedding_dim=CHAR_EMBEDDING_DIM,\n",
        "    char_hidden_dim=CHAR_HIDDEN_DIM,\n",
        "    char_size=len(char_to_idx),\n",
        "    vocab_size=len(word_to_idx),\n",
        "    tagset_size=len(tag_to_idx)\n",
        ")\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "\n",
        "### END YOUR CODE ###\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_char(epoch, model, loss_function, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xslNYW8EBKMQ"
      },
      "source": [
        "*Hint: Under the default hyperparameter setting, after 5 epochs you should be able to get at least `0.85` accuracy on the validation set.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtsHtaCQIo05"
      },
      "source": [
        "### 3.3 Error analysis [5 points]\n",
        "Write a method to generate predictions for the validation set.\n",
        "Create lists of words, tags predicted by the model and ground truth tags.\n",
        "\n",
        "Then use these lists to carry out error analysis to find the top-10 types of errors made by the model.\n",
        "\n",
        "This part is very similar to part 1.7. You may want to refer to your implementation there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vUawGsWIo06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dad61d5-13a4-48c3-b369-939c1caaafc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('VBN', 'VBD', 318, ['added', 'brought', 'thought', 'watched', 'felt'])\n",
            "(\"''\", 'VBD', 313, ['was', 'was', 'were', 'was', 'were'])\n",
            "('DT', 'VBD', 238, ['said', 'said', 'said', 'said', 'said'])\n",
            "('DT', 'IN', 206, ['that', 'that', 'that', 'that', 'that'])\n",
            "('JJ', 'NNP', 198, ['KTXL', 'Stapleton', 'German', 'JROE', 'Asia'])\n",
            "('JJ', 'NN', 189, ['executive', 'other', 'secret', 'fossil', 'evasion'])\n",
            "('VBP', 'VB', 172, ['operate', 'have', 'have', 'seem', 'hold'])\n",
            "('NN', 'JJ', 164, ['youthful', 'first', 'daunting', 'idle', '120-day'])\n",
            "('NNS', 'VBZ', 162, ['grouses', 'markets', 'allows', 'shares', 'improves'])\n",
            "('NN', 'NNP', 153, ['Cardiovascular', 'Eli', 'Lilly', 'Hamburg', 'Bremen'])\n"
          ]
        }
      ],
      "source": [
        "def generate_predictions(model, val_data):\n",
        "    \"\"\"\n",
        "    Generate predictions for val_data\n",
        "\n",
        "    Create lists of words, tags predicted by the model and ground truth tags.\n",
        "    Hint: It should look very similar to the evaluate function.\n",
        "\n",
        "    returns:: word_list (str list)\n",
        "    returns:: model_tags (str list)\n",
        "    returns:: gt_tags (str list)\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE ###\n",
        "    word_list = []\n",
        "    model_tags = []\n",
        "    gt_tags = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for sentence, tags in val_data:\n",
        "            input_sentence = prepare_sequence(sentence, word_to_idx)\n",
        "            gold_labels = prepare_sequence(tags, tag_to_idx)\n",
        "            chars_padded = []\n",
        "            for word in sentence:\n",
        "                padded_word = ' ' * (MAX_WORD_LEN - len(word)) + word\n",
        "                char_indices = [char_to_idx.get(char, 0) for char in padded_word]\n",
        "                chars_padded.append(char_indices)\n",
        "\n",
        "            chars_input = torch.tensor(chars_padded, dtype=torch.long)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                input_sentence = input_sentence.cuda()\n",
        "                chars_input = chars_input.cuda()\n",
        "\n",
        "            tag_scores = model(input_sentence, chars_input)\n",
        "            predicted = torch.argmax(tag_scores, dim=1)\n",
        "\n",
        "            predicted_tags = [idx_to_tag[idx.item()] for idx in predicted]\n",
        "            true_tags = tags\n",
        "\n",
        "            word_list.extend(sentence)\n",
        "            model_tags.extend(predicted_tags)\n",
        "            gt_tags.extend(true_tags)\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return word_list, model_tags, gt_tags\n",
        "\n",
        "def error_analysis(word_list, model_tags, gt_tags):\n",
        "    \"\"\"\n",
        "    Carry out error analysis\n",
        "\n",
        "    From those lists collected from the above method, find the\n",
        "    top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
        "    sorted by frequency\n",
        "\n",
        "    returns: errors (list of tuples)\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE ###\n",
        "    from collections import defaultdict\n",
        "    error_dict = defaultdict(lambda: {'count': 0, 'words': []})\n",
        "\n",
        "    for word, pred_tag, true_tag in zip(word_list, model_tags, gt_tags):\n",
        "        if pred_tag != true_tag:\n",
        "            key = (pred_tag, true_tag)\n",
        "            error_dict[key]['count'] += 1\n",
        "            if len(error_dict[key]['words']) < 5:\n",
        "                error_dict[key]['words'].append(word)\n",
        "\n",
        "    errors = [ (model_tag, true_tag, info['count'], info['words'])\n",
        "               for (model_tag, true_tag), info in error_dict.items() ]\n",
        "\n",
        "    errors.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return errors\n",
        "\n",
        "word_list, model_tags, gt_tags = generate_predictions(model, val_data)\n",
        "errors = error_analysis(word_list, model_tags, gt_tags)\n",
        "\n",
        "for i in errors[:10]:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuLl_BSMeovb"
      },
      "source": [
        "**Report your findings here.**  \n",
        "What kinds of errors does the character-level model make as compared to the original model, and why do you think it made them?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Common Error Types\n",
        "\n",
        "  a. Adjective vs. Noun Confusion:\n",
        "\n",
        "    ex) Misclassifying adjectives like \"greedy\" as nouns (NN instead of JJ).\n",
        "\n",
        "  b. Proper Noun vs. Common Noun Confusion:\n",
        "\n",
        "    ex) Labeling proper nouns such as \"Harrison\" as common nouns (NN instead of NNP).\n",
        "\n",
        "  c. Verb Tense Misclassification:\n",
        "\n",
        "    ex) Confusing past tense verbs with past participles (VBD vs. VBN).\n",
        "\n",
        "  d. Determiner vs. Proper Noun Confusion:\n",
        "\n",
        "    ex) Mistaking determiners like \"Mr.\" as proper nouns (DT instead of NNP).\n",
        "\n",
        "2. Possible Reasons\n",
        "\n",
        "  a. Words serving multiple grammatical roles, like \"change\", can confuse the model, especially when relying heavily on character patterns.\n",
        "\n",
        "  b. While character embeddings capture morphological features, they might lack broader contextual cues essential for accurate POS tagging.\n",
        "\n",
        "  c. With relatively small hyperparameters (EMBEDDING_DIM = 4, HIDDEN_DIM = 8), the model might struggle to capture complex patterns and distinctions between similar POS tags.\n",
        "\n",
        "\n",
        "3. CharPOSTagger has better handling of morphological variations and out-of-vocab words through character level embeddings, while it introduces new types of errors related to grammatical role distinctions and can overemphasize character patterns at the expense of contextual information.\n"
      ],
      "metadata": {
        "id": "_GQMlybzdjuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Fine-tuned BERT POS Tagger [Extra Credit - 5 points]\n",
        "\n",
        "In the above sections, we trained sequence-based models for POS tagging on a fairly limited dataset of *labeled* part of speech data. However, we can imagine the model is having to both learn the basics of language *and* part of speech tagging simultaneously. Perhaps, we can use a model pre-trained on a much larger corpus of language, and *fine-tune* the model on our specific task.\n",
        "\n",
        "For this, we can use **BERT** (see [*Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://aclanthology.org/N19-1423.pdf) NAACL, 2019). BERT introduces a method of pre-training a transformer encoder and fine-tuning the encoder on downstream tasks, and is extrordinarily infuential in NLP research and engineering (e.g., [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) has 45M downloads per month from Huggingface). The core idea is *transfer learning*, or that pre-training on a self-supervised mask language modeling objective can help with our downstream language task of POS tagging. For a step-by-step introduction to the BERT architecture, please see Jay Almmar's [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/).\n",
        "\n",
        "This section will walk you through the use of the popular **Huggingface Transformers** library (see [*Transformers: State-of-the-Art Natural Language Processing*](https://aclanthology.org/2020.emnlp-demos.6), the [HuggingFace Documentation](https://huggingface.co/transformers/) and [Abhishek Mishra's HF tutorial](https://github.com/abhimishra91/transformers-tutorials)), which is a widely used library for distributing and using transformer models. Luckily, we can think of the HuggingFace library as a wrapper on top of PyTorch, so these sections should look familiar to your work so far.\n",
        "\n",
        "**For this extra credit section, we will use a pre-trained BERT model, and fine-tune it on the POS tagging task.**"
      ],
      "metadata": {
        "id": "VRGyRLbSFT0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Install `transformers` and download DistilBERT\n",
        "\n",
        "For your fine-tuning code to run a bit faster, we will use a smaller \"distilled\" version of BERT called **DistilBERT** (see [*DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter*](https://arxiv.org/abs/1910.01108)). Fortunately with the `transformers` library, we could swap out the underlying model with no code changes to our dataloaders, architecture or traning setup!"
      ],
      "metadata": {
        "id": "FpxSsAVhFXIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU tokenizers transformers"
      ],
      "metadata": {
        "id": "RZ-T9h2DFZls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4bbece4-c42f-402d-f039-a52cd0bb50d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you are interested in what other models are available, you can find a\n",
        "# list of model names here (e.g., roberta-base, bert-base-uncased):\n",
        "# https://huggingface.co/transformers/pretrained_models.html\n",
        "\n",
        "from transformers import DistilBertModel, DistilBertTokenizerFast\n",
        "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "1GiJc6b4FZoZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "c294b931317d412990e32ce1bbc6cb57",
            "6fff4ed938144cae940f0e9a0349c50e",
            "a28c82ccfdc244c0b885d99b9f9c36e2",
            "d1d98267830042bb94ea7e6f2bd5225b",
            "0755b95f0ead49f0ac6325c93b674f5f",
            "82873b96ac23459c8a48c3622994f03b",
            "503940bd9532494c8e7977667a55f839",
            "effc6fb749fa4cec87034fd2cb7cf178",
            "5da4ccfdd9e34945b1a493a107206fee",
            "d897590ea8334ba5bd6a8f6a7acdf9ba",
            "bb3ab368a011441388a5a63e70bfe185",
            "2a4c515a5545447d9f0f84342bf6dbbf",
            "0f408cee5e55470f89dc6c0dd1202269",
            "e62163147aab4eed95f43bad9578f7a1",
            "205fa794974248b9b2cdd667a5120924",
            "ea89ad6205dd4cbda45fb9d0c3d7a7ea",
            "6c11a4e86e184822aff1e1722f319b1f",
            "6d461e41edc44adea9c4cd6b0fee1945",
            "2f87b61bacc04fb6a5c6a22425e4e0da",
            "1c1681ac68f545a7b2fe9f429d59ad27",
            "a5d2daad33e847a190ea9675fb689b62",
            "41e1a8dcc5914e2aa043c28762484494",
            "779bbfae92c5454c8c615a99ebc26066",
            "d8bd8c1297464722aeb80d9e43eb5b33",
            "e118f54f01d54ba5bf4e6468a19af0cf",
            "653476d4ca6e4b91807bb470a918e1ff",
            "ebc22d4fc7c8473d90cf478f01caadbe",
            "05be051868f34733b5970ec2a1d8a338",
            "537cc1a8518a4492870a9beb3a52383e",
            "76c9bed42167442cbc605494c0d56c4c",
            "53774a2e3d554791a6a9f57fdc99f129",
            "803f81d800c44bea9b8a093d33e29091",
            "ec19f0d1301d4f26baa317b02d16fa68",
            "7d2482f3b30f4ee4b477279330ba34ba",
            "e44fbc4839064df99226ac0aab13fa28",
            "06693ff28a7d42d79ffb2619876019c6",
            "bfbff3f010ec4e34a4c1228543c16eec",
            "ab81ac062b9e41348185f5039369d602",
            "a6ddf3ae29714c3ea634de3f421f6e77",
            "7361625fa7f54f4592b9c26b9c22958c",
            "e4a9001e0c4f4f8fa3b1e859fa33089b",
            "f8e6edb1d0f2416890d9d355cc6f08d1",
            "296a6ee5bb114a6c997c29a085cb68af",
            "35d1d4fe8319422fab4c5a00a075e206",
            "749636cbbaca4420a6aefe8c9c8ba6cc",
            "1b1778abd0a64b78877ca3ad7e55ec52",
            "1e1bcf38417846df984e24b9726462c6",
            "1ece2be6af4d4917a2bb83d5ed6945c9",
            "d9998e2682b2475dad3d3b2ada104483",
            "180dbc751a2a4e328d4caddac63efda7",
            "f543a9297bc44c8095f245711c55bde8",
            "5aa22b3acc44463b8fb514cb8e6036f4",
            "922138ab68994fb992acf946a8766850",
            "7064780362ac42e584f4b7eb64c24c91",
            "c3379fd0324a445883cea73aa775c78d"
          ]
        },
        "outputId": "df513172-fa6a-4125-e09d-f310096ec9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c294b931317d412990e32ce1bbc6cb57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a4c515a5545447d9f0f84342bf6dbbf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "779bbfae92c5454c8c615a99ebc26066"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d2482f3b30f4ee4b477279330ba34ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "749636cbbaca4420a6aefe8c9c8ba6cc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at our DistilBERT architecture\n",
        "bert_model"
      ],
      "metadata": {
        "id": "8izjKB9QFZrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5cf7c5-31ef-430f-a263-9253020f98b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertModel(\n",
              "  (embeddings): Embeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x TransformerBlock(\n",
              "        (attention): DistilBertSdpaAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Load the dataset with a PyTorch dataloader\n",
        "\n",
        "Please take a look at the `bert-base-cased` tokenizer on the [Tokenizer Playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground). Our goal will be to predict the POS of each word, but BERT is trained on sub-word tokens, so we need to segment our dataset such that **only the first token of each word is classified**."
      ],
      "metadata": {
        "id": "9kOLPfCkFZ3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class POSDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer, max_len, tag_to_idx):\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.tag_to_idx = tag_to_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "    Given an index, return the value in your training data (self.data). Make\n",
        "    sure the full output dict from self.tokenizer is returned, with an additional\n",
        "    value for your labels.\n",
        "\n",
        "    Remember! Your BERT tokenizer will give multiple tokens to words with the\n",
        "    same POS tag. We want the FIRST token be given the tag and all other tokens\n",
        "    to be given -100.\n",
        "\n",
        "    Hint: You may use the prepare_sequence() function from earlier sections\n",
        "    Hint: Our training data is already tokenized, so you may find the `is_split_into_words=True`\n",
        "      and `return_offsets_mapping=True` arguments helpful for getting the token offsets.\n",
        "    Hint: When using the tokenizer, you can also use padding='max_length' for [PAD]\n",
        "      tokens to be added for you.\n",
        "    \"\"\"\n",
        "    encoding = {}\n",
        "\n",
        "    ### BEGIN YOUR CODE ###\n",
        "\n",
        "    # Get the sentence and POS tags\n",
        "    sentence, pos_tags = self.data[index]\n",
        "\n",
        "    # Use the BERT tokenizer (self.tokenizer) to encode the sentence. Make sure to\n",
        "    # truncate the sentence if it is longer than self.max_len, and pad the sentence if it\n",
        "    # is less than self.max_len.\n",
        "\n",
        "    encoding = self.tokenizer(\n",
        "        sentence,\n",
        "        is_split_into_words=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=self.max_len,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Create token labels, where the first token of each word is the POS tag, and\n",
        "    # all others are -100.\n",
        "\n",
        "    labels = []\n",
        "    word_ids = encoding.word_ids(batch_index=0)\n",
        "\n",
        "    previous_word_idx = None\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            labels.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            # Add the token labels back to the tokenized dict\n",
        "            if word_idx < len(pos_tags):\n",
        "                pos_tag = pos_tags[word_idx]\n",
        "                label = self.tag_to_idx.get(pos_tag, -100)\n",
        "            else:\n",
        "                label = -100\n",
        "            labels.append(label)\n",
        "        else:\n",
        "            labels.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    # Make sure both your encoded sentence, labels and attention mask are PyTorch tensors\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    encoding['labels'] = labels\n",
        "    encoding.pop(\"offset_mapping\")\n",
        "\n",
        "    for key in encoding:\n",
        "        encoding[key] = encoding[key].squeeze(0)\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return encoding"
      ],
      "metadata": {
        "id": "mrz8yao9Fa2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use your POSDataset class to create a train and test set\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Further split your train data into train/test. You now have train/test/val.\n",
        "train_test_data, split = training_data, int(0.7 * len(training_data))\n",
        "random.shuffle(train_test_data)\n",
        "split_training_data, split_test_data = train_test_data[:split], train_test_data[split:]\n",
        "\n",
        "pos_tags_list = [tag for sentence, tags in training_data for tag in tags]\n",
        "unique_tags = sorted(list(set(pos_tags_list)))\n",
        "tag_to_idx = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "\n",
        "training_set = POSDataset(split_training_data, tokenizer, MAX_LEN, tag_to_idx)\n",
        "testing_set = POSDataset(split_test_data, tokenizer, MAX_LEN, tag_to_idx)\n",
        "validation_set = POSDataset(val_data, tokenizer, MAX_LEN, tag_to_idx)"
      ],
      "metadata": {
        "id": "cVyxHLOQFa4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a few values from your Dataloader!\n",
        "print(training_set.__getitem__(0)['input_ids'])\n",
        "print(training_set.__getitem__(0)['labels'])"
      ],
      "metadata": {
        "id": "6sAWlebYFa7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff0fcb9d-3e56-47aa-b01d-ee3e28d9dc9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  101,  2141,  1999,  1037, 11275,  2237,  1999,  2019,  2181,  2029,\n",
            "         2003,  2085,  2112,  1997,  3735,  1010,  2002,  2038,  4056,  2010,\n",
            "         2166,  2000,  1996,  2283, 14709,  1012,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor([-100,   36,   13,   10,   14,   18,   13,   10,   18,   39,   38,   14,\n",
            "          18,   13,   19,    5,   24,   38,   36,   25,   18,   31,   10,   18,\n",
            "          18,    6, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create PyTorch dataloaders from the POSDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "training_loader = DataLoader(training_set, batch_size=64, shuffle=True)\n",
        "testing_loader = DataLoader(testing_set, batch_size=64, shuffle=True)\n",
        "validating_loader = DataLoader(validation_set, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "_Ptnepq_Fa9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Define your `BertForPOSTagging` Model\n",
        "\n",
        "Now we will modify BERT by extending the `DistilBertModel` class for our task."
      ],
      "metadata": {
        "id": "Aue0XpaOFbK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForPOSTagging(DistilBertModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass through your model. Returns output logits for each POS\n",
        "        label and the loss (if labels is not None)\n",
        "\n",
        "        Hint: You may use nn.CrossEntropyLoss() to calculate your loss.\n",
        "        \"\"\"\n",
        "        loss, logits = None, None\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "        outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = outputs.last_hidden_state\n",
        "        logits = self.classifier(hidden_state)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        if loss is not None:\n",
        "          return loss, logits\n",
        "        return logits"
      ],
      "metadata": {
        "id": "JU0xBXEUFb5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForPOSTagging.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    num_labels=len(tag_to_idx)\n",
        ").to(device)\n",
        "\n",
        "MAX_GRAD_NORM = 10\n",
        "EPOCHS = 5\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-04)"
      ],
      "metadata": {
        "id": "MlQ_tBv8Fb7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ffd4d7-54a4-4960-c78f-10ecb72ac460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForPOSTagging were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.classifier.bias', 'distilbert.classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Training and Evaluation\n",
        "\n",
        "Now we have instantiated our model, please create the train loop!\n",
        "\n",
        "*Hint: If your implementation is correct, you can expect a validation accuracy of `0.88`*"
      ],
      "metadata": {
        "id": "8ehjPIReFcGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBERT will take up a lot of memory (particularly during development)\n",
        "# use this to check the amount of memory you currently have. (Note: you should\n",
        "# be able to fine-tune with ~5 GB of GPU memory)\n",
        "print(f\"Currently allocated GPU memory: {torch.cuda.memory_allocated(device) / 1024**3:.2f} GB / {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "# Hint: use `torch.cuda.empty_cache()` to clear the CUDA cache"
      ],
      "metadata": {
        "id": "-fx-SiOTFdBL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e25c7b-4b23-45cf-85e1-eb4bb0061c63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently allocated GPU memory: 0.26 GB / 14.75 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    train_loss = 0\n",
        "    train_examples, train_steps = 0, 0\n",
        "\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "        labels = batch['labels'].to(device, dtype=torch.long)\n",
        "\n",
        "        ### BEGIN YOUR CODE ###\n",
        "\n",
        "        # forward pass\n",
        "        loss, logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        ### END YOUR CODE ###\n",
        "\n",
        "        train_steps += 1\n",
        "\n",
        "        train_examples += labels.size(0)\n",
        "\n",
        "    avg_train_loss = train_loss / train_steps\n",
        "    avg_val_loss, val_accuracy = evaluate_bert(model)\n",
        "\n",
        "    print(f\"Epoch: {epoch}/{EPOCHS}\\tAvg Train Loss: {avg_train_loss:.4f}\\tAvg Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.0f}\")\n",
        "\n",
        "def evaluate_bert(model):\n",
        "    correct, val_loss, val_examples = 0, 0, 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(validating_loader):\n",
        "            \"\"\"\n",
        "            Implement the evaluate method. Find the average validation loss\n",
        "            along with the validation accuracy.\n",
        "\n",
        "            Remember! You have labeled only the first token of each word. Make\n",
        "            sure you only calculate accuracy on values which are not -100.\n",
        "            \"\"\"\n",
        "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "            labels = batch['labels'].to(device, dtype=torch.long)\n",
        "\n",
        "            ### BEGIN YOUR CODE ###\n",
        "\n",
        "            logits = model(input_ids=ids, attention_mask=mask)\n",
        "\n",
        "            loss_function = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_function(logits.view(-1, model.num_labels), labels.view(-1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Only compute accuracy at active labels\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            active_mask = labels != -100\n",
        "\n",
        "            # Get the predicted labels\n",
        "            active_labels = labels[active_mask]\n",
        "\n",
        "            # Get number of correct predictions\n",
        "            active_predictions = predictions[active_mask]\n",
        "\n",
        "            # Increase running total loss and the number of past valid samples\n",
        "            correct += (active_predictions == active_labels).sum().item()\n",
        "            val_examples += active_labels.size(0)\n",
        "\n",
        "            ### END YOUR CODE ###\n",
        "\n",
        "    val_accuracy = 100 * correct / val_examples\n",
        "    avg_val_loss = val_loss / val_examples\n",
        "    return avg_val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "Z51iRKbcFdDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "id": "WKu-SRnDFdFv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935e2ba3-7e41-4fbe-b4ff-5f81de4de1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0/5\tAvg Train Loss: 0.7742\tAvg Val Loss: 0.0007\t Val Accuracy: 97\n",
            "Epoch: 1/5\tAvg Train Loss: 0.1009\tAvg Val Loss: 0.0005\t Val Accuracy: 97\n",
            "Epoch: 2/5\tAvg Train Loss: 0.0553\tAvg Val Loss: 0.0005\t Val Accuracy: 98\n",
            "Epoch: 3/5\tAvg Train Loss: 0.0350\tAvg Val Loss: 0.0005\t Val Accuracy: 98\n",
            "Epoch: 4/5\tAvg Train Loss: 0.0242\tAvg Val Loss: 0.0005\t Val Accuracy: 98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Inference\n",
        "\n",
        "Good job! Now we can use our fine-tuned BERT model for POS tagging.\n",
        "\n",
        "In fact, if you have a fine-tuned transformer model (such as in a final project), you could directly upload the model to HuggingFace for others to use (see [this group](https://huggingface.co/QCRI/bert-base-multilingual-cased-pos-english), which fine-tuned on a much larger corpus of POS tags)."
      ],
      "metadata": {
        "id": "59uAtU7eFdQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prediction(model, sentence):\n",
        "    \"\"\"\n",
        "    Given a sentence, generate a full prediction of POS tags.\n",
        "\n",
        "    In this case, you are given a full sentence (not array of tokens), so you\n",
        "    will need to use your tokenizer differently.\n",
        "\n",
        "    Return your prediction in the format:\n",
        "      [(token 1, POS prediction 1), (token 2, POS prediction 2), ...]\n",
        "\n",
        "    E.g., \"The imperatives that\" => [('the', 'DT'), ('imperative', 'NNS'), ('that', 'WDT')]\n",
        "    \"\"\"\n",
        "    prediction = []\n",
        "\n",
        "    ### BEGIN YOUR CODE ###\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        sentence,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(logits, dim=-1)  # [1, seq_len]\n",
        "\n",
        "    predictions = predictions.cpu().numpy()[0]\n",
        "    word_ids = encoding.word_ids(batch_index=0)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    pos_tags = []\n",
        "\n",
        "    for idx, word_idx in enumerate(word_ids):\n",
        "        if word_idx is None:\n",
        "            continue\n",
        "        elif word_idx != (word_ids[idx - 1] if idx > 0 else None):\n",
        "            pos_tag = idx_to_tag[predictions[idx]]\n",
        "            pos_tags.append(pos_tag)\n",
        "        else:\n",
        "            continue\n",
        "    words = sentence.split()\n",
        "    for word, tag in zip(words, pos_tags):\n",
        "        prediction.append((word, tag))\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "YsER2OtOFeEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The imperatives that can be obeyed by a machine that has no limbs are bound to be of a rather intellectual character.\"\n",
        "print(generate_prediction(model, sentence))"
      ],
      "metadata": {
        "id": "YRt0NvoAFeG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada5d196-4301-46bf-9776-4052a9e5c383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'RBR'), ('imperatives', 'PRP$'), ('that', 'EX'), ('can', 'RP'), ('be', ':'), ('obeyed', '$'), ('by', \"''\"), ('a', 'RBR'), ('machine', 'PDT'), ('that', 'EX'), ('has', '.'), ('no', 'RBR'), ('limbs', 'PRP$'), ('are', 'NNPS'), ('bound', '$'), ('to', ')'), ('be', ':'), ('of', \"''\"), ('a', 'RBR'), ('rather', 'CD'), ('intellectual', 'JJ'), ('character.', 'PDT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Submit Your Homework\n",
        "This is the end of Project 2. Congratulations!\n",
        "\n",
        "Now, follow the steps below to submit your homework in Gradescope:\n",
        "\n",
        "1. Rename this ipynb file to 'CS4650_p2_GTusername.ipynb'. We recommend ensuring you have removed any extraneous cells & print statements, clearing all outputs, and using the Runtime --> Run all tool to make sure all output is update to date. Additionally, leaving comments in your code to help us understand your operations will assist the teaching staff in grading. It is not a requirement, but is recommended.\n",
        "2. Click on the menu 'File' --> 'Download' --> 'Download .py'.\n",
        "3. Click on the menu 'File' --> 'Download' --> 'Download .ipynb'.\n",
        "4. Download the notebook as a .pdf document. Make sure the outputs are captured so we can see how the loss and accuracy changes while training.\n",
        "5. Upload all 3 files to Gradescope. Double check the files start with `CS4650_p2_*`, capitalization matters.\n",
        "\n"
      ],
      "metadata": {
        "id": "W09rDJA03pcT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hBIfxZTCPpIu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c294b931317d412990e32ce1bbc6cb57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fff4ed938144cae940f0e9a0349c50e",
              "IPY_MODEL_a28c82ccfdc244c0b885d99b9f9c36e2",
              "IPY_MODEL_d1d98267830042bb94ea7e6f2bd5225b"
            ],
            "layout": "IPY_MODEL_0755b95f0ead49f0ac6325c93b674f5f"
          }
        },
        "6fff4ed938144cae940f0e9a0349c50e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82873b96ac23459c8a48c3622994f03b",
            "placeholder": "​",
            "style": "IPY_MODEL_503940bd9532494c8e7977667a55f839",
            "value": "config.json: 100%"
          }
        },
        "a28c82ccfdc244c0b885d99b9f9c36e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_effc6fb749fa4cec87034fd2cb7cf178",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5da4ccfdd9e34945b1a493a107206fee",
            "value": 483
          }
        },
        "d1d98267830042bb94ea7e6f2bd5225b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d897590ea8334ba5bd6a8f6a7acdf9ba",
            "placeholder": "​",
            "style": "IPY_MODEL_bb3ab368a011441388a5a63e70bfe185",
            "value": " 483/483 [00:00&lt;00:00, 14.5kB/s]"
          }
        },
        "0755b95f0ead49f0ac6325c93b674f5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82873b96ac23459c8a48c3622994f03b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "503940bd9532494c8e7977667a55f839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "effc6fb749fa4cec87034fd2cb7cf178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5da4ccfdd9e34945b1a493a107206fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d897590ea8334ba5bd6a8f6a7acdf9ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb3ab368a011441388a5a63e70bfe185": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a4c515a5545447d9f0f84342bf6dbbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f408cee5e55470f89dc6c0dd1202269",
              "IPY_MODEL_e62163147aab4eed95f43bad9578f7a1",
              "IPY_MODEL_205fa794974248b9b2cdd667a5120924"
            ],
            "layout": "IPY_MODEL_ea89ad6205dd4cbda45fb9d0c3d7a7ea"
          }
        },
        "0f408cee5e55470f89dc6c0dd1202269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c11a4e86e184822aff1e1722f319b1f",
            "placeholder": "​",
            "style": "IPY_MODEL_6d461e41edc44adea9c4cd6b0fee1945",
            "value": "model.safetensors: 100%"
          }
        },
        "e62163147aab4eed95f43bad9578f7a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f87b61bacc04fb6a5c6a22425e4e0da",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c1681ac68f545a7b2fe9f429d59ad27",
            "value": 267954768
          }
        },
        "205fa794974248b9b2cdd667a5120924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5d2daad33e847a190ea9675fb689b62",
            "placeholder": "​",
            "style": "IPY_MODEL_41e1a8dcc5914e2aa043c28762484494",
            "value": " 268M/268M [00:02&lt;00:00, 180MB/s]"
          }
        },
        "ea89ad6205dd4cbda45fb9d0c3d7a7ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c11a4e86e184822aff1e1722f319b1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d461e41edc44adea9c4cd6b0fee1945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f87b61bacc04fb6a5c6a22425e4e0da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c1681ac68f545a7b2fe9f429d59ad27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5d2daad33e847a190ea9675fb689b62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41e1a8dcc5914e2aa043c28762484494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "779bbfae92c5454c8c615a99ebc26066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8bd8c1297464722aeb80d9e43eb5b33",
              "IPY_MODEL_e118f54f01d54ba5bf4e6468a19af0cf",
              "IPY_MODEL_653476d4ca6e4b91807bb470a918e1ff"
            ],
            "layout": "IPY_MODEL_ebc22d4fc7c8473d90cf478f01caadbe"
          }
        },
        "d8bd8c1297464722aeb80d9e43eb5b33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05be051868f34733b5970ec2a1d8a338",
            "placeholder": "​",
            "style": "IPY_MODEL_537cc1a8518a4492870a9beb3a52383e",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "e118f54f01d54ba5bf4e6468a19af0cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76c9bed42167442cbc605494c0d56c4c",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53774a2e3d554791a6a9f57fdc99f129",
            "value": 48
          }
        },
        "653476d4ca6e4b91807bb470a918e1ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_803f81d800c44bea9b8a093d33e29091",
            "placeholder": "​",
            "style": "IPY_MODEL_ec19f0d1301d4f26baa317b02d16fa68",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.14kB/s]"
          }
        },
        "ebc22d4fc7c8473d90cf478f01caadbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05be051868f34733b5970ec2a1d8a338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "537cc1a8518a4492870a9beb3a52383e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76c9bed42167442cbc605494c0d56c4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53774a2e3d554791a6a9f57fdc99f129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "803f81d800c44bea9b8a093d33e29091": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec19f0d1301d4f26baa317b02d16fa68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d2482f3b30f4ee4b477279330ba34ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e44fbc4839064df99226ac0aab13fa28",
              "IPY_MODEL_06693ff28a7d42d79ffb2619876019c6",
              "IPY_MODEL_bfbff3f010ec4e34a4c1228543c16eec"
            ],
            "layout": "IPY_MODEL_ab81ac062b9e41348185f5039369d602"
          }
        },
        "e44fbc4839064df99226ac0aab13fa28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6ddf3ae29714c3ea634de3f421f6e77",
            "placeholder": "​",
            "style": "IPY_MODEL_7361625fa7f54f4592b9c26b9c22958c",
            "value": "vocab.txt: 100%"
          }
        },
        "06693ff28a7d42d79ffb2619876019c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4a9001e0c4f4f8fa3b1e859fa33089b",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8e6edb1d0f2416890d9d355cc6f08d1",
            "value": 231508
          }
        },
        "bfbff3f010ec4e34a4c1228543c16eec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_296a6ee5bb114a6c997c29a085cb68af",
            "placeholder": "​",
            "style": "IPY_MODEL_35d1d4fe8319422fab4c5a00a075e206",
            "value": " 232k/232k [00:00&lt;00:00, 2.74MB/s]"
          }
        },
        "ab81ac062b9e41348185f5039369d602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6ddf3ae29714c3ea634de3f421f6e77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7361625fa7f54f4592b9c26b9c22958c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4a9001e0c4f4f8fa3b1e859fa33089b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8e6edb1d0f2416890d9d355cc6f08d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "296a6ee5bb114a6c997c29a085cb68af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d1d4fe8319422fab4c5a00a075e206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "749636cbbaca4420a6aefe8c9c8ba6cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b1778abd0a64b78877ca3ad7e55ec52",
              "IPY_MODEL_1e1bcf38417846df984e24b9726462c6",
              "IPY_MODEL_1ece2be6af4d4917a2bb83d5ed6945c9"
            ],
            "layout": "IPY_MODEL_d9998e2682b2475dad3d3b2ada104483"
          }
        },
        "1b1778abd0a64b78877ca3ad7e55ec52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_180dbc751a2a4e328d4caddac63efda7",
            "placeholder": "​",
            "style": "IPY_MODEL_f543a9297bc44c8095f245711c55bde8",
            "value": "tokenizer.json: 100%"
          }
        },
        "1e1bcf38417846df984e24b9726462c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aa22b3acc44463b8fb514cb8e6036f4",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_922138ab68994fb992acf946a8766850",
            "value": 466062
          }
        },
        "1ece2be6af4d4917a2bb83d5ed6945c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7064780362ac42e584f4b7eb64c24c91",
            "placeholder": "​",
            "style": "IPY_MODEL_c3379fd0324a445883cea73aa775c78d",
            "value": " 466k/466k [00:00&lt;00:00, 6.43MB/s]"
          }
        },
        "d9998e2682b2475dad3d3b2ada104483": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "180dbc751a2a4e328d4caddac63efda7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f543a9297bc44c8095f245711c55bde8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5aa22b3acc44463b8fb514cb8e6036f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "922138ab68994fb992acf946a8766850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7064780362ac42e584f4b7eb64c24c91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3379fd0324a445883cea73aa775c78d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
