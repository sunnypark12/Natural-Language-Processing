{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d1zvoX4bfCF"
      },
      "source": [
        "## Mixture of Fine-Tuned Cross-Modal Experts\n",
        "\n",
        "### Minjun Kim, Heidi Lau, David Liu, Sunny Park"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbr1lOPtba7f"
      },
      "source": [
        "# 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa8nGV4dUVcT",
        "vscode": {
          "languageId": "shellscript"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d281e5e9-d4e7-439e-9d87-1e5894bec103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 11 11:18:17 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0              61W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: py7zr in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.10/dist-packages (from py7zr) (1.7.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.16.0 in /usr/local/lib/python3.10/dist-packages (from py7zr) (3.21.0)\n",
            "Requirement already satisfied: pyzstd>=0.15.9 in /usr/local/lib/python3.10/dist-packages (from py7zr) (0.16.2)\n",
            "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from py7zr) (1.1.0)\n",
            "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from py7zr) (1.0.2)\n",
            "Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from py7zr) (0.2.3)\n",
            "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from py7zr) (1.0.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from py7zr) (1.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from py7zr) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Check GPU configuration\n",
        "!nvidia-smi\n",
        "\n",
        "# Install required libraries\n",
        "!pip install torch torchvision torchaudio transformers datasets scikit-learn py7zr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk6mlGpFOKlF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random, json\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import cross_entropy\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import CIFAR10, OxfordIIITPet\n",
        "\n",
        "from transformers import BertModel, ViTModel, BertTokenizer, DistilBertModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGG6G4Amb_ws"
      },
      "source": [
        "# 2. Data Preparation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure directories exist\n",
        "os.makedirs(\"datasets\", exist_ok=True)"
      ],
      "metadata": {
        "id": "lJZ4vqfhG6Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o flickr8k https://www.kaggle.com/api/v1/datasets/download/adityajn105/flickr8k"
      ],
      "metadata": {
        "id": "bFGwghgnPbII",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "868dd272-b8d0-41a2-dd9f-78439e8a0104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1061M  100 1061M    0     0   153M      0  0:00:06  0:00:06 --:--:--  216M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip flickr8k -d datasets/flickr8k"
      ],
      "metadata": {
        "id": "LPRjSaH3Qgha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "826203c7-06b4-43a3-82b9-8f23c85dea3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  flickr8k\n",
            "replace datasets/flickr8k/Images/1000268201_693b08cb0e.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMbms6rklSrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35614192-f867-481b-f7a3-809e397dad76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define Image Transformations\n",
        "image_transform = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Task 1: Image Classification (CIFAR-10)\n",
        "cifar10_dataset = CIFAR10(root=\"datasets\", train=True, transform=image_transform, download=True)\n",
        "\n",
        "# Task 2: Sentiment Analysis (SST-2)\n",
        "sst2_dataset = load_dataset(\"glue\", \"sst2\", split=\"train\")\n",
        "\n",
        "# Task 3: Image Captioning (Custom Flickr8k Loader)\n",
        "class CustomFlickr8k(Dataset):\n",
        "    def __init__(self, root, ann_file, transform=None):\n",
        "        \"\"\"\n",
        "        Custom dataset loader for Flickr8k with captions.txt formatted as 'image,caption'.\n",
        "        :param root: Root directory containing the 'images/' folder.\n",
        "        :param ann_file: Path to the captions.txt file.\n",
        "        :param transform: Transformations to apply to images.\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.ann_file = ann_file\n",
        "        self.transform = transform\n",
        "        self.image_captions = []\n",
        "\n",
        "        # Parse the captions file\n",
        "        with open(ann_file, \"r\") as f:\n",
        "            lines = f.readlines()[1:]\n",
        "            for line in lines:\n",
        "                parts = line.strip().split(\",\", 1)  # Split into image and caption\n",
        "                if len(parts) == 2:\n",
        "                    image_id, caption = parts\n",
        "                    self.image_captions.append((image_id, caption))\n",
        "                else:\n",
        "                    print(f\"Skipping malformed line: {line.strip()}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_captions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id, caption = self.image_captions[idx]\n",
        "        image_path = os.path.join(self.root, \"Images\", image_id)\n",
        "\n",
        "        # Load the image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, caption\n",
        "\n",
        "# Setup Flickr8k Dataset\n",
        "flickr8k_dataset = CustomFlickr8k(\n",
        "    root=\"datasets/flickr8k\",\n",
        "    ann_file=\"datasets/flickr8k/captions.txt\",\n",
        "    transform=image_transform\n",
        ")\n",
        "\n",
        "# Task 4: Text Summarization (Samsum)\n",
        "samsum_dataset = load_dataset(\"samsum\", split=\"train\", trust_remote_code=True)\n",
        "\n",
        "# Task 5: Image Segmentation (Oxford-IIIT Pet Dataset)\n",
        "pet_dataset = OxfordIIITPet(root=\"datasets\", split=\"trainval\", transform=image_transform, download=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2Vptx57OKlF"
      },
      "outputs": [],
      "source": [
        "# Unified Multi-Task Dataset\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, datasets):\n",
        "        \"\"\"\n",
        "        Unified dataset wrapper for multi-task learning.\n",
        "        :param datasets: A dictionary mapping task names to datasets.\n",
        "        \"\"\"\n",
        "        self.datasets = datasets\n",
        "        self.task_keys = list(datasets.keys())\n",
        "\n",
        "        # Precompute dataset offsets for deterministic sampling\n",
        "        self.dataset_offsets = {}\n",
        "        offset = 0\n",
        "        for task, dataset in datasets.items():\n",
        "            self.dataset_offsets[task] = (offset, offset + len(dataset))\n",
        "            offset += len(dataset)\n",
        "        self.total_length = offset\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Find the corresponding task and local index\n",
        "        for task, (start, end) in self.dataset_offsets.items():\n",
        "            if start <= idx < end:\n",
        "                dataset_idx = idx - start\n",
        "                sample = self.datasets[task][dataset_idx]\n",
        "\n",
        "                # Standardize output format\n",
        "                if task == \"classification\":  # CIFAR-10\n",
        "                    input_data, label = sample\n",
        "                elif task == \"sentiment\":  # SST-2\n",
        "                    input_data = sample[\"sentence\"]\n",
        "                    label = sample[\"label\"]\n",
        "                elif task == \"captioning\":  # Flickr8k\n",
        "                    input_data, label = sample  # Input: image, Label: caption\n",
        "                elif task == \"summarization\":  # Samsum\n",
        "                    input_data = sample[\"dialogue\"]\n",
        "                    label = sample[\"summary\"]\n",
        "                elif task == \"segmentation\":  # Oxford-IIIT Pet\n",
        "                    input_data, label = sample  # Input: image, Label: breed label\n",
        "\n",
        "                return {\"task\": task, \"input\": input_data, \"label\": label}\n",
        "\n",
        "        raise IndexError(f\"Index {idx} out of range for MultiTaskDataset\")\n",
        "\n",
        "# Combine Datasets\n",
        "multi_task_dataset = MultiTaskDataset({\n",
        "    \"classification\": cifar10_dataset,\n",
        "    \"sentiment\": sst2_dataset,\n",
        "    \"captioning\": flickr8k_dataset,\n",
        "    \"summarization\": samsum_dataset,\n",
        "    \"segmentation\": pet_dataset\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvqPEZtMOKlG"
      },
      "outputs": [],
      "source": [
        "def multi_task_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to handle multi-task data with mixed types.\n",
        "    :param batch: List of samples returned by the dataset.\n",
        "    :return: A dictionary of grouped data by task.\n",
        "    \"\"\"\n",
        "    grouped_batch = {\"task\": [], \"input\": {}, \"label\": {}}\n",
        "\n",
        "    for sample in batch:\n",
        "        task = sample[\"task\"]\n",
        "        input_data = sample[\"input\"]\n",
        "        label = sample[\"label\"]\n",
        "\n",
        "        # Append task\n",
        "        grouped_batch[\"task\"].append(task)\n",
        "\n",
        "        # Group inputs and labels by task\n",
        "        if task not in grouped_batch[\"input\"]:\n",
        "            grouped_batch[\"input\"][task] = []\n",
        "            grouped_batch[\"label\"][task] = []\n",
        "\n",
        "        grouped_batch[\"input\"][task].append(input_data)\n",
        "        grouped_batch[\"label\"][task].append(label)\n",
        "\n",
        "    # Convert tensors to batched tensors for tasks with tensor data\n",
        "    for task in grouped_batch[\"input\"]:\n",
        "        if isinstance(grouped_batch[\"input\"][task][0], torch.Tensor):\n",
        "            grouped_batch[\"input\"][task] = torch.stack(grouped_batch[\"input\"][task])\n",
        "        # Leave text data as lists\n",
        "\n",
        "    for task in grouped_batch[\"label\"]:\n",
        "        if isinstance(grouped_batch[\"label\"][task][0], torch.Tensor):\n",
        "            grouped_batch[\"label\"][task] = torch.stack(grouped_batch[\"label\"][task])\n",
        "        # Leave text labels as lists\n",
        "\n",
        "    return grouped_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHqXMIlTOKlG"
      },
      "outputs": [],
      "source": [
        "# DataLoader\n",
        "multi_task_loader = DataLoader(\n",
        "    multi_task_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    collate_fn=multi_task_collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX-DnG0mOKlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f8a578-5639-4cd5-e3ff-e6641683f828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:\n",
            "Tasks: ['summarization', 'summarization', 'sentiment', 'classification', 'classification', 'sentiment', 'summarization', 'sentiment']\n",
            "Input type: <class 'dict'>\n",
            "Label type: <class 'dict'>\n",
            "Batch 2:\n",
            "Tasks: ['classification', 'captioning', 'sentiment', 'captioning', 'sentiment', 'sentiment', 'summarization', 'sentiment']\n",
            "Input type: <class 'dict'>\n",
            "Label type: <class 'dict'>\n",
            "Batch 3:\n",
            "Tasks: ['sentiment', 'summarization', 'sentiment', 'sentiment', 'sentiment', 'classification', 'sentiment', 'classification']\n",
            "Input type: <class 'dict'>\n",
            "Label type: <class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "# Verify batches\n",
        "for batch_idx, batch in enumerate(multi_task_loader):\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "    print(f\"Tasks: {batch['task']}\")  # List of task names\n",
        "    print(f\"Input type: {type(batch['input'])}\")  # Tensor for images, list for text\n",
        "    print(f\"Label type: {type(batch['label'])}\")  # Tensor or list depending on task\n",
        "\n",
        "    if isinstance(batch['input'], torch.Tensor):  # If input is image data\n",
        "        print(f\"Input shape: {batch['input'].shape}\")  # Tensor shape\n",
        "    if isinstance(batch['label'], torch.Tensor):  # If labels are tensors\n",
        "        print(f\"Label shape: {batch['label'].shape}\")  # Tensor shape\n",
        "\n",
        "    if batch_idx == 2:  # Limit to 3 batches for verification\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR-t0SyjOKlG"
      },
      "source": [
        "Verify Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss8oFd-POKlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510215ef-0d71-45d9-bd71-d7a5b7f9fe08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying CIFAR-10 Dataset (Image Classification)...\n",
            "Sample 1:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Label: 6\n",
            "Sample 2:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Label: 9\n",
            "Sample 3:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Label: 9\n",
            "\n",
            "\n",
            "Verifying SST-2 Dataset (Sentiment Analysis)...\n",
            "Sample 1:\n",
            "Text: hide new secretions from the parental units \n",
            "Label: 0\n",
            "Sample 2:\n",
            "Text: contains no wit , only labored gags \n",
            "Label: 0\n",
            "Sample 3:\n",
            "Text: that loves its characters and communicates something rather beautiful about human nature \n",
            "Label: 1\n",
            "\n",
            "\n",
            "Verifying Flickr8k Dataset (Image Captioning)...\n",
            "Sample 1:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Caption: A child in a pink dress is climbing up a set of stairs in an entry way .\n",
            "Sample 2:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Caption: A girl going into a wooden building .\n",
            "Sample 3:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Caption: A little girl climbing into a wooden playhouse .\n",
            "\n",
            "\n",
            "Verifying samsum Dataset (Text Summarization)...\n",
            "Sample 1:\n",
            "Dialogue: Amanda: I baked  cookies. Do you want some?\r\n",
            "Jerry: Sure!\r\n",
            "Amanda: I'll bring you tomorrow :-)\n",
            "Summary: Amanda baked cookies and will bring Jerry some tomorrow.\n",
            "Sample 2:\n",
            "Dialogue: Olivia: Who are you voting for in this election? \r\n",
            "Oliver: Liberals as always.\r\n",
            "Olivia: Me too!!\r\n",
            "Oliver: Great\n",
            "Summary: Olivia and Olivier are voting for liberals in this election. \n",
            "Sample 3:\n",
            "Dialogue: Tim: Hi, what's up?\r\n",
            "Kim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating\r\n",
            "Tim: What did you plan on doing?\r\n",
            "Kim: Oh you know, uni stuff and unfucking my room\r\n",
            "Kim: Maybe tomorrow I'll move my ass and do everything\r\n",
            "Kim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies\r\n",
            "Tim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores\r\n",
            "Tim: It really helps\r\n",
            "Kim: thanks, maybe I'll do that\r\n",
            "Tim: I also like using post-its in kaban style\n",
            "Summary: Kim may try the pomodoro technique recommended by Tim to get more stuff done.\n",
            "\n",
            "\n",
            "Verifying Oxford-IIIT Pet Dataset (Image Classification)...\n",
            "Sample 1:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Label: 0\n",
            "Sample 2:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Label: 0\n",
            "Sample 3:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Label: 0\n",
            "Sample 4:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Label: 0\n",
            "Sample 5:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Label: 0\n",
            "Sample 6:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Label: 0\n",
            "Sample 7:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Label: 0\n",
            "Sample 8:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Label: 0\n",
            "Sample 9:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Label: 0\n",
            "Sample 10:\n",
            "Image shape: torch.Size([3, 224, 224])\n",
            "Label: 0\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # Verify CIFAR-10 Dataset\n",
        "print(\"Verifying CIFAR-10 Dataset (Image Classification)...\")\n",
        "for idx in range(3):  # Check the first 3 samples\n",
        "    image, label = cifar10_dataset[idx]\n",
        "    print(f\"Sample {idx + 1}:\")\n",
        "    print(f\"Image shape: {image.shape}\")  # Should be (3, 224, 224)\n",
        "    print(f\"Label: {label}\")  # Integer label (0-9)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Verify SST-2 Dataset\n",
        "print(\"Verifying SST-2 Dataset (Sentiment Analysis)...\")\n",
        "for idx in range(3):  # Check the first 3 samples\n",
        "    sample = sst2_dataset[idx]\n",
        "    print(f\"Sample {idx + 1}:\")\n",
        "    print(f\"Text: {sample['sentence']}\")  # Input text\n",
        "    print(f\"Label: {sample['label']}\")  # 0 (negative) or 1 (positive)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Verify Flickr8k Dataset\n",
        "print(\"Verifying Flickr8k Dataset (Image Captioning)...\")\n",
        "flickr8k_dataset = CustomFlickr8k(\n",
        "    root=\"datasets/flickr8k\",\n",
        "    ann_file=\"datasets/flickr8k/captions.txt\",\n",
        "    transform=image_transform\n",
        ")\n",
        "for idx in range(3):  # Check the first 3 samples\n",
        "    try:\n",
        "        image, caption = flickr8k_dataset[idx]\n",
        "        print(f\"Sample {idx + 1}:\")\n",
        "        print(f\"Image shape: {image.shape}\")  # Should be (3, 224, 224)\n",
        "        print(f\"Caption: {caption}\")  # Caption as a string\n",
        "    except Exception as e:\n",
        "        print(f\"Error with sample {idx + 1}: {e}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Verify samsum Dataset\n",
        "print(\"Verifying samsum Dataset (Text Summarization)...\")\n",
        "for idx in range(3):  # Check the first 3 samples\n",
        "    sample = samsum_dataset[idx]\n",
        "    print(f\"Sample {idx + 1}:\")\n",
        "    print(f\"Dialogue: {sample['dialogue']}\")  # Input article\n",
        "    print(f\"Summary: {sample['summary']}\")  # Target summary\n",
        "print(\"\\n\")\n",
        "\n",
        "# Verify Oxford-IIIT Pet Dataset\n",
        "print(\"Verifying Oxford-IIIT Pet Dataset (Image Classification)...\")\n",
        "for idx in range(10):  # Check the first 3 samples\n",
        "    image, label = pet_dataset[idx]\n",
        "    print(f\"Sample {idx + 1}:\")\n",
        "    print(f\"Image shape: {image.shape}\")  # Should be (3, 224, 224)\n",
        "    print(f\"Label: {label}\")  # Integer label corresponding to the pet breed\n",
        "print(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gyxotYfjeJ3"
      },
      "source": [
        "# 3. Define the Mixture-of-Modalities (MoM) Backbone\n",
        "\n",
        "The MoM backbone includes modality-specific processing for text and image inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOK4jS3VjdFf"
      },
      "outputs": [],
      "source": [
        "class MixtureOfModalities(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MixtureOfModalities, self).__init__()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Process inputs for the Mixture of Experts.\n",
        "        :param inputs: Dictionary containing raw text or image input.\n",
        "        :return: Formatted dictionary with 'prompt' and 'input' keys.\n",
        "        \"\"\"\n",
        "        formatted_inputs = {}\n",
        "\n",
        "        # Handle text input\n",
        "        if \"text\" in inputs:\n",
        "            text = inputs[\"text\"]\n",
        "            if \".\" in text:\n",
        "                split_idx = text.index(\".\") + 1  # Split at the first period\n",
        "                formatted_inputs[\"prompt\"] = text[:split_idx].strip()\n",
        "                formatted_inputs[\"input\"] = text[split_idx:].strip() or None\n",
        "            else:\n",
        "                formatted_inputs[\"prompt\"] = text.strip()\n",
        "                formatted_inputs[\"input\"] = None\n",
        "\n",
        "        # Handle image input\n",
        "        if \"image\" in inputs:\n",
        "            image_input = inputs[\"image\"]\n",
        "            if \"input\" in formatted_inputs and formatted_inputs[\"input\"]:\n",
        "                formatted_inputs[\"input\"] = {\n",
        "                    \"text\": formatted_inputs[\"input\"],\n",
        "                    \"image\": image_input\n",
        "                }\n",
        "            else:\n",
        "                formatted_inputs[\"input\"] = image_input\n",
        "\n",
        "        # Ensure at least the prompt is available\n",
        "        if \"prompt\" not in formatted_inputs:\n",
        "            raise ValueError(\"At least a 'prompt' is required for processing.\")\n",
        "\n",
        "        return formatted_inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwf0fgfSjxrK"
      },
      "source": [
        "# 4. Define the Task Classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Model Definition\n",
        "\n",
        "The Task Classifier predicts a singular task. It uses a fully connected network with sigmoid activation for task probabilities."
      ],
      "metadata": {
        "id": "RQmFcjf75tUF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v2CB0RjjxPd"
      },
      "outputs": [],
      "source": [
        "class TaskClassifier(nn.Module):\n",
        "    def __init__(self, num_tasks=5):\n",
        "        super(TaskClassifier, self).__init__()\n",
        "        # Pretrained BERT model as the text encoder\n",
        "        self.text_encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        # Fully connected classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_tasks)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_input):\n",
        "        \"\"\"\n",
        "        Forward pass for the Task Classifier.\n",
        "        :param text_input: Tokenized text input (BERT format).\n",
        "        :return: Task logits and probabilities.\n",
        "        \"\"\"\n",
        "        # Get BERT's pooled output\n",
        "        text_features = self.text_encoder(**text_input).pooler_output  # Shape: (batch_size, 768)\n",
        "        # Pass through the classification head\n",
        "        task_logits = self.classifier(text_features)  # Shape: (batch_size, num_tasks)\n",
        "        probabilities = F.softmax(task_logits, dim=-1)  # Convert to probabilities\n",
        "        return task_logits, probabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEnSLAOiOKlH"
      },
      "source": [
        "### 4.2 Dataset Preparation\n",
        "\n",
        "We’ll create a custom dataset with 125 prompts for each task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aFEkPL1OKlI"
      },
      "outputs": [],
      "source": [
        "TASK_PROMPTS = {\n",
        "    \"captioning\": [\n",
        "        \"Write a caption for this image.\",\n",
        "        \"Describe what is happening in the picture.\",\n",
        "        \"What is shown in the image?\",\n",
        "        \"Explain the content of this image in a sentence.\",\n",
        "        \"Provide a one-sentence description of this image.\",\n",
        "        \"Describe the scene in the image.\",\n",
        "        \"What does the image depict?\",\n",
        "        \"Write a descriptive caption for the picture.\",\n",
        "        \"Summarize the content of the image.\",\n",
        "        \"Generate a caption for the given image.\",\n",
        "        \"What activity is occurring in the picture?\",\n",
        "        \"Provide a sentence describing the objects and scene in the image.\",\n",
        "        \"What is the main focus of this image?\",\n",
        "        \"Generate a natural language description of this image.\",\n",
        "        \"Explain the image in your own words.\",\n",
        "        \"What event is taking place in the picture?\",\n",
        "        \"Describe the objects and actions in the image.\",\n",
        "        \"Provide a short description of this photograph.\",\n",
        "        \"Write a caption that explains the image.\",\n",
        "        \"Summarize the activity or objects in the picture.\",\n",
        "        \"What does the photograph show?\",\n",
        "        \"Provide a single-sentence summary of this image.\",\n",
        "        \"Describe this image in detail.\",\n",
        "        \"Write a sentence about what is happening in the image.\",\n",
        "        \"Summarize the visual content of the picture.\"\n",
        "    ],\n",
        "    \"summarization\": [\n",
        "        \"Summarize the main points of this document.\",\n",
        "        \"Provide a brief summary of the text.\",\n",
        "        \"What is the gist of this article?\",\n",
        "        \"Condense the content of the document into a few sentences.\",\n",
        "        \"Summarize the document in one or two lines.\",\n",
        "        \"What are the key points discussed in this text?\",\n",
        "        \"Provide a concise summary of the article.\",\n",
        "        \"What is the main idea of the document?\",\n",
        "        \"Generate a short summary of this article.\",\n",
        "        \"Summarize the content in simple terms.\",\n",
        "        \"Write a brief overview of this document.\",\n",
        "        \"Extract the essential information from the text.\",\n",
        "        \"What is the central idea of the article?\",\n",
        "        \"Provide a short description of the document’s content.\",\n",
        "        \"Summarize this article in plain language.\",\n",
        "        \"Generate a few sentences that capture the main idea of this text.\",\n",
        "        \"What is this document about?\",\n",
        "        \"What are the highlights of this text?\",\n",
        "        \"Provide an abstract of the content.\",\n",
        "        \"Write a brief account of what the document says.\",\n",
        "        \"Summarize this document into its main themes.\",\n",
        "        \"Provide a synopsis of the article.\",\n",
        "        \"What are the critical points made in this text?\",\n",
        "        \"Condense the article into a short summary.\",\n",
        "        \"What is the summary of this document?\"\n",
        "    ],\n",
        "    \"segmentation\": [\n",
        "        \"Segment the regions of a pet in this image.\",\n",
        "        \"Highlight the areas containing pets in the picture.\",\n",
        "        \"Draw boundaries around the pets in the image.\",\n",
        "        \"What are the segmented regions of this image?\",\n",
        "        \"Segment this image into pet-related regions.\",\n",
        "        \"Divide this image into regions for each pet.\",\n",
        "        \"What parts of the image contain pets?\",\n",
        "        \"Mark the pets in this image.\",\n",
        "        \"Which areas of the image represent pets?\",\n",
        "        \"Outline the pets visible in this picture.\",\n",
        "        \"Segment the objects in the image and highlight the pets.\",\n",
        "        \"Identify the pet regions in the image.\",\n",
        "        \"Create a segmentation map of this image.\",\n",
        "        \"Draw boundaries around all the pets in this picture.\",\n",
        "        \"Which regions in the image belong to the pets?\",\n",
        "        \"Separate the pet regions from the background in this image.\",\n",
        "        \"Create a segmented output for this pet image.\",\n",
        "        \"Mark each pet in this image with a boundary.\",\n",
        "        \"Segment the pets visible in this photo.\",\n",
        "        \"Highlight the segmented regions of this image.\",\n",
        "        \"Create a boundary map for the pets in this image.\",\n",
        "        \"What regions of the image correspond to pets?\",\n",
        "        \"Outline the pet areas in the image.\",\n",
        "        \"Generate a segmentation mask for the pets.\",\n",
        "        \"What are the pet regions in this picture?\"\n",
        "    ],\n",
        "    \"sentiment\": [\n",
        "        \"Is this sentence positive or negative?\",\n",
        "        \"Determine the sentiment of this statement.\",\n",
        "        \"What is the sentiment of this text?\",\n",
        "        \"Classify the emotion conveyed in this sentence.\",\n",
        "        \"What is the emotional tone of this statement?\",\n",
        "        \"Identify whether the sentiment is positive or negative.\",\n",
        "        \"Analyze the sentiment of this text.\",\n",
        "        \"Is the sentiment of this sentence positive, negative, or neutral?\",\n",
        "        \"Assess the emotional tone of the sentence.\",\n",
        "        \"What feeling does this sentence convey?\",\n",
        "        \"Categorize the sentiment of this statement.\",\n",
        "        \"Classify this sentence as having positive or negative sentiment.\",\n",
        "        \"What is the mood expressed in this sentence?\",\n",
        "        \"Does this text express positive or negative sentiment?\",\n",
        "        \"Determine if this text is optimistic or pessimistic.\",\n",
        "        \"Evaluate the sentiment of this text snippet.\",\n",
        "        \"What is the emotional content of this sentence?\",\n",
        "        \"Analyze the tone of this statement.\",\n",
        "        \"Is the sentiment in this sentence good or bad?\",\n",
        "        \"Categorize the emotion in this sentence.\",\n",
        "        \"Label this sentence as positive or negative.\",\n",
        "        \"What is the sentiment score of this sentence?\",\n",
        "        \"Identify the polarity of this statement.\",\n",
        "        \"Determine the positive or negative sentiment of the text.\",\n",
        "        \"What emotional response does this sentence evoke?\"\n",
        "    ],\n",
        "    \"classification\": [\n",
        "        \"Classify the objects in the image into categories.\",\n",
        "        \"Identify the type of object in this image.\",\n",
        "        \"What class does this object belong to?\",\n",
        "        \"Categorize the object shown in the image.\",\n",
        "        \"Classify the image into one of several categories.\",\n",
        "        \"What is the object type in this image?\",\n",
        "        \"Which category does this image fall under?\",\n",
        "        \"Classify this photo into its appropriate category.\",\n",
        "        \"What is the class of the object in this picture?\",\n",
        "        \"Label the object in the image with its category.\",\n",
        "        \"Determine the category of this image.\",\n",
        "        \"What type of item is displayed in this picture?\",\n",
        "        \"Identify the classification for this image.\",\n",
        "        \"What is the label for this image?\",\n",
        "        \"Categorize this image into its appropriate class.\",\n",
        "        \"Classify the contents of this image.\",\n",
        "        \"Which category does this photo belong to?\",\n",
        "        \"Determine the correct class for this image.\",\n",
        "        \"What label should be assigned to this image?\",\n",
        "        \"Identify the object in this photo by its type.\",\n",
        "        \"Classify the objects in this picture.\",\n",
        "        \"What type of object does this image show?\",\n",
        "        \"Identify the object class in this image.\",\n",
        "        \"Categorize the image based on its content.\",\n",
        "        \"Label this image with the correct class.\"\n",
        "    ]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These text files MUST be downloaded manually, as they are custom-made .txt files."
      ],
      "metadata": {
        "id": "tl6Oie0MuhZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('summarization.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        TASK_PROMPTS[\"summarization\"].append(line.strip())\n",
        "\n",
        "with open('sentiment_analysis.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        TASK_PROMPTS[\"sentiment\"].append(line.strip())\n",
        "\n",
        "with open('segmentation.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        TASK_PROMPTS[\"segmentation\"].append(line.strip())\n",
        "\n",
        "with open('captioning.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        TASK_PROMPTS[\"captioning\"].append(line.strip())\n",
        "\n",
        "with open('classification.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        TASK_PROMPTS[\"classification\"].append(line.strip())"
      ],
      "metadata": {
        "id": "yDSYDiLuKIQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TASK_PROMPTS"
      ],
      "metadata": {
        "id": "4ALe9MonL15N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366e7f28-8079-432f-f0fb-a78c058f9218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'captioning': ['Write a caption for this image.',\n",
              "  'Describe what is happening in the picture.',\n",
              "  'What is shown in the image?',\n",
              "  'Explain the content of this image in a sentence.',\n",
              "  'Provide a one-sentence description of this image.',\n",
              "  'Describe the scene in the image.',\n",
              "  'What does the image depict?',\n",
              "  'Write a descriptive caption for the picture.',\n",
              "  'Summarize the content of the image.',\n",
              "  'Generate a caption for the given image.',\n",
              "  'What activity is occurring in the picture?',\n",
              "  'Provide a sentence describing the objects and scene in the image.',\n",
              "  'What is the main focus of this image?',\n",
              "  'Generate a natural language description of this image.',\n",
              "  'Explain the image in your own words.',\n",
              "  'What event is taking place in the picture?',\n",
              "  'Describe the objects and actions in the image.',\n",
              "  'Provide a short description of this photograph.',\n",
              "  'Write a caption that explains the image.',\n",
              "  'Summarize the activity or objects in the picture.',\n",
              "  'What does the photograph show?',\n",
              "  'Provide a single-sentence summary of this image.',\n",
              "  'Describe this image in detail.',\n",
              "  'Write a sentence about what is happening in the image.',\n",
              "  'Summarize the visual content of the picture.',\n",
              "  'Create a descriptive caption for this picture.',\n",
              "  'What is happening in this image?',\n",
              "  'Provide a detailed explanation of this photo.',\n",
              "  'Compose a caption to describe the scene in this image.',\n",
              "  'Summarize the events in this photograph with a sentence.',\n",
              "  'Describe the visible elements in the image.',\n",
              "  'Write a narrative to match the scene in this picture.',\n",
              "  'What story does this image tell?',\n",
              "  'Craft a descriptive caption that explains this photo.',\n",
              "  'What activities or objects can be identified in this image?',\n",
              "  'Describe the setting and action shown in this image.',\n",
              "  'Provide a creative caption for this image.',\n",
              "  'Write a one-line caption that summarizes this image.',\n",
              "  'What visual story is captured in this photo?',\n",
              "  'Compose a concise description of the events shown here.',\n",
              "  'Highlight the key features of this scene in one sentence.',\n",
              "  'Write a descriptive line about the objects in this photo.',\n",
              "  'Summarize what this image depicts in a few words.',\n",
              "  'What can you infer from the elements shown in this image?',\n",
              "  'Provide a short explanation of the visual context here.',\n",
              "  'Write a caption that describes the focus of this photograph.',\n",
              "  'What’s the main activity or object in this image?',\n",
              "  'How would you caption this image creatively?',\n",
              "  'Describe the overall mood or scene depicted in this picture.',\n",
              "  'What key details stand out in this photograph?',\n",
              "  'Summarize the scene shown in the picture.',\n",
              "  'Compose a one-sentence explanation of the image’s content.',\n",
              "  'Describe the environment captured in this image.',\n",
              "  'Provide a narrative to go with the visible elements here.',\n",
              "  'What is the significance of the items in this photo?',\n",
              "  'Write a brief line to explain what this image shows.',\n",
              "  'Highlight the primary actions or objects in this picture.',\n",
              "  'Describe the relationships between elements in this image.',\n",
              "  'What is the main theme of this picture?',\n",
              "  'Craft a caption that explains the events in the photo.',\n",
              "  'Write a descriptive line about the scenery shown here.',\n",
              "  'Summarize the objects and actions visible in this image.',\n",
              "  'What is the overall message or story of this photo?',\n",
              "  'Write a caption emphasizing the key activity in this image.',\n",
              "  'Describe the emotions captured in this photograph.',\n",
              "  'Provide a brief summary of the setting shown here.',\n",
              "  'What is happening between the subjects in this photo?',\n",
              "  'Explain the scene depicted in this image in one sentence.',\n",
              "  'What stands out visually in this picture?',\n",
              "  'Describe the interplay of elements in this photograph.',\n",
              "  'Write a caption that focuses on the mood of the image.',\n",
              "  'What do you notice about the objects or people here?',\n",
              "  'Create a description that matches the tone of this image.',\n",
              "  'What kind of story does this image convey?',\n",
              "  'Highlight the most important features of this photograph.',\n",
              "  'What unique elements make this image stand out?',\n",
              "  'Provide a descriptive caption for this complex scene.',\n",
              "  'What are the key visual themes in this image?',\n",
              "  'How would you describe the focus of this picture?',\n",
              "  'Write a sentence that summarizes the action in this photo.',\n",
              "  'Explain the relationships shown in this image.',\n",
              "  'Highlight the primary features visible in this picture.',\n",
              "  'Describe the essence of this image in a sentence.',\n",
              "  'What makes this photo distinctive or memorable?',\n",
              "  'Write a creative caption for the events shown here.',\n",
              "  'Describe the major elements that define this scene.',\n",
              "  'What do the objects in this image represent?',\n",
              "  'Compose a caption that encapsulates this picture’s theme.',\n",
              "  'What is the setting of this image, and why is it significant?',\n",
              "  'Write a description that brings this photo to life.',\n",
              "  'Summarize the events or actions visible in this image.',\n",
              "  'What is the central focus of this photograph?',\n",
              "  'Describe the composition of this image in one sentence.',\n",
              "  'Highlight the contrasts or harmony in this picture.',\n",
              "  'What is the background story behind this image?',\n",
              "  'Write a caption that aligns with the mood of the photo.',\n",
              "  'What stands out about the interaction shown in this image?',\n",
              "  'Describe the story implied by the elements in this photo.',\n",
              "  'What could this scene represent symbolically?',\n",
              "  'Highlight the visual impact of this image in a caption.',\n",
              "  'What emotions are conveyed through this photo’s composition?',\n",
              "  'Write a summary for the environment shown in this picture.',\n",
              "  'What is the relationship between the subjects in this image?',\n",
              "  'Describe the overall setting and its significance in this photo.',\n",
              "  'Write a one-liner summarizing the activity in this image.',\n",
              "  'Explain the purpose of the objects shown in this photo.',\n",
              "  'Highlight the beauty of the scene in this photograph.',\n",
              "  'Describe the context surrounding the objects in this image.',\n",
              "  'Provide a creative caption that fits this unique scene.',\n",
              "  'What is the connection between the elements in this photo?',\n",
              "  'Write a summary that includes the major details of this image.',\n",
              "  'Describe the focal point of this picture in one sentence.',\n",
              "  'What kind of emotions does this image evoke?',\n",
              "  'Highlight the story depicted through the objects in this photo.',\n",
              "  'Summarize the setting captured in this photograph.',\n",
              "  'Describe the atmosphere of this picture creatively.',\n",
              "  'Provide a caption that matches the tone of this image.',\n",
              "  'What details are most significant in this scene?',\n",
              "  'Explain the significance of the actions in this photo.',\n",
              "  'Highlight the primary themes shown in this photograph.',\n",
              "  'Summarize the relationships between subjects in this image.',\n",
              "  'Describe the spatial arrangement of elements in this scene.',\n",
              "  'What unique perspective does this photo offer?',\n",
              "  'Write a concise summary of the image’s visual content.',\n",
              "  'What does this picture tell you about its environment?'],\n",
              " 'summarization': ['Summarize the main points of this document.',\n",
              "  'Provide a brief summary of the text.',\n",
              "  'What is the gist of this article?',\n",
              "  'Condense the content of the document into a few sentences.',\n",
              "  'Summarize the document in one or two lines.',\n",
              "  'What are the key points discussed in this text?',\n",
              "  'Provide a concise summary of the article.',\n",
              "  'What is the main idea of the document?',\n",
              "  'Generate a short summary of this article.',\n",
              "  'Summarize the content in simple terms.',\n",
              "  'Write a brief overview of this document.',\n",
              "  'Extract the essential information from the text.',\n",
              "  'What is the central idea of the article?',\n",
              "  'Provide a short description of the document’s content.',\n",
              "  'Summarize this article in plain language.',\n",
              "  'Generate a few sentences that capture the main idea of this text.',\n",
              "  'What is this document about?',\n",
              "  'What are the highlights of this text?',\n",
              "  'Provide an abstract of the content.',\n",
              "  'Write a brief account of what the document says.',\n",
              "  'Summarize this document into its main themes.',\n",
              "  'Provide a synopsis of the article.',\n",
              "  'What are the critical points made in this text?',\n",
              "  'Condense the article into a short summary.',\n",
              "  'What is the summary of this document?',\n",
              "  'Summarize the main points of this text.',\n",
              "  'Provide a concise summary of the following passage.',\n",
              "  'What are the key ideas in this document?',\n",
              "  'Reduce the following text to its essentials.',\n",
              "  'Highlight the major themes in this passage.',\n",
              "  'Create a brief summary for this article.',\n",
              "  'What is the gist of this text?',\n",
              "  'Summarize the content of this text in a few sentences.',\n",
              "  'Provide the main takeaways from this passage.',\n",
              "  'What are the most critical details in this text?',\n",
              "  'Condense the information in this article into key points.',\n",
              "  'Write a short summary of this document.',\n",
              "  'Highlight the core arguments in this passage.',\n",
              "  'What is the overall purpose of this text?',\n",
              "  'Summarize the main topics covered in this writing.',\n",
              "  'Provide a one-paragraph summary of this document.',\n",
              "  'Highlight the critical aspects of this passage.',\n",
              "  'Reduce this text to its core elements.',\n",
              "  'What is the most important information in this writing?',\n",
              "  'Summarize the overall message of this document.',\n",
              "  'Highlight the summary-worthy aspects of this article.',\n",
              "  'Extract the main points from this writing.',\n",
              "  'What is the primary focus of this text?',\n",
              "  'Provide a concise recap of this document.',\n",
              "  'Summarize the narrative described in this passage.',\n",
              "  'Highlight the details central to this text.',\n",
              "  'What is the core content of this document?',\n",
              "  'Condense this article into a brief outline.',\n",
              "  'Provide a condensed version of this text.',\n",
              "  'Summarize this text in a single sentence.',\n",
              "  'What is the most significant takeaway from this writing?',\n",
              "  'Provide a brief synopsis of this article.',\n",
              "  'Highlight the main arguments presented in this text.',\n",
              "  'Extract the summary-worthy content from this passage.',\n",
              "  'What are the principal details in this text?',\n",
              "  'Write a condensed summary of this document.',\n",
              "  'Highlight the key takeaways from this article.',\n",
              "  'Summarize the central message of this passage.',\n",
              "  'What is the crux of this text?',\n",
              "  'Provide a brief description of the main points here.',\n",
              "  'Reduce the content of this document to its essentials.',\n",
              "  'Write a synopsis that captures the essence of this article.',\n",
              "  'Highlight the overall intent of this text.',\n",
              "  'Summarize the story conveyed in this passage.',\n",
              "  \"Provide a concise summary of the text's arguments.\",\n",
              "  'Extract the core narrative from this document.',\n",
              "  'Write a quick recap of the major details in this passage.',\n",
              "  'What is the essence of this article?',\n",
              "  'Summarize this document in a brief outline.',\n",
              "  'What are the primary topics covered in this text?',\n",
              "  'Provide a summary that captures the article’s intent.',\n",
              "  'Condense the story described in this document.',\n",
              "  'Highlight the primary themes in this passage.',\n",
              "  \"Write a short version of this text's arguments.\",\n",
              "  'Extract the main themes of this document.',\n",
              "  'What is the key information conveyed in this passage?',\n",
              "  'Highlight the major points of this article in one paragraph.',\n",
              "  'Write a brief summary of the central ideas here.',\n",
              "  'Summarize the critical arguments in this passage.',\n",
              "  'Condense the text into a summary format.',\n",
              "  'Provide a synopsis of the content described in this article.',\n",
              "  'Highlight the crucial elements of this writing.',\n",
              "  'Write a summary that encapsulates this passage.',\n",
              "  'What is the focus of this text, summarized briefly?',\n",
              "  'Provide a short description of the key topics discussed.',\n",
              "  'What are the main lessons conveyed by this text?',\n",
              "  'Extract the most important arguments in this document.',\n",
              "  'Highlight the details central to understanding this text.',\n",
              "  'Summarize this text in three sentences or less.',\n",
              "  'Provide a concise version of this document.',\n",
              "  'What are the major sections of this writing?',\n",
              "  'Condense this passage into key highlights.',\n",
              "  'Write a synopsis of the events described in this text.',\n",
              "  'Highlight the main takeaways from the passage.',\n",
              "  'What are the summary-worthy parts of this document?',\n",
              "  'Write a brief recap of the central points in this passage.',\n",
              "  'Highlight the important ideas presented in this text.',\n",
              "  'Provide a short version of the narrative in this document.',\n",
              "  'Summarize the primary focus of this passage.',\n",
              "  \"Write a concise explanation of this document's content.\",\n",
              "  'Highlight the key information included in this article.',\n",
              "  'Extract the summary content from this document.',\n",
              "  'What is the essence of the message in this text?',\n",
              "  'Summarize the most relevant points from this article.',\n",
              "  'Provide a clear and brief synopsis of this writing.',\n",
              "  'Condense the major details in this text into a few lines.',\n",
              "  'Highlight the key concepts discussed in this document.',\n",
              "  'Write a summary that captures the story in this passage.',\n",
              "  'What are the main themes emphasized in this text?',\n",
              "  'Provide a synopsis of the most critical elements here.',\n",
              "  'What are the essential arguments presented in this document?',\n",
              "  'Highlight the overall conclusions of this article.',\n",
              "  'Write a short and clear summary of this text.',\n",
              "  'Extract the fundamental aspects of this document.',\n",
              "  'Summarize the purpose and message of this article.',\n",
              "  \"Provide a concise recap of this text's content.\",\n",
              "  'Highlight the summary-worthy points in this article.',\n",
              "  'What is the narrative structure of this document?',\n",
              "  'Summarize the most critical conclusions of this passage.',\n",
              "  'Provide a brief and clear synopsis of the ideas here.'],\n",
              " 'segmentation': ['Segment the regions of a pet in this image.',\n",
              "  'Highlight the areas containing pets in the picture.',\n",
              "  'Draw boundaries around the pets in the image.',\n",
              "  'What are the segmented regions of this image?',\n",
              "  'Segment this image into pet-related regions.',\n",
              "  'Divide this image into regions for each pet.',\n",
              "  'What parts of the image contain pets?',\n",
              "  'Mark the pets in this image.',\n",
              "  'Which areas of the image represent pets?',\n",
              "  'Outline the pets visible in this picture.',\n",
              "  'Segment the objects in the image and highlight the pets.',\n",
              "  'Identify the pet regions in the image.',\n",
              "  'Create a segmentation map of this image.',\n",
              "  'Draw boundaries around all the pets in this picture.',\n",
              "  'Which regions in the image belong to the pets?',\n",
              "  'Separate the pet regions from the background in this image.',\n",
              "  'Create a segmented output for this pet image.',\n",
              "  'Mark each pet in this image with a boundary.',\n",
              "  'Segment the pets visible in this photo.',\n",
              "  'Highlight the segmented regions of this image.',\n",
              "  'Create a boundary map for the pets in this image.',\n",
              "  'What regions of the image correspond to pets?',\n",
              "  'Outline the pet areas in the image.',\n",
              "  'Generate a segmentation mask for the pets.',\n",
              "  'What are the pet regions in this picture?',\n",
              "  'Divide the image into distinct regions based on content.',\n",
              "  'Identify the boundaries of objects in the photo.',\n",
              "  'Separate the different parts of this image visually.',\n",
              "  'Highlight the segmented regions in this picture.',\n",
              "  'Analyze and label each section of this image.',\n",
              "  'Segment the visible elements in the photo accurately.',\n",
              "  'What are the major parts of this image, and how do they divide?',\n",
              "  'Outline the boundaries between the regions in this picture.',\n",
              "  'Identify and differentiate objects in the image.',\n",
              "  'Mark the areas of interest within the image.',\n",
              "  'How would you segment this image into meaningful parts?',\n",
              "  'Break down the objects in this photo into sections.',\n",
              "  'What regions can you identify in this picture?',\n",
              "  'Highlight the boundaries of key items in this image.',\n",
              "  'Separate the foreground and background in this image.',\n",
              "  'What are the individual components visible in this picture?',\n",
              "  'Create a segmentation map for the objects in this image.',\n",
              "  'Label the distinct sections of the image.',\n",
              "  'What parts of the image form separate entities?',\n",
              "  'Divide the image into logical parts based on objects.',\n",
              "  'Segment the photo into its fundamental elements.',\n",
              "  'What are the main objects visible in this segmented view?',\n",
              "  'Identify the regions that make up the image.',\n",
              "  'Mark the areas where objects change in this image.',\n",
              "  'Create a boundary map of the image content.',\n",
              "  'Label the foreground elements separately from the background.',\n",
              "  'Highlight the separations between major areas in the photo.',\n",
              "  'What regions of interest are present in this image?',\n",
              "  'Create a visual separation of objects in this picture.',\n",
              "  'What are the boundaries of the main subjects here?',\n",
              "  'Segment the different layers visible in the photo.',\n",
              "  'How would you isolate the objects in this image?',\n",
              "  'Divide the photo based on the content in each region.',\n",
              "  'What are the divisions within this scene?',\n",
              "  'Separate the prominent objects from the background.',\n",
              "  'What distinct areas does this image contain?',\n",
              "  'Highlight the structural differences across the image.',\n",
              "  \"What segments define this photo's content?\",\n",
              "  'Draw lines to indicate object boundaries in the image.',\n",
              "  'Identify and outline each major area in the picture.',\n",
              "  'How is the scene divided into meaningful areas?',\n",
              "  'Break down the image into smaller identifiable sections.',\n",
              "  'Label the areas of this image based on object boundaries.',\n",
              "  'What sections of the image represent distinct objects?',\n",
              "  'Segment this image to focus on its key elements.',\n",
              "  'What are the separations between layers in this picture?',\n",
              "  'Divide this image into its constituent visual components.',\n",
              "  'What parts of the image represent unique objects?',\n",
              "  'Highlight the segmentation details of this photo.',\n",
              "  'Label the different segments present in this image.',\n",
              "  'What is the segmentation of objects in this photo?',\n",
              "  'Mark the boundaries of significant objects in the picture.',\n",
              "  'Identify the layers present in this visual content.',\n",
              "  'How many segments can you count in this image?',\n",
              "  'Create distinct segments for each object in this photo.',\n",
              "  'What divisions are noticeable in this scene?',\n",
              "  'Separate the overlapping objects visible in this image.',\n",
              "  'Identify the sections where objects intersect in the photo.',\n",
              "  'Label the visual segments in this scene.',\n",
              "  'What are the boundaries between different regions here?',\n",
              "  'Divide the image into sections to analyze its content.',\n",
              "  'What areas of the photo are visually distinct?',\n",
              "  'Separate this image into meaningful visual regions.',\n",
              "  'What parts of the photo can be segmented clearly?',\n",
              "  'Label each major region of this photograph.',\n",
              "  'How would you create segments for the objects in this image?',\n",
              "  'Break the image into logical regions based on boundaries.',\n",
              "  'Identify the segmentation of overlapping areas in this photo.',\n",
              "  'What objects or regions can you isolate in this picture?',\n",
              "  'Highlight the separations visible in the image.',\n",
              "  'How many unique segments are in this photo?',\n",
              "  'Outline the differences between regions in this image.',\n",
              "  'What areas of the image are separate from others?',\n",
              "  'Identify and mark the divisions in this picture.',\n",
              "  'Segment the image into distinguishable parts.',\n",
              "  'Highlight the segments formed by natural boundaries.',\n",
              "  'How are the objects distributed across the image?',\n",
              "  'Label each section of the image for analysis.',\n",
              "  'Identify the spatial divisions of this scene.',\n",
              "  'What are the natural boundaries between regions here?',\n",
              "  'How can this photo be divided into meaningful parts?',\n",
              "  'What is the segmentation pattern of this image?',\n",
              "  'Identify the primary regions visible in this picture.',\n",
              "  'Divide this image into visual layers for clarity.',\n",
              "  'Highlight the borders between each object in this photo.',\n",
              "  'What distinct segments does this image contain?',\n",
              "  'How would you describe the segmentation in this photo?',\n",
              "  'Identify the transitions between objects in this image.',\n",
              "  'Break the photo into areas of interest for labeling.',\n",
              "  'Highlight the divisions within this image.',\n",
              "  'What parts of the image have the clearest boundaries?',\n",
              "  'Create segments for analysis of the objects in this picture.',\n",
              "  'Identify the natural separations in the scene.',\n",
              "  'How many distinct regions can be marked here?',\n",
              "  'Label the key divisions visible in this photo.',\n",
              "  'What areas in this image represent different segments?',\n",
              "  'How is the photo divided into its main sections?',\n",
              "  'Highlight the contrast between segments in this image.',\n",
              "  'Create a segmentation map showing the regions of this photo.',\n",
              "  'How can this image be segmented into logical sections?'],\n",
              " 'sentiment': ['Is this sentence positive or negative?',\n",
              "  'Determine the sentiment of this statement.',\n",
              "  'What is the sentiment of this text?',\n",
              "  'Classify the emotion conveyed in this sentence.',\n",
              "  'What is the emotional tone of this statement?',\n",
              "  'Identify whether the sentiment is positive or negative.',\n",
              "  'Analyze the sentiment of this text.',\n",
              "  'Is the sentiment of this sentence positive, negative, or neutral?',\n",
              "  'Assess the emotional tone of the sentence.',\n",
              "  'What feeling does this sentence convey?',\n",
              "  'Categorize the sentiment of this statement.',\n",
              "  'Classify this sentence as having positive or negative sentiment.',\n",
              "  'What is the mood expressed in this sentence?',\n",
              "  'Does this text express positive or negative sentiment?',\n",
              "  'Determine if this text is optimistic or pessimistic.',\n",
              "  'Evaluate the sentiment of this text snippet.',\n",
              "  'What is the emotional content of this sentence?',\n",
              "  'Analyze the tone of this statement.',\n",
              "  'Is the sentiment in this sentence good or bad?',\n",
              "  'Categorize the emotion in this sentence.',\n",
              "  'Label this sentence as positive or negative.',\n",
              "  'What is the sentiment score of this sentence?',\n",
              "  'Identify the polarity of this statement.',\n",
              "  'Determine the positive or negative sentiment of the text.',\n",
              "  'What emotional response does this sentence evoke?',\n",
              "  'Does this text express positive, negative, or neutral sentiment?',\n",
              "  'Evaluate the tone of the given passage.',\n",
              "  'What is the emotional stance of the following statement?',\n",
              "  'Classify the sentiment expressed in the text snippet.',\n",
              "  'Analyze the mood conveyed by the writing.',\n",
              "  'Determine the attitude reflected in this message.',\n",
              "  'Is this statement happy, sad, or neutral?',\n",
              "  'Identify the sentiment behind the following sentence.',\n",
              "  'What feelings are conveyed through this text?',\n",
              "  'Categorize the tone of this message as positive or negative.',\n",
              "  'What is the overall emotion expressed in this text?',\n",
              "  'Analyze whether this text shows optimism or pessimism.',\n",
              "  'How would you describe the tone of this statement?',\n",
              "  'What kind of emotion does this text express?',\n",
              "  'Does this passage have a positive or negative vibe?',\n",
              "  'Determine the underlying emotion in this sentence.',\n",
              "  'What mood does this piece of text evoke?',\n",
              "  'Evaluate the emotional content of this paragraph.',\n",
              "  'What attitude is portrayed in this writing?',\n",
              "  'Analyze the feelings conveyed through the following sentence.',\n",
              "  'Is this text cheerful, angry, or neutral in tone?',\n",
              "  'What does the tone of this message suggest?',\n",
              "  'Identify the positivity or negativity in this text.',\n",
              "  'How would you classify the mood of this text?',\n",
              "  'Does this text express anger, happiness, or indifference?',\n",
              "  'Determine the intensity of the emotion in this statement.',\n",
              "  'Classify the text based on its emotional tone.',\n",
              "  'Is the sentiment here enthusiastic, calm, or critical?',\n",
              "  'What kind of feeling is expressed in this sentence?',\n",
              "  'Analyze whether the tone is supportive or dismissive.',\n",
              "  'Is this passage conveying joy, frustration, or neither?',\n",
              "  'Categorize the text as hopeful, fearful, or indifferent.',\n",
              "  'What emotion is evident in this piece of writing?',\n",
              "  'Does this statement reflect confidence or doubt?',\n",
              "  'How would you describe the emotional nature of this text?',\n",
              "  'Is this text showing approval, disapproval, or neutrality?',\n",
              "  'Identify the dominant mood of this writing.',\n",
              "  'Does this sentence suggest agreement or disagreement?',\n",
              "  'Classify the sentiment as loving, hateful, or neutral.',\n",
              "  'Determine the tone of voice used in this statement.',\n",
              "  'Does this paragraph exude excitement, worry, or indifference?',\n",
              "  'What underlying attitude does this text reveal?',\n",
              "  'Is the text hopeful, critical, or indifferent?',\n",
              "  'Analyze whether the statement expresses gratitude or complaint.',\n",
              "  'What is the main emotion conveyed by this writing?',\n",
              "  'Is this text conveying contentment or dissatisfaction?',\n",
              "  'Does the tone suggest curiosity, anger, or neutrality?',\n",
              "  'How would you describe the positivity in this passage?',\n",
              "  'Does this statement feel welcoming, distant, or neutral?',\n",
              "  'Classify the attitude expressed in this text.',\n",
              "  'What is the general vibe of this sentence?',\n",
              "  'Is this paragraph encouraging or discouraging in tone?',\n",
              "  'Determine the emotional depth in this writing.',\n",
              "  'Is this piece more critical or complimentary in nature?',\n",
              "  'What kind of atmosphere does this text create?',\n",
              "  'Does this writing suggest agreement or contradiction?',\n",
              "  'Analyze the sentiment and categorize it clearly.',\n",
              "  'Is this text projecting confidence, doubt, or indifference?',\n",
              "  'What is the dominant feeling present in this sentence?',\n",
              "  'Does this statement carry a supportive or negative tone?',\n",
              "  'How would you describe the mood of this message?',\n",
              "  'Is this text optimistic, neutral, or pessimistic?',\n",
              "  'Does the passage indicate approval, critique, or neutrality?',\n",
              "  'What is the emotional weight of this writing?',\n",
              "  'Determine the happiness level expressed in this text.',\n",
              "  'Is this text friendly, critical, or indifferent?',\n",
              "  'What is the tone used in this piece of writing?',\n",
              "  'Does the statement show trust, fear, or neither?',\n",
              "  'Identify the level of negativity in this sentence.',\n",
              "  'Is the text inspiring, discouraging, or neutral?',\n",
              "  'What kind of feeling does this writing elicit?',\n",
              "  'Analyze whether the tone is grateful, regretful, or indifferent.',\n",
              "  'Is the statement demonstrating excitement or boredom?',\n",
              "  'What is the dominant attitude in this paragraph?',\n",
              "  'Does the text feel personal, formal, or neutral in tone?',\n",
              "  'What is the level of enthusiasm present in this text?',\n",
              "  'Is this passage lighthearted, serious, or neutral?',\n",
              "  'How would you describe the level of empathy in this sentence?',\n",
              "  'Does this writing convey admiration, annoyance, or neither?',\n",
              "  'What attitude does this text project toward its subject?',\n",
              "  'Is the tone encouraging, skeptical, or neutral?',\n",
              "  'Determine whether this message feels engaging or detached.',\n",
              "  'Is this statement appreciative or dismissive in tone?',\n",
              "  'What does the overall emotion in this text suggest?',\n",
              "  'Does this passage reflect an optimistic or critical tone?',\n",
              "  'How would you classify the level of positivity in this text?',\n",
              "  'Is the mood supportive, indifferent, or negative?',\n",
              "  'What does the tone of this writing reveal about its intent?',\n",
              "  'Is this statement confident, doubtful, or neutral?',\n",
              "  'Analyze whether this sentence is enthusiastic or critical.',\n",
              "  'What kind of energy does this text convey?',\n",
              "  'Does the tone reflect excitement, caution, or indifference?',\n",
              "  'Is this text showing admiration, disapproval, or neutrality?',\n",
              "  'What feeling dominates this piece of writing?',\n",
              "  'Is this passage uplifting, neutral, or discouraging?',\n",
              "  'How would you describe the level of negativity in this text?',\n",
              "  'Is this writing calm, passionate, or indifferent in tone?',\n",
              "  \"What does the text suggest about the writer's feelings?\",\n",
              "  'Is the sentiment in this statement light or heavy?',\n",
              "  'Does this paragraph lean toward optimism or pessimism?'],\n",
              " 'classification': ['Classify the objects in the image into categories.',\n",
              "  'Identify the type of object in this image.',\n",
              "  'What class does this object belong to?',\n",
              "  'Categorize the object shown in the image.',\n",
              "  'Classify the image into one of several categories.',\n",
              "  'What is the object type in this image?',\n",
              "  'Which category does this image fall under?',\n",
              "  'Classify this photo into its appropriate category.',\n",
              "  'What is the class of the object in this picture?',\n",
              "  'Label the object in the image with its category.',\n",
              "  'Determine the category of this image.',\n",
              "  'What type of item is displayed in this picture?',\n",
              "  'Identify the classification for this image.',\n",
              "  'What is the label for this image?',\n",
              "  'Categorize this image into its appropriate class.',\n",
              "  'Classify the contents of this image.',\n",
              "  'Which category does this photo belong to?',\n",
              "  'Determine the correct class for this image.',\n",
              "  'What label should be assigned to this image?',\n",
              "  'Identify the object in this photo by its type.',\n",
              "  'Classify the objects in this picture.',\n",
              "  'What type of object does this image show?',\n",
              "  'Identify the object class in this image.',\n",
              "  'Categorize the image based on its content.',\n",
              "  'Label this image with the correct class.',\n",
              "  'What type of object is shown in this image?',\n",
              "  'Assign a category to this photo based on its content.',\n",
              "  'Identify the primary subject of this image.',\n",
              "  'What class does this picture belong to?',\n",
              "  'Describe the object type featured in this image.',\n",
              "  'Classify the object visible in this photograph.',\n",
              "  'What is the main element in this image?',\n",
              "  'How would you label the content of this picture?',\n",
              "  'Categorize this image according to its theme.',\n",
              "  'Determine the group or type this image represents.',\n",
              "  \"What is the best label for this photo's subject?\",\n",
              "  'Which category does this image fit into?',\n",
              "  'What is the dominant element in this image?',\n",
              "  'Provide a class label for the item in this photo.',\n",
              "  'Identify the type of subject depicted here.',\n",
              "  'Determine the nature of the object in the image.',\n",
              "  'How would you describe the type of this photo?',\n",
              "  'Specify the class for the given visual content.',\n",
              "  \"Which label best describes this image's content?\",\n",
              "  'Analyze and categorize the object in the image.',\n",
              "  'What kind of item is displayed in this image?',\n",
              "  'Determine the category of the scene in this picture.',\n",
              "  'Label the type of environment captured in the photo.',\n",
              "  'What class of object does this image show?',\n",
              "  'Categorize the image into its proper group.',\n",
              "  'Identify the featured item in this image.',\n",
              "  'How would you describe the subject matter here?',\n",
              "  'Determine the classification of the photo’s content.',\n",
              "  'What is the main subject in this image?',\n",
              "  'What class is associated with this picture?',\n",
              "  'Assign a suitable label to this photograph.',\n",
              "  'How can this image be categorized based on its theme?',\n",
              "  'Identify the category of this object or scene.',\n",
              "  'What category does the object in this image fall under?',\n",
              "  'Label this image with its appropriate type.',\n",
              "  'Analyze the content and classify this image.',\n",
              "  'Determine the group this photograph belongs to.',\n",
              "  'Categorize the object captured in this image.',\n",
              "  'What label best describes the content of this image?',\n",
              "  'Classify this image into its correct group.',\n",
              "  'Identify the type of item displayed here.',\n",
              "  'Assign a meaningful label to this picture.',\n",
              "  'What is the classification of this visual content?',\n",
              "  'Determine the category of the object in the photo.',\n",
              "  'Label the content of this photograph appropriately.',\n",
              "  'What type of object is depicted in this image?',\n",
              "  'How would you classify the item in this photo?',\n",
              "  'What is the theme of this picture?',\n",
              "  'Provide a category for the objects in this image.',\n",
              "  'What classification does this photo belong to?',\n",
              "  'Determine the primary focus of this photograph.',\n",
              "  'Label the type of object represented in this image.',\n",
              "  'Categorize this image by identifying the object.',\n",
              "  'What group does this image fall under?',\n",
              "  'Identify the type of object captured in the photo.',\n",
              "  'How can this photograph be classified?',\n",
              "  'Assign a proper category to this visual content.',\n",
              "  \"What is the label for this image's primary subject?\",\n",
              "  'Determine the nature of the content in this photo.',\n",
              "  'Categorize the photograph based on its subject.',\n",
              "  'What is the category of the object shown here?',\n",
              "  'Classify the visual elements in this photograph.',\n",
              "  'Label the scene captured in this image.',\n",
              "  'What is the dominant feature of this image?',\n",
              "  'Determine the object type represented here.',\n",
              "  'Categorize this image into an appropriate type.',\n",
              "  \"How would you classify this photo's content?\",\n",
              "  'Assign a category to the subject in this photo.',\n",
              "  \"What is the classification of this picture's theme?\",\n",
              "  'Categorize the item visible in the image.',\n",
              "  'Determine the type of this scene.',\n",
              "  'Label the focus of this image accurately.',\n",
              "  'Classify the objects visible in this picture.',\n",
              "  'What category does this image fit into?',\n",
              "  'Provide a type label for the subject in this photo.',\n",
              "  \"Analyze and determine the image's classification.\",\n",
              "  'What is the group associated with this photo?',\n",
              "  'Categorize the primary subject of this image.',\n",
              "  'Label this image according to its content.',\n",
              "  'Assign the proper category to this visual.',\n",
              "  'Determine the category of the objects shown here.',\n",
              "  'What label would best suit this photo’s subject?',\n",
              "  'Classify the focus of this image.',\n",
              "  'Categorize the elements shown in this photo.',\n",
              "  'Assign a class to the item captured in the photo.',\n",
              "  'What is the type of object displayed in this image?',\n",
              "  'Identify the type of content visible in this image.',\n",
              "  'Categorize the subject matter shown here.',\n",
              "  \"How would you classify this scene's theme?\",\n",
              "  'Assign a meaningful label to the elements in the photo.',\n",
              "  \"What is the classification of this picture's content?\",\n",
              "  'Determine the category of the main subject here.',\n",
              "  'Label the primary focus of this image.',\n",
              "  'Assign a group to the item in the photograph.',\n",
              "  'What category is the content of this photo associated with?',\n",
              "  'Determine the type of objects represented in this image.',\n",
              "  'Categorize the scene captured in the picture.',\n",
              "  'Classify the theme depicted in this photograph.',\n",
              "  'Assign the correct group to the object in this image.',\n",
              "  'Label the elements shown in this photo.']}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = {\n",
        "    \"captioning\": 0,\n",
        "    \"summarization\": 1,\n",
        "    \"segmentation\": 2,\n",
        "    \"sentiment\": 3,\n",
        "    \"classification\": 4\n",
        "}"
      ],
      "metadata": {
        "id": "Dm8Ls7k1mu4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_prompts = {}\n",
        "for task, prompts in TASK_PROMPTS.items():\n",
        "    random.shuffle(prompts)  # Shuffle prompts for randomness\n",
        "    train, temp = train_test_split(prompts, test_size=0.3, random_state=42)\n",
        "    val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
        "    split_prompts[task] = {\"train\": train, \"val\": val, \"test\": test}"
      ],
      "metadata": {
        "id": "fzg3zOYgnBa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_split_dataset(split):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for task, splits in split_prompts.items():\n",
        "        texts.extend(splits[split])\n",
        "        labels.extend([tasks[task]] * len(splits[split]))\n",
        "    return texts, labels\n"
      ],
      "metadata": {
        "id": "SawRzSrvm5LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, train_labels = create_split_dataset(\"train\")\n",
        "val_texts, val_labels = create_split_dataset(\"val\")\n",
        "test_texts, test_labels = create_split_dataset(\"test\")\n",
        "\n",
        "# Verify split sizes\n",
        "print(f\"Training size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n"
      ],
      "metadata": {
        "id": "qDBxHP5em5qW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf987197-b099-4ba0-e335-fa521ae4ffc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training size: 435, Validation size: 95, Test size: 95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print(\"Train distribution:\", Counter(train_labels))\n",
        "print(\"Validation distribution:\", Counter(val_labels))\n",
        "print(\"Test distribution:\", Counter(test_labels))"
      ],
      "metadata": {
        "id": "vb5Vaoe5nUJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3cef81-8b26-46fb-ad94-53b0e8ad2165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train distribution: Counter({0: 87, 1: 87, 2: 87, 3: 87, 4: 87})\n",
            "Validation distribution: Counter({0: 19, 1: 19, 2: 19, 3: 19, 4: 19})\n",
            "Test distribution: Counter({0: 19, 1: 19, 2: 19, 3: 19, 4: 19})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maTMfIC3OKlJ"
      },
      "source": [
        "### 4.3 Task Classification Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCX0UB5XOKlJ"
      },
      "outputs": [],
      "source": [
        "class TaskClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        \"\"\"\n",
        "        :param texts: List of text inputs.\n",
        "        :param labels: List of task labels (0-4).\n",
        "        :param tokenizer: Pretrained tokenizer (e.g., BERT tokenizer).\n",
        "        \"\"\"\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Assign a padding token if it's not set\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token or '[PAD]'\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        tokenized = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        tokenized = {key: val.squeeze(0) for key, val in tokenized.items()}\n",
        "        return tokenized, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLwL5lOOOKlK"
      },
      "outputs": [],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Prepare datasets\n",
        "train_dataset = TaskClassificationDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = TaskClassificationDataset(val_texts, val_labels, tokenizer)\n",
        "test_dataset = TaskClassificationDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1t17qY2OKlK"
      },
      "source": [
        "### 4.4 Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIDerpQyOKlK"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=5, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Train the Task Classifier model.\n",
        "    :param model: The TaskClassifier model.\n",
        "    :param train_loader: DataLoader for training data with weighted sampling.\n",
        "    :param val_loader: DataLoader for validation data.\n",
        "    :param optimizer: Optimizer for the model.\n",
        "    :param criterion: Loss function (e.g., CrossEntropyLoss).\n",
        "    :param num_epochs: Number of epochs to train.\n",
        "    :param device: Device to run training on.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            # Unpack batch\n",
        "            tokenized_text, labels = batch\n",
        "            tokenized_text = {key: val.to(device) for key, val in tokenized_text.items()}\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            task_logits, _ = model(tokenized_text)  # Extract task_logits\n",
        "            loss = criterion(task_logits, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute accuracy\n",
        "            preds = torch.argmax(task_logits, dim=-1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        train_accuracy = correct / total\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validate the model\n",
        "        val_loss, val_accuracy = validate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIq7jzVsOKlK"
      },
      "outputs": [],
      "source": [
        "def validate_model(model, val_loader, criterion, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Validate the Task Classifier model.\n",
        "    :param model: The trained TaskClassifier model.\n",
        "    :param val_loader: DataLoader for validation data.\n",
        "    :param criterion: Loss function (e.g., CrossEntropyLoss).\n",
        "    :param device: Device to run validation on.\n",
        "    :return: Validation loss and accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            # Unpack batch\n",
        "            tokenized_text, labels = batch\n",
        "            tokenized_text = {key: val.to(device) for key, val in tokenized_text.items()}\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            task_logits, _ = model(tokenized_text)  # Extract task_logits\n",
        "            loss = criterion(task_logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            preds = torch.argmax(task_logits, dim=-1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qNibXPEOKlK"
      },
      "outputs": [],
      "source": [
        "def test_model(model, test_loader, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Test the Task Classifier model.\n",
        "    :param model: The trained TaskClassifier model.\n",
        "    :param test_loader: DataLoader for test data.\n",
        "    :param device: Device to run testing on.\n",
        "    :return: Test accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            # Unpack batch\n",
        "            tokenized_text, labels = batch\n",
        "            tokenized_text = {key: val.to(device) for key, val in tokenized_text.items()}\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            task_logits, _ = model(tokenized_text)  # Extract task_logits\n",
        "            preds = torch.argmax(task_logits, dim=-1)\n",
        "\n",
        "            # Compute accuracy\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wDdYPngOKlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366dc0b7-00c8-4e33-94c8-b95786e14bb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 1.5866, Train Accuracy: 0.2690\n",
            "Validation Loss: 1.5107, Validation Accuracy: 0.4947\n",
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 1.4214, Train Accuracy: 0.6253\n",
            "Validation Loss: 1.2701, Validation Accuracy: 0.7895\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 1.1529, Train Accuracy: 0.8069\n",
            "Validation Loss: 0.9628, Validation Accuracy: 0.9263\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.8498, Train Accuracy: 0.9494\n",
            "Validation Loss: 0.6808, Validation Accuracy: 0.9895\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 0.5842, Train Accuracy: 0.9770\n",
            "Validation Loss: 0.4452, Validation Accuracy: 0.9789\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3767, Train Accuracy: 0.9954\n",
            "Validation Loss: 0.2987, Validation Accuracy: 0.9895\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2593, Train Accuracy: 0.9954\n",
            "Validation Loss: 0.1923, Validation Accuracy: 1.0000\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 0.1761, Train Accuracy: 0.9977\n",
            "Validation Loss: 0.1473, Validation Accuracy: 1.0000\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 0.1250, Train Accuracy: 0.9977\n",
            "Validation Loss: 0.1222, Validation Accuracy: 0.9895\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 0.0987, Train Accuracy: 0.9977\n",
            "Validation Loss: 0.1014, Validation Accuracy: 0.9895\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "task_classifier = TaskClassifier(num_tasks=5).to(\"cuda\")\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = Adam(task_classifier.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "train_model(task_classifier, train_loader, val_loader, optimizer, criterion, num_epochs=10, device=\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPLDaYfHOKlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64caee4f-5fcf-4cc6-bcbc-abfcdad9799b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9895\n"
          ]
        }
      ],
      "source": [
        "# Test the model\n",
        "test_accuracy = test_model(task_classifier, test_loader, device=\"cuda\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(task_classifier.state_dict(), \"task_classifier.pth\")"
      ],
      "metadata": {
        "id": "t1Ks8aLVhz3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXP_CZvDj4ID"
      },
      "source": [
        "# 5. Define the Mixture-of-Experts (MoE) Module\n",
        "\n",
        "The MoE module contains several task-specific experts, each designed for a specific task, and a generalist to handle low confidence classification. Our `TaskClassifer` above activates one expert based on the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-t7D8rTOKlK"
      },
      "source": [
        "### 5.1 Evaluation of Fine-Tuned Text Task Expert Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15rwvg03OKlM"
      },
      "source": [
        "1. Fine-Tuned Bart for Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing Fine-Tuned model with vanilla model"
      ],
      "metadata": {
        "id": "EgV5s7SHvjQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "import transformers\n",
        "import datasets"
      ],
      "metadata": {
        "id": "3eWbEPDSvPAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import DatasetDict, load_dataset\n",
        "\n",
        "# Convert DataFrames to Hugging Face Datasets\n",
        "dataset_train = load_dataset(\"samsum\", split=\"train\")\n",
        "dataset_test = load_dataset(\"samsum\", split=\"test\")\n",
        "dataset_val = load_dataset(\"samsum\", split=\"validation\")\n",
        "\n",
        "# Create DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': dataset_train,\n",
        "    'test': dataset_test,\n",
        "    'validation': dataset_val\n",
        "})\n",
        "\n",
        "print(dataset_dict)\n",
        "dataset_samsum = dataset_dict"
      ],
      "metadata": {
        "id": "6mBgHObmUj9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca32a93-3c66-4369-a48f-2e3a98fd728d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary'],\n",
            "        num_rows: 14732\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary'],\n",
            "        num_rows: 819\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary'],\n",
            "        num_rows: 818\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum[\"train\"][:10]"
      ],
      "metadata": {
        "id": "HzvZtVkheCBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dde196eb-0059-480d-b4f7-8a65000b746a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': ['13818513',\n",
              "  '13728867',\n",
              "  '13681000',\n",
              "  '13730747',\n",
              "  '13728094',\n",
              "  '13716343',\n",
              "  '13611672',\n",
              "  '13730463',\n",
              "  '13809976',\n",
              "  '13809912'],\n",
              " 'dialogue': [\"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n",
              "  'Olivia: Who are you voting for in this election? \\r\\nOliver: Liberals as always.\\r\\nOlivia: Me too!!\\r\\nOliver: Great',\n",
              "  \"Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating\\r\\nTim: What did you plan on doing?\\r\\nKim: Oh you know, uni stuff and unfucking my room\\r\\nKim: Maybe tomorrow I'll move my ass and do everything\\r\\nKim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies\\r\\nTim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores\\r\\nTim: It really helps\\r\\nKim: thanks, maybe I'll do that\\r\\nTim: I also like using post-its in kaban style\",\n",
              "  \"Edward: Rachel, I think I'm in ove with Bella..\\r\\nrachel: Dont say anything else..\\r\\nEdward: What do you mean??\\r\\nrachel: Open your fu**ing door.. I'm outside\",\n",
              "  \"Sam: hey  overheard rick say something\\r\\nSam: i don't know what to do :-/\\r\\nNaomi: what did he say??\\r\\nSam: he was talking on the phone with someone\\r\\nSam: i don't know who\\r\\nSam: and he was telling them that he wasn't very happy here\\r\\nNaomi: damn!!!\\r\\nSam: he was saying he doesn't like being my roommate\\r\\nNaomi: wow, how do you feel about it?\\r\\nSam: i thought i was a good rommate\\r\\nSam: and that we have a nice place\\r\\nNaomi: that's true man!!!\\r\\nNaomi: i used to love living with you before i moved in with me boyfriend\\r\\nNaomi: i don't know why he's saying that\\r\\nSam: what should i do???\\r\\nNaomi: honestly if it's bothering you that much you should talk to him\\r\\nNaomi: see what's going on\\r\\nSam: i don't want to get in any kind of confrontation though\\r\\nSam: maybe i'll just let it go\\r\\nSam: and see how it goes in the future\\r\\nNaomi: it's your choice sam\\r\\nNaomi: if i were you i would just talk to him and clear the air\",\n",
              "  \"Neville: Hi there, does anyone remember what date I got married on?\\r\\nDon: Are you serious?\\r\\nNeville: Dead serious. We're on vacation, and Tina's mad at me about something. I have a strange suspicion that this might have something to do with our wedding anniversary, but I have nowhere to check.\\r\\nWyatt: Hang on, I'll ask my wife.\\r\\nDon: Haha, someone's in a lot of trouble :D\\r\\nWyatt: September 17. I hope you remember the year ;)\",\n",
              "  \"John: Ave. Was there any homework for tomorrow?\\r\\nCassandra: hello :D Of course, as always :D\\r\\nJohn: What exactly?\\r\\nCassandra: I'm not sure so I'll check it for you in 20minutes. \\r\\nJohn: Cool, thanks. Sorry I couldn't be there, but I was busy as fuck...my stupid boss as always was trying to piss me off\\r\\nCassandra: No problem, what did he do this time?\\r\\nJohn: Nothing special, just the same as always, treating us like children, commanding to do this and that...\\r\\nCassandra: sorry to hear that. but why don't you just go to your chief and tell him everything?\\r\\nJohn: I would, but I don't have any support from others, they are like goddamn pupets and pretend that everything's fine...I'm not gonna fix everything for everyone\\r\\nCassandra: I understand...Nevertheless, just try to ignore him. I know it might sound ridiculous as fuck, but sometimes there's nothing more you can do.\\r\\nJohn: yeah I know...maybe some beer this week?\\r\\nCassandra: Sure, but I got some time after classes only...this week is gonna be busy\\r\\nJohn: no problem, I can drive you home and we can go to some bar or whatever.\\r\\nCassandra: cool. ok, I got this homework. it's page 15 ex. 2 and 3, I also asked the others to study another chapter, especially the vocabulary from the very first pages. Just read it.\\r\\nJohn: gosh...I don't know if I'm smart enough to do it :'D\\r\\nCassandra: you are, don't worry :P Just circle all the words you don't know and we'll continue on Monday.\\r\\nJohn: ok...then I'll try my best :D\\r\\nCassandra: sure, if you will have any questions just either text or call me and I'll help you.\\r\\nJohn: I hope I won't have to waste your time xD\\r\\nCassandra: you're not wasting my time, I'm your teacher, I'm here to help. This is what I get money for, also :P\\r\\nJohn: just kidding :D ok, so i guess we'll stay in touch then\\r\\nCassandra: sure, have a nice evening :D\\r\\nJohn: you too, se ya\\r\\nCassandra: Byeeeee\",\n",
              "  \"Sarah: I found a song on youtube and I think you'll like it\\r\\nJames: What song?\\r\\nSarah: <file_other>\\r\\nJames: Oh. I know it! \\r\\nJames: I heard it before in some compilation\\r\\nSarah: I can't stop playing it over and over\\r\\nJames: That's exactly how I know lyrics to all of the songs on my playlist :D\\r\\nSarah: Haha. No lyrics here though. Instrumental ;D\\r\\nJames: Instrumental songs are different kind of music. \\r\\nJames: But you have to remember that the activity you do when you listen to this song\\r\\nJames: Is the actvity your brain will connect to the song\\r\\nJames: And everytime you play this song at home\\r\\nJames: You'll be thinking of your work\\r\\nSarah: Yeah, I know that. That's why we sometimes say - I used to like that song, but now it just reminds me of bad memories\\r\\nJames: Yup. Everytime you change your partner, you have to get rid of your favorite music :D\\r\\nSarah: Hahaha. True, true.\",\n",
              "  'Noah: When and where are we meeting? :)\\r\\nMadison: I thought you were busy...?\\r\\nNoah: Yeah, I WAS. I quit my job. \\r\\nMadison: No way! :o :o :o Why? I thought you liked it...?\\r\\nNoah: Well, I used to, until my boss turned into a complete cock... Long story.',\n",
              "  \"Matt: Do you want to go for date?\\r\\nAgnes: Wow! You caught me out with this question Matt.\\r\\nMatt: Why?\\r\\nAgnes: I simply didn't expect this from you.\\r\\nMatt: Well, expect the unexpected.\\r\\nAgnes: Can I think about it?\\r\\nMatt: What is there to think about?\\r\\nAgnes: Well, I don't really know you.\\r\\nMatt: This is the perfect time to get to know eachother\\r\\nAgnes: Well that's true.\\r\\nMatt: So let's go to the Georgian restaurant in Kazimierz.\\r\\nAgnes: Now your convincing me.\\r\\nMatt: Cool, saturday at 6pm?\\r\\nAgnes: That's fine.\\r\\nMatt: I can pick you up on the way to the restaurant.\\r\\nAgnes: That's really kind of you.\\r\\nMatt: No problem.\\r\\nAgnes: See you on saturday.\\r\\nMatt: Yes, looking forward to it.\\r\\nAgnes: Me too.\"],\n",
              " 'summary': ['Amanda baked cookies and will bring Jerry some tomorrow.',\n",
              "  'Olivia and Olivier are voting for liberals in this election. ',\n",
              "  'Kim may try the pomodoro technique recommended by Tim to get more stuff done.',\n",
              "  'Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside. ',\n",
              "  'Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do.',\n",
              "  \"Wyatt reminds Neville his wedding anniversary is on the 17th of September. Neville's wife is upset and it might be because Neville forgot about their anniversary.\",\n",
              "  \"John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which chapter to study. They are going to meet up for a beer sometime this week after class. \",\n",
              "  'Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories.',\n",
              "  'Noah wants to meet, he quit his job, because his boss was a dick.',\n",
              "  \"Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place.\"]}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get lengths of train, test and val data\n",
        "split_train_test_val = [len(dataset_samsum[split]) for split in dataset_samsum]"
      ],
      "metadata": {
        "id": "522rgtHeeJBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_train_test_val"
      ],
      "metadata": {
        "id": "kFg_lkASeMAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17bc13c0-bbd5-429d-c6d7-6a4979f5456e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[14732, 819, 818]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Split lengths: {split_train_test_val}\")\n",
        "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
        "print(\"\\nDialogue:\")\n",
        "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
        "print(\"\\nSummary:\")\n",
        "print(dataset_samsum[\"test\"][0][\"summary\"])"
      ],
      "metadata": {
        "id": "YtscQuOUeOF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edff48a4-f06d-4eee-acfc-a228e6d79337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split lengths: [14732, 819, 818]\n",
            "Features: ['id', 'dialogue', 'summary']\n",
            "\n",
            "Dialogue:\n",
            "Hannah: Hey, do you have Betty's number?\n",
            "Amanda: Lemme check\n",
            "Hannah: <file_gif>\n",
            "Amanda: Sorry, can't find it.\n",
            "Amanda: Ask Larry\n",
            "Amanda: He called her last time we were at the park together\n",
            "Hannah: I don't know him well\n",
            "Hannah: <file_gif>\n",
            "Amanda: Don't be shy, he's very nice\n",
            "Hannah: If you say so..\n",
            "Hannah: I'd rather you texted him\n",
            "Amanda: Just text him 🙂\n",
            "Hannah: Urgh.. Alright\n",
            "Hannah: Bye\n",
            "Amanda: Bye bye\n",
            "\n",
            "Summary:\n",
            "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "jawTU7QLvSsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ckpt = \"dhivyeshrk/bart-large-cnn-samsum\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
      ],
      "metadata": {
        "id": "bhDl9ZhLsZwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score evaluate"
      ],
      "metadata": {
        "id": "mtXaRItUsecm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13783f53-e321-4eb7-df88-60a629176b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.9)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]"
      ],
      "metadata": {
        "id": "RwM4rom1st9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunks(list_of_elements, batch_size):\n",
        "    \"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i + batch_size]"
      ],
      "metadata": {
        "id": "2esooQeWsuVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_summaries(dataset, metric, model, tokenizer, batch_size=16, device=device,\n",
        "                                   column_text=\"dialogue\",\n",
        "                                   column_summary=\"summary\"):\n",
        "    '''Calculate respective rouge metric for the given data'''\n",
        "    article_batches = list(chunks(dataset[column_text], batch_size)) # dialogue batches\n",
        "    target_batches = list(chunks(dataset[column_summary], batch_size))  # target batches\n",
        "\n",
        "    for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total=len(article_batches)):\n",
        "            inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
        "                            padding=\"max_length\", return_tensors=\"pt\") # encode the input\n",
        "            print(type(article_batch))\n",
        "            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),  # generate summary\n",
        "                             attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                             length_penalty=0.8, num_beams=8, max_length=128)\n",
        "            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
        "                                    clean_up_tokenization_spaces=True) for s in summaries] # decode them\n",
        "\n",
        "            decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]  # misc processing\n",
        "            metric.add_batch(predictions=decoded_summaries, references=target_batch)  # add this batch to the metric\n",
        "\n",
        "    score = metric.compute() # Calculate final metric score\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "DT2mCAwOsv9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_fineTuned = evaluate_summaries(\n",
        "    dataset_samsum[\"test\"],\n",
        "    rouge_metric,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    column_text=\"dialogue\",\n",
        "    column_summary=\"summary\",\n",
        "    batch_size=8\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5_x5O0ztXn7",
        "outputId": "b237cc38-184a-4cec-9d73-1b8496b089f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/103 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 1/103 [00:03<06:17,  3.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 2/103 [00:06<05:20,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 3/103 [00:09<05:08,  3.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 4/103 [00:12<05:00,  3.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▍         | 5/103 [00:15<04:50,  2.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 6/103 [00:18<05:07,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 7/103 [00:21<04:59,  3.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 8/103 [00:24<04:45,  3.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▊         | 9/103 [00:27<04:44,  3.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|▉         | 10/103 [00:30<04:37,  2.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 11/103 [00:33<04:30,  2.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 12/103 [00:36<04:29,  2.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 13/103 [00:39<04:23,  2.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▎        | 14/103 [00:42<04:30,  3.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 15/103 [00:45<04:28,  3.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 16/103 [00:48<04:21,  3.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 17/103 [00:51<04:17,  3.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 18/103 [00:54<04:11,  2.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 19/103 [00:57<04:04,  2.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 20/103 [01:00<04:00,  2.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 21/103 [01:02<03:56,  2.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██▏       | 22/103 [01:06<04:03,  3.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 23/103 [01:09<03:58,  2.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 24/103 [01:12<03:55,  2.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 25/103 [01:15<03:53,  2.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 26/103 [01:18<03:49,  2.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 27/103 [01:20<03:43,  2.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 28/103 [01:24<03:46,  3.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 29/103 [01:27<03:49,  3.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 30/103 [01:30<03:41,  3.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 31/103 [01:33<03:36,  3.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 32/103 [01:36<03:32,  3.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 33/103 [01:39<03:35,  3.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 34/103 [01:42<03:28,  3.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 35/103 [01:45<03:23,  3.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▍      | 36/103 [01:48<03:29,  3.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 37/103 [01:51<03:25,  3.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 38/103 [01:54<03:22,  3.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 39/103 [01:57<03:14,  3.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 40/103 [02:00<03:06,  2.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|███▉      | 41/103 [02:03<03:03,  2.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 42/103 [02:06<02:58,  2.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 43/103 [02:09<03:02,  3.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 44/103 [02:14<03:21,  3.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▎     | 45/103 [02:16<03:09,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▍     | 46/103 [02:19<02:59,  3.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 47/103 [02:22<02:53,  3.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 48/103 [02:25<02:45,  3.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 49/103 [02:28<02:41,  2.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▊     | 50/103 [02:31<02:45,  3.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|████▉     | 51/103 [02:34<02:39,  3.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 52/103 [02:37<02:35,  3.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████▏    | 53/103 [02:41<02:37,  3.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 54/103 [02:44<02:37,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 55/103 [02:47<02:29,  3.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 56/103 [02:50<02:29,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 57/103 [02:53<02:21,  3.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▋    | 58/103 [02:56<02:17,  3.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 59/103 [02:59<02:13,  3.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 60/103 [03:02<02:10,  3.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 61/103 [03:05<02:05,  2.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 62/103 [03:08<02:00,  2.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 63/103 [03:11<01:58,  2.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 64/103 [03:14<02:00,  3.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 65/103 [03:17<01:55,  3.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 66/103 [03:20<01:50,  2.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 67/103 [03:23<01:46,  2.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 68/103 [03:26<01:43,  2.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 69/103 [03:29<01:38,  2.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 70/103 [03:32<01:35,  2.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 71/103 [03:34<01:32,  2.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|██████▉   | 72/103 [03:37<01:28,  2.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 73/103 [03:40<01:28,  2.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 74/103 [03:43<01:26,  2.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 75/103 [03:46<01:22,  2.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 76/103 [03:49<01:20,  2.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▍  | 77/103 [03:52<01:16,  2.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 78/103 [03:55<01:13,  2.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 79/103 [03:58<01:10,  2.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 80/103 [04:01<01:08,  2.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▊  | 81/103 [04:04<01:07,  3.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|███████▉  | 82/103 [04:08<01:10,  3.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 83/103 [04:11<01:04,  3.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 84/103 [04:14<00:59,  3.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 85/103 [04:17<00:55,  3.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 86/103 [04:21<00:53,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 87/103 [04:24<00:49,  3.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 88/103 [04:27<00:45,  3.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▋ | 89/103 [04:30<00:45,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 90/103 [04:33<00:40,  3.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 91/103 [04:36<00:36,  3.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 92/103 [04:39<00:33,  3.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 93/103 [04:42<00:29,  2.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████▏| 94/103 [04:45<00:28,  3.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 95/103 [04:48<00:24,  3.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 96/103 [04:52<00:22,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 97/103 [04:55<00:18,  3.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 98/103 [04:58<00:15,  3.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 99/103 [05:00<00:12,  3.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 100/103 [05:03<00:09,  3.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 101/103 [05:07<00:06,  3.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 102/103 [05:10<00:03,  3.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [05:11<00:00,  3.03s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_dict_fineTuned = dict((rn, score_fineTuned[rn]) for rn in rouge_names)\n"
      ],
      "metadata": {
        "id": "ebM2VWfBxKO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame.from_records(rouge_dict_fineTuned, index=[f\"bart-cnn-FineTuned\"])\n"
      ],
      "metadata": {
        "id": "qgP5BmHVuti1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "9670c187-4439-4802-886b-6992b42cba4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      rouge1    rouge2    rougeL  rougeLsum\n",
              "bart-cnn-FineTuned  0.392169  0.197575  0.302004   0.301908"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b4ae4b6f-b0c6-492d-b117-570d73379b67\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rouge2</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>rougeLsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bart-cnn-FineTuned</th>\n",
              "      <td>0.392169</td>\n",
              "      <td>0.197575</td>\n",
              "      <td>0.302004</td>\n",
              "      <td>0.301908</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4ae4b6f-b0c6-492d-b117-570d73379b67')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b4ae4b6f-b0c6-492d-b117-570d73379b67 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b4ae4b6f-b0c6-492d-b117-570d73379b67');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3921694774280362,\n        \"max\": 0.3921694774280362,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3921694774280362\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.19757471673683644,\n        \"max\": 0.19757471673683644,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.19757471673683644\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.30200407608340607,\n        \"max\": 0.30200407608340607,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.30200407608340607\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeLsum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.301908131789264,\n        \"max\": 0.301908131789264,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.301908131789264\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()  # Free GPU Memory to test on original model."
      ],
      "metadata": {
        "id": "2_TIIKLjuvoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ckpt = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
      ],
      "metadata": {
        "id": "6jpNx0RcuwUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_original = evaluate_summaries(\n",
        "    dataset_samsum[\"test\"],\n",
        "    rouge_metric,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    column_text=\"dialogue\",\n",
        "    column_summary=\"summary\",\n",
        "    batch_size=8\n",
        ")\n",
        "rouge_dict_original = dict((rn, score_original[rn]) for rn in rouge_names)\n"
      ],
      "metadata": {
        "id": "fhAwGnZluxQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b9f3bd-a4ee-4297-97ee-2adb55b901a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/103 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 1/103 [00:03<05:58,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 2/103 [00:06<05:44,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 3/103 [00:10<05:39,  3.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 4/103 [00:15<06:31,  3.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▍         | 5/103 [00:18<06:15,  3.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 6/103 [00:22<06:22,  3.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 7/103 [00:26<06:06,  3.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 8/103 [00:29<05:46,  3.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▊         | 9/103 [00:33<05:41,  3.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|▉         | 10/103 [00:37<05:53,  3.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 11/103 [00:41<05:48,  3.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 12/103 [00:47<06:44,  4.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 13/103 [00:50<06:09,  4.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▎        | 14/103 [00:53<05:44,  3.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 15/103 [00:57<05:44,  3.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 16/103 [01:01<05:22,  3.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 17/103 [01:04<05:06,  3.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 18/103 [01:07<05:00,  3.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 19/103 [01:13<05:53,  4.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 20/103 [01:17<05:50,  4.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 21/103 [01:21<05:32,  4.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██▏       | 22/103 [01:25<05:20,  3.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 23/103 [01:28<05:07,  3.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 24/103 [01:32<04:55,  3.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 25/103 [01:36<04:52,  3.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 26/103 [01:39<04:50,  3.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 27/103 [01:43<04:41,  3.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 28/103 [01:47<04:40,  3.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 29/103 [01:50<04:29,  3.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 30/103 [01:53<04:17,  3.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 31/103 [01:57<04:06,  3.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 32/103 [02:01<04:15,  3.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 33/103 [02:05<04:21,  3.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 34/103 [02:08<04:08,  3.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 35/103 [02:11<03:54,  3.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▍      | 36/103 [02:14<03:51,  3.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 37/103 [02:19<04:01,  3.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 38/103 [02:22<03:55,  3.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 39/103 [02:26<03:48,  3.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 40/103 [02:29<03:44,  3.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|███▉      | 41/103 [02:33<03:42,  3.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 42/103 [02:36<03:34,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 43/103 [02:40<03:31,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 44/103 [02:45<03:52,  3.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▎     | 45/103 [02:48<03:44,  3.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▍     | 46/103 [02:52<03:31,  3.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 47/103 [02:55<03:20,  3.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 48/103 [02:58<03:10,  3.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 49/103 [03:03<03:22,  3.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▊     | 50/103 [03:06<03:12,  3.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|████▉     | 51/103 [03:09<03:06,  3.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 52/103 [03:13<02:56,  3.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████▏    | 53/103 [03:17<03:01,  3.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 54/103 [03:20<02:55,  3.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 55/103 [03:24<02:50,  3.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 56/103 [03:27<02:44,  3.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 57/103 [03:30<02:42,  3.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▋    | 58/103 [03:34<02:35,  3.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 59/103 [03:37<02:33,  3.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 60/103 [03:41<02:26,  3.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 61/103 [03:44<02:27,  3.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 62/103 [03:48<02:20,  3.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 63/103 [03:51<02:21,  3.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 64/103 [03:55<02:15,  3.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 65/103 [03:58<02:09,  3.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 66/103 [04:02<02:10,  3.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 67/103 [04:05<02:05,  3.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 68/103 [04:09<02:02,  3.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 69/103 [04:15<02:23,  4.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 70/103 [04:18<02:13,  4.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 71/103 [04:22<02:08,  4.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|██████▉   | 72/103 [04:26<01:58,  3.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 73/103 [04:31<02:13,  4.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 74/103 [04:35<02:01,  4.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 75/103 [04:39<01:54,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 76/103 [04:42<01:44,  3.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▍  | 77/103 [04:46<01:37,  3.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 78/103 [04:49<01:33,  3.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 79/103 [04:53<01:25,  3.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 80/103 [04:56<01:22,  3.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▊  | 81/103 [05:00<01:19,  3.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|███████▉  | 82/103 [05:04<01:20,  3.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 83/103 [05:07<01:12,  3.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 84/103 [05:11<01:08,  3.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 85/103 [05:14<01:04,  3.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 86/103 [05:18<01:03,  3.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 87/103 [05:22<00:58,  3.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 88/103 [05:26<00:55,  3.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▋ | 89/103 [05:30<00:52,  3.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 90/103 [05:33<00:47,  3.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 91/103 [05:37<00:43,  3.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 92/103 [05:40<00:38,  3.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 93/103 [05:44<00:35,  3.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████▏| 94/103 [05:48<00:33,  3.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 95/103 [05:51<00:28,  3.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 96/103 [05:54<00:24,  3.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 97/103 [05:59<00:22,  3.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 98/103 [06:02<00:18,  3.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 99/103 [06:05<00:14,  3.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 100/103 [06:09<00:10,  3.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 101/103 [06:13<00:07,  3.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 102/103 [06:16<00:03,  3.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [06:18<00:00,  3.68s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame.from_records(rouge_dict_original, index=[f\"bart-cnn-Facebook\"])"
      ],
      "metadata": {
        "id": "AWuFX5tkuyGK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "6a503d04-671e-46f6-f708-ba42a5eaec15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     rouge1    rouge2    rougeL  rougeLsum\n",
              "bart-cnn-Facebook  0.300203  0.097865  0.226568   0.226614"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b25f35b-5ed2-4ad8-8e40-1821e2f85a11\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rouge2</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>rougeLsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bart-cnn-Facebook</th>\n",
              "      <td>0.300203</td>\n",
              "      <td>0.097865</td>\n",
              "      <td>0.226568</td>\n",
              "      <td>0.226614</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b25f35b-5ed2-4ad8-8e40-1821e2f85a11')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1b25f35b-5ed2-4ad8-8e40-1821e2f85a11 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1b25f35b-5ed2-4ad8-8e40-1821e2f85a11');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3002028859026864,\n        \"max\": 0.3002028859026864,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3002028859026864\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.09786491224608423,\n        \"max\": 0.09786491224608423,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.09786491224608423\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.22656800233148366,\n        \"max\": 0.22656800233148366,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.22656800233148366\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeLsum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.2266139903793049,\n        \"max\": 0.2266139903793049,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.2266139903793049\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame.from_records(rouge_dict_fineTuned, index=[f\"bart-cnn-FineTuned\"])"
      ],
      "metadata": {
        "id": "PWQDaebbxLVt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "20118335-ada1-4000-fa2a-7862be5f33e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      rouge1    rouge2    rougeL  rougeLsum\n",
              "bart-cnn-FineTuned  0.392169  0.197575  0.302004   0.301908"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2ee6177f-7e44-444f-8480-8a188aea7efd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rouge2</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>rougeLsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bart-cnn-FineTuned</th>\n",
              "      <td>0.392169</td>\n",
              "      <td>0.197575</td>\n",
              "      <td>0.302004</td>\n",
              "      <td>0.301908</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ee6177f-7e44-444f-8480-8a188aea7efd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2ee6177f-7e44-444f-8480-8a188aea7efd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2ee6177f-7e44-444f-8480-8a188aea7efd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.3921694774280362,\n        \"max\": 0.3921694774280362,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3921694774280362\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.19757471673683644,\n        \"max\": 0.19757471673683644,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.19757471673683644\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.30200407608340607,\n        \"max\": 0.30200407608340607,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.30200407608340607\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeLsum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.301908131789264,\n        \"max\": 0.301908131789264,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.301908131789264\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see here that we see a significant increase in all Rouge metrics. The model successfully demonstrates specialization in summarization tasks."
      ],
      "metadata": {
        "id": "4Cvec7bzw8NT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSsRvdClOKlM"
      },
      "source": [
        "2. Fine-Tuned BERT for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "QBZDb0z0yLQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"bert-base-cased-fine-tuned-sst2\", name=\"ckandrew04\")"
      ],
      "metadata": {
        "id": "IjsbZlPcHkyf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "8a754d68-1b95-4f83-f178-f27687b65a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mckandrew04\u001b[0m (\u001b[33mcs4650\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241211_113102-t7sb45eb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs4650/bert-base-cased-fine-tuned-sst2/runs/t7sb45eb' target=\"_blank\">ckandrew04</a></strong> to <a href='https://wandb.ai/cs4650/bert-base-cased-fine-tuned-sst2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs4650/bert-base-cased-fine-tuned-sst2' target=\"_blank\">https://wandb.ai/cs4650/bert-base-cased-fine-tuned-sst2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs4650/bert-base-cased-fine-tuned-sst2/runs/t7sb45eb' target=\"_blank\">https://wandb.ai/cs4650/bert-base-cased-fine-tuned-sst2/runs/t7sb45eb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs4650/bert-base-cased-fine-tuned-sst2/runs/t7sb45eb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ff02f2bf760>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"glue\", \"sst2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "5-JK-0q_KQsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_uy0331OKlM"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], truncation=True, padding=True, max_length=128)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
        "encoded_dataset = encoded_dataset.remove_columns([\"sentence\", \"idx\"])\n",
        "encoded_dataset.set_format(type=\"torch\")"
      ],
      "metadata": {
        "id": "AmZd9X1n3-X6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f9e0aa3023094a64995a1d107aa8d836",
            "f2955ea047084bee992fffdcc33ab8fb",
            "e1a2debca94447c5b641c60e52b1d526",
            "6603c948055344ceb82379dd8f5b1934",
            "383ae6dcd5414609a794fb2b324292a0",
            "c2ee39b9e39741619abce0cdd65b9e35",
            "2bff9d641adc48f693d8be9aaf95898e",
            "baf917ac8c754778a3033dab92066afc",
            "c9ffa0004a9144c6a71bf0ad6074ebed",
            "e53fe5a08bfd47aa8ca1fc750d793bc8",
            "7331c5cdc10d41d093de49c517a9c1e3"
          ]
        },
        "outputId": "39f7c1e9-523c-4aa9-a212-d6a8fefae519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9e0aa3023094a64995a1d107aa8d836"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n"
      ],
      "metadata": {
        "id": "aaTqrL336f2L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89bf1b2-7a56-4c1e-f18b-959e57ff9927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"bert-base-cased-fine-tuned-sst2\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"wandb\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "KHwxm9UK6gXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ef7d3a-dc34-4da9-cf9f-0670d152442e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "EvcIDAxtHQkH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303,
          "referenced_widgets": [
            "e7fb23c993a9453584b56f5c690bddaa",
            "cba20794471b4d5c83e75b947a8d5289",
            "41d66426fb65492681d932573aacfca4",
            "946986c6d0b44af993151987a77ba127",
            "a8faf2cc5b504364abf01fee55fb9434",
            "e52c7897c0d44abbb5dd7cbeb6e78d43",
            "c5233e9db73b427492f557a13e094e63",
            "87a25ac4521446d4bab023f683d9cb98",
            "f39a3f9b9f3845189c945d35b53c9587",
            "3f83748102b6427a808429b668201933",
            "6b3a6ffdd5d044dc996e5b7cc4c3973f",
            "24ea97f25b974d92b331012aa0e300cb",
            "f07e410450414859a57200c1a716f600",
            "3f3eca17de2e4c7ca9275ac228936d05",
            "a633f43508624d1293dd759ec957b1ac",
            "3eb139b1630e40fabda1f041ad92faa6",
            "c55f5e2c891f4f0ab244deea93e46f8a"
          ]
        },
        "outputId": "b6ad04cd-9c89-4fb2-bc88-d7fe759a93a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7fb23c993a9453584b56f5c690bddaa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": accuracy}"
      ],
      "metadata": {
        "id": "m7BpdeHZNgG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "FZrqtV9KNga1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3865869-ae7b-410a-9506-f17f780b2f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-10d880e057ed>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "UZB4prc9Njq-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "ca50ba9b-759d-46b7-95ff-09dd1ea6fd71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12630' max='12630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12630/12630 13:19, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.180900</td>\n",
              "      <td>0.242000</td>\n",
              "      <td>0.915138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>0.340481</td>\n",
              "      <td>0.910550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.073600</td>\n",
              "      <td>0.365363</td>\n",
              "      <td>0.917431</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=12630, training_loss=0.14392916890051471, metrics={'train_runtime': 799.7679, 'train_samples_per_second': 252.632, 'train_steps_per_second': 15.792, 'total_flos': 7246198200700140.0, 'train_loss': 0.14392916890051471, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.push_to_hub(\"Training done for bert-base-cased-fine-tuned-sst2\")"
      ],
      "metadata": {
        "id": "c6S0vUuvuGBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate(encoded_dataset[\"validation\"])\n",
        "print(f\"Fine-Tuned BERT-base-cased Accuracy: {results['eval_accuracy']:.4f}\")"
      ],
      "metadata": {
        "id": "CDEhSt1wNk84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f49ecd9b-d969-4c83-d76e-88bd816ef591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [109/109 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tuned BERT-base-cased Accuracy: 0.9174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = dataset[\"validation\"]\n",
        "\n",
        "fine_tuned_model_name = \"ckandrew04/bert-base-cased-fine-tuned-sst2\"\n",
        "base_model_name = \"bert-base-cased\"\n",
        "\n",
        "fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(fine_tuned_model_name)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
      ],
      "metadata": {
        "id": "1r6KfypLSDW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(dataset):\n",
        "    tokenized = tokenizer(\n",
        "        dataset[\"sentence\"],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = torch.tensor(dataset[\"label\"])\n",
        "    return tokenized, labels"
      ],
      "metadata": {
        "id": "P1fHOlG4SHRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_inputs, val_labels = preprocess_data(val_dataset)"
      ],
      "metadata": {
        "id": "vFMuAx9zSSGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "val_loader = DataLoader(list(zip(val_inputs[\"input_ids\"], val_inputs[\"attention_mask\"], val_labels)), batch_size=batch_size)"
      ],
      "metadata": {
        "id": "WzQPujs4STTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, device=\"cpu\"):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "fE11lGVwSWa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "JoTHg4y0SW8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating Fine-Tuned Model...\")\n",
        "fine_tuned_val_accuracy = evaluate_model(fine_tuned_model, val_loader, device)\n",
        "\n",
        "# Print results\n",
        "print(f\"Fine-Tuned BERT-base-cased Accuracy: {fine_tuned_val_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "Hzvv0ZDPSX-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d79df1-924f-44eb-fd0b-142752f3e5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Fine-Tuned Model...\n",
            "Fine-Tuned BERT-base-cased Accuracy: 0.9209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have successfully fine-tuned the bert-base-cased model using the SST2 dataset, for Sentiment Analysis. We achieved accuracy of 0.9209. The model can be found under Minjun's Hugging Face profile, `ckandrew04/bert-base-cased-fine-tuned-sst2`."
      ],
      "metadata": {
        "id": "wlCHYT5ESlEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Mixture of Experts Module definition"
      ],
      "metadata": {
        "id": "PqkFnGnR5h6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForImageClassification, AutoModelForSemanticSegmentation, VisionEncoderDecoderModel\n"
      ],
      "metadata": {
        "id": "ee77zMS1P2Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MixtureOfExperts(nn.Module):\n",
        "    def __init__(self, task_classifier, experts, generalist, device=\"cuda\"):\n",
        "        super(MixtureOfExperts, self).__init__()\n",
        "        self.task_classifier = task_classifier.to(device)\n",
        "        self.experts = experts\n",
        "        self.generalist = generalist.to(device)\n",
        "        self.device = device\n",
        "\n",
        "        # Mapping of integer labels to task names\n",
        "        self.label_to_task = {\n",
        "            0: \"Image Captioning\",\n",
        "            1: \"Summarization\",\n",
        "            2: \"Image Segmentation\",\n",
        "            3: \"Sentiment Analysis\",\n",
        "            4: \"Image Classification\"\n",
        "        }\n",
        "\n",
        "    def forward(self, inputs, task_type=None):\n",
        "        \"\"\"\n",
        "        Forward pass for the Mixture of Experts.\n",
        "        :param inputs: Input data for the task (text or image or both).\n",
        "        :param task_type: Optional manual override for the task type.\n",
        "        :return: Model outputs based on the routed expert or generalist.\n",
        "        \"\"\"\n",
        "        print(f\"Inputs received: {inputs}\")\n",
        "        print(f\"Type of inputs: {type(inputs)}\")\n",
        "\n",
        "        # If task_type is manually provided\n",
        "        if task_type is not None:\n",
        "            if task_type in self.experts:\n",
        "                return self._route_to_expert(task_type, inputs)\n",
        "            else:\n",
        "                return self.generalist(inputs)\n",
        "\n",
        "        # Use TaskClassifier to determine task type\n",
        "        if \"prompt\" in inputs:  # Assuming text prompt for task classification\n",
        "            tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "            tokenized_prompt = tokenizer(\n",
        "                inputs[\"prompt\"],\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=128,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            tokenized_prompt = {key: val.to(self.device) for key, val in tokenized_prompt.items()}\n",
        "            task_logits, confidences = self.task_classifier(tokenized_prompt)\n",
        "            task_idx = torch.argmax(confidences, dim=-1).item()\n",
        "            task_type = self.label_to_task[task_idx]\n",
        "            return self._route_to_expert(task_type, inputs[\"input\"])  # Pass only the actual input\n",
        "        else:\n",
        "            raise ValueError(\"Inputs must contain a 'prompt' for task classification.\")\n",
        "\n",
        "    def _route_to_expert(self, task_type, actual_input):\n",
        "        \"\"\"\n",
        "        Route the inputs to the appropriate expert or generalist and process the output.\n",
        "        :param task_type: Type of the task.\n",
        "        :param actual_input: Actual input data for the task.\n",
        "        :return: Processed expert or generalist output.\n",
        "        \"\"\"\n",
        "        print(f\"Routing to expert for task: {task_type}\")\n",
        "\n",
        "        # If task type corresponds to an expert, route to the expert\n",
        "        expert = self.experts.get(task_type)\n",
        "        if expert:\n",
        "            # Process inputs based on expert type (text or image)\n",
        "            if task_type == \"Summarization\":\n",
        "                tokenizer = AutoTokenizer.from_pretrained(\"dhivyeshrk/bart-large-cnn-samsum\")\n",
        "                if isinstance(actual_input, str):\n",
        "                    tokenized_input = tokenizer(\n",
        "                        actual_input,\n",
        "                        padding=\"longest\",  # Use longest padding\n",
        "                        truncation=True,\n",
        "                        max_length=512,\n",
        "                        return_tensors=\"pt\"\n",
        "                    )\n",
        "                    tokenized_input.pop(\"token_type_ids\", None)\n",
        "\n",
        "                    tokenized_input[\"attention_mask\"] = tokenized_input[\"input_ids\"].ne(tokenizer.pad_token_id)\n",
        "                    tokenized_input = {key: val.to(self.device) for key, val in tokenized_input.items()}\n",
        "\n",
        "                    outputs = expert.generate(\n",
        "                        **tokenized_input,\n",
        "                        max_new_tokens=100,  # Set maximum output length for generation\n",
        "                        no_repeat_ngram_size=2,  # Avoid repetition\n",
        "                        early_stopping=True\n",
        "                    )\n",
        "                    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                else:\n",
        "                    raise ValueError(\"For Summarization, `actual_input` must be a string.\")\n",
        "\n",
        "            elif task_type == \"Sentiment Analysis\":\n",
        "                tokenizer = AutoTokenizer.from_pretrained(\"ckandrew04/bert-base-cased-fine-tuned-sst2\")\n",
        "                if isinstance(actual_input, str):\n",
        "                    tokenized_input = tokenizer(\n",
        "                        actual_input,\n",
        "                        padding=\"max_length\",\n",
        "                        truncation=True,\n",
        "                        max_length=128,\n",
        "                        return_tensors=\"pt\"\n",
        "                    )\n",
        "                    tokenized_input = {key: val.to(self.device) for key, val in tokenized_input.items()}\n",
        "                    outputs = expert(**tokenized_input)\n",
        "                    predicted_label = torch.argmax(outputs.logits, dim=-1).item()\n",
        "\n",
        "                    print(outputs)\n",
        "\n",
        "                    print(predicted_label)\n",
        "\n",
        "                    return \"Positive\" if predicted_label == 1 else \"Negative\"\n",
        "                else:\n",
        "                    raise ValueError(\"For Sentiment Analysis, `actual_input` must be a string.\")\n",
        "\n",
        "            elif task_type == \"Image Classification\":\n",
        "                if isinstance(actual_input, torch.Tensor):\n",
        "                    actual_input = actual_input.to(self.device)\n",
        "                    outputs = expert(pixel_values=actual_input)\n",
        "                    predicted_label = torch.argmax(outputs.logits, dim=-1).item()\n",
        "                    return expert.config.id2label[predicted_label]  # Map index to class label\n",
        "                else:\n",
        "                    raise ValueError(\"For Image Classification, `actual_input` must be a torch.Tensor.\")\n",
        "\n",
        "            elif task_type == \"Image Segmentation\":\n",
        "                if isinstance(actual_input, torch.Tensor):\n",
        "                    actual_input = actual_input.to(self.device)\n",
        "                    outputs = expert(pixel_values=actual_input)\n",
        "                    return outputs.logits.argmax(dim=1)  # Return the segmentation mask\n",
        "                else:\n",
        "                    raise ValueError(\"For Image Segmentation, `actual_input` must be a torch.Tensor.\")\n",
        "\n",
        "            elif task_type == \"Image Captioning\":\n",
        "                if isinstance(actual_input, torch.Tensor):\n",
        "                    actual_input = actual_input.to(self.device)\n",
        "                    outputs = expert.generate(pixel_values=actual_input)\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "                    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                else:\n",
        "                    raise ValueError(\"For Image Captioning, `actual_input` must be a torch.Tensor.\")\n",
        "\n",
        "        # Fallback to generalist if task type does not match any expert\n",
        "        elif self.generalist:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "            if isinstance(actual_input, str):\n",
        "                tokenized_input = tokenizer(\n",
        "                    actual_input,\n",
        "                    padding=\"max_length\",\n",
        "                    truncation=True,\n",
        "                    max_length=128,\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "                tokenized_input = {key: val.to(self.device) for key, val in tokenized_input.items()}\n",
        "                outputs = self.generalist(**tokenized_input)\n",
        "                mask_token_index = (tokenized_input[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "                predicted_tokens = torch.argmax(outputs.logits, dim=-1)\n",
        "                return tokenizer.decode(predicted_tokens[mask_token_index], skip_special_tokens=True)\n",
        "            else:\n",
        "                raise ValueError(\"For generalist tasks, `actual_input` must be a string.\")\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"No expert or generalist found for task type: {task_type}\")\n"
      ],
      "metadata": {
        "id": "kDKu8oRV5pCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    BertTokenizer, BertForSequenceClassification, BartForConditionalGeneration,\n",
        "    ViTForImageClassification, SegformerForSemanticSegmentation, VisionEncoderDecoderModel,\n",
        "    AutoModelForMaskedLM\n",
        ")\n",
        "import torch.nn as nn\n",
        "\n",
        "# Task Classifier\n",
        "task_classifier = TaskClassifier(num_tasks=5)\n",
        "# Load pre-trained or trained weights\n",
        "task_classifier.load_state_dict(torch.load(\"task_classifier.pth\"))  # Replace with your saved weights path\n",
        "task_classifier.eval()  # Ensure the model is in evaluation mode\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Experts for each task\n",
        "experts = {\n",
        "    \"Summarization\": AutoModelForSeq2SeqLM.from_pretrained(\"dhivyeshrk/bart-large-cnn-samsum\").to(device),  # Fine-tuned BART\n",
        "    \"Sentiment Analysis\": AutoModelForSequenceClassification.from_pretrained(\"ckandrew04/bert-base-cased-fine-tuned-sst2\").to(device),  # Fine-tuned BERT\n",
        "    \"Image Classification\": ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\").to(device),  # Vision Transformer\n",
        "    \"Image Segmentation\": SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\").to(device),  # SegFormer model\n",
        "    \"Image Captioning\": VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(device)  # ViT + GPT-2 for captioning\n",
        "}\n",
        "\n",
        "# Generalist fallback model\n",
        "generalist = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "# Generalist pre-trained multi-modal model, this must be replaced by a multi-modal generalist model, such as GPT-4o.\n",
        "# Currently implemented as bert-base-cased for simplification. bert-base-cased is NOT a multimodal model.\n",
        "\n",
        "# Mixture of Experts instantiation\n",
        "moe = MixtureOfExperts(\n",
        "    task_classifier=task_classifier,\n",
        "    experts=experts,\n",
        "    generalist=generalist,\n",
        "    device=device\n",
        ")\n"
      ],
      "metadata": {
        "id": "iVpkc5pyP7u6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "915e7c9e-f5c7-4377-b2e2-bc8cf81d7196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-88-f1d8cbc11a83>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  task_classifier.load_state_dict(torch.load(\"task_classifier.pth\"))  # Replace with your saved weights path\n",
            "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
            "  \"architectures\": [\n",
            "    \"ViTModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.46.3\"\n",
            "}\n",
            "\n",
            "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"add_cross_attention\": true,\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_inputs = {\n",
        "    \"Summarization\": {\n",
        "        \"prompt\": \"Provide a brief summary of the dialogue.\",\n",
        "        \"input\": \"Hannah: Hey, do you have Betty's number? Amanda: Lemme check Hannah: <file_gif> Amanda: Sorry, can't find it. Amanda: Ask Larry Amanda: He called her last time we were at the park together Hannah: I don't know him well Hannah: <file_gif> Amanda: Don't be shy, he's very nice Hannah: If you say so.. Hannah: I'd rather you texted him Amanda: Just text him 🙂 Hannah: Urgh.. Alright Hannah: Bye Amanda: Bye bye\"\n",
        "    },\n",
        "    \"Sentiment Analysis\": {\n",
        "        \"prompt\": \"Analyze the sentiment of the following review:\",\n",
        "        \"input\": \"it takes a strange kind of laziness to waste the talents of robert forster , anne meara , eugene levy , and reginald veljohnson all in the same movie .\"\n",
        "    },\n",
        "    \"Image Classification\": {\n",
        "        \"prompt\": \"Classify the following image:\",\n",
        "        \"input\": torch.rand(1, 3, 224, 224)  # Random image tensor simulating an RGB image\n",
        "    },\n",
        "    \"Image Segmentation\": {\n",
        "        \"prompt\": \"Segment the objects in the following image:\",\n",
        "        \"input\": torch.rand(1, 3, 224, 224)  # Random image tensor simulating an RGB image\n",
        "    },\n",
        "    \"Image Captioning\": {\n",
        "        \"prompt\": \"Generate a caption for the following image:\",\n",
        "        \"input\": torch.rand(1, 3, 224, 224)  # Random image tensor simulating an RGB image\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "k9HUaC2FmMiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with updated logic\n",
        "for task_name, data in sample_inputs.items():\n",
        "    print(f\"\\nTesting for Task: {task_name}\")\n",
        "\n",
        "    try:\n",
        "        prompt = data[\"prompt\"]\n",
        "        actual_input = data[\"input\"]\n",
        "\n",
        "        # Prepare the inputs\n",
        "        inputs = {\"prompt\": prompt, \"input\": actual_input}\n",
        "\n",
        "        # Forward pass through MixtureOfExperts\n",
        "        output = moe(inputs)\n",
        "        print(f\"Output for {task_name}: {output}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during {task_name} testing: {e}\")\n"
      ],
      "metadata": {
        "id": "dr50RZ7WQv7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c94a5d05-4d54-4ee4-e3bb-da853b35e3c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing for Task: Summarization\n",
            "Inputs received: {'prompt': 'Provide a brief summary of the dialogue.', 'input': \"Hannah: Hey, do you have Betty's number? Amanda: Lemme check Hannah: <file_gif> Amanda: Sorry, can't find it. Amanda: Ask Larry Amanda: He called her last time we were at the park together Hannah: I don't know him well Hannah: <file_gif> Amanda: Don't be shy, he's very nice Hannah: If you say so.. Hannah: I'd rather you texted him Amanda: Just text him 🙂 Hannah: Urgh.. Alright Hannah: Bye Amanda: Bye bye\"}\n",
            "Type of inputs: <class 'dict'>\n",
            "Routing to expert for task: Summarization\n",
            "Output for Summarization: Hannah is looking for Betty's number. Amanda suggests Hannah to ask Larry if he called Betty last time they were at the park together. Hannah doesn't know him well, but he's very nice. She should text him.   Hannah and Amanda agree to do that.\n",
            "\n",
            "Testing for Task: Sentiment Analysis\n",
            "Inputs received: {'prompt': 'Analyze the sentiment of the following review:', 'input': 'it takes a strange kind of laziness to waste the talents of robert forster , anne meara , eugene levy , and reginald veljohnson all in the same movie .'}\n",
            "Type of inputs: <class 'dict'>\n",
            "Routing to expert for task: Sentiment Analysis\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5442, -4.8372]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "0\n",
            "Output for Sentiment Analysis: Negative\n",
            "\n",
            "Testing for Task: Image Classification\n",
            "Inputs received: {'prompt': 'Classify the following image:', 'input': tensor([[[[0.1623, 0.9982, 0.2652,  ..., 0.7831, 0.9142, 0.4631],\n",
            "          [0.6497, 0.1836, 0.7666,  ..., 0.4070, 0.2778, 0.8757],\n",
            "          [0.0542, 0.9708, 0.9226,  ..., 0.5168, 0.8672, 0.3295],\n",
            "          ...,\n",
            "          [0.2928, 0.5923, 0.7429,  ..., 0.9105, 0.9854, 0.5447],\n",
            "          [0.6420, 0.9244, 0.8735,  ..., 0.8366, 0.4138, 0.0133],\n",
            "          [0.1302, 0.9762, 0.5012,  ..., 0.9834, 0.7570, 0.0288]],\n",
            "\n",
            "         [[0.5087, 0.8974, 0.2286,  ..., 0.7402, 0.8685, 0.5344],\n",
            "          [0.2756, 0.2104, 0.0553,  ..., 0.1006, 0.9126, 0.2996],\n",
            "          [0.6187, 0.7729, 0.9266,  ..., 0.1316, 0.6543, 0.7376],\n",
            "          ...,\n",
            "          [0.5357, 0.2121, 0.3186,  ..., 0.1509, 0.1221, 0.7084],\n",
            "          [0.7801, 0.5497, 0.0088,  ..., 0.9955, 0.8874, 0.4010],\n",
            "          [0.8222, 0.2452, 0.0236,  ..., 0.4592, 0.7599, 0.1403]],\n",
            "\n",
            "         [[0.0223, 0.1000, 0.8937,  ..., 0.2311, 0.3841, 0.9967],\n",
            "          [0.8865, 0.5698, 0.5427,  ..., 0.0179, 0.9439, 0.8374],\n",
            "          [0.8006, 0.3152, 0.5108,  ..., 0.8459, 0.3888, 0.3252],\n",
            "          ...,\n",
            "          [0.3657, 0.9499, 0.0218,  ..., 0.1560, 0.0152, 0.0015],\n",
            "          [0.6815, 0.7089, 0.6791,  ..., 0.0631, 0.5054, 0.3458],\n",
            "          [0.9964, 0.5875, 0.9444,  ..., 0.8877, 0.5084, 0.4673]]]])}\n",
            "Type of inputs: <class 'dict'>\n",
            "Routing to expert for task: Image Classification\n",
            "Output for Image Classification: kite\n",
            "\n",
            "Testing for Task: Image Segmentation\n",
            "Inputs received: {'prompt': 'Segment the objects in the following image:', 'input': tensor([[[[0.3545, 0.0070, 0.9114,  ..., 0.5342, 0.3500, 0.4836],\n",
            "          [0.9299, 0.8285, 0.0568,  ..., 0.0482, 0.8858, 0.6604],\n",
            "          [0.6621, 0.4282, 0.6037,  ..., 0.1626, 0.1659, 0.2096],\n",
            "          ...,\n",
            "          [0.7092, 0.1272, 0.8966,  ..., 0.2566, 0.2427, 0.1698],\n",
            "          [0.2101, 0.6767, 0.7777,  ..., 0.4310, 0.0729, 0.5309],\n",
            "          [0.1115, 0.5490, 0.6302,  ..., 0.2923, 0.6724, 0.8966]],\n",
            "\n",
            "         [[0.0938, 0.7423, 0.0280,  ..., 0.9102, 0.0814, 0.5396],\n",
            "          [0.5041, 0.6814, 0.1281,  ..., 0.8057, 0.8532, 0.8218],\n",
            "          [0.2386, 0.6231, 0.0892,  ..., 0.6181, 0.1995, 0.7334],\n",
            "          ...,\n",
            "          [0.9308, 0.2861, 0.7102,  ..., 0.3953, 0.4983, 0.5916],\n",
            "          [0.6263, 0.8916, 0.7711,  ..., 0.9738, 0.9552, 0.5260],\n",
            "          [0.2265, 0.0929, 0.8376,  ..., 0.0442, 0.5697, 0.0528]],\n",
            "\n",
            "         [[0.6775, 0.2043, 0.3799,  ..., 0.3884, 0.0830, 0.2491],\n",
            "          [0.0089, 0.7819, 0.3660,  ..., 0.2368, 0.4439, 0.1856],\n",
            "          [0.7493, 0.5896, 0.0306,  ..., 0.6528, 0.6490, 0.1319],\n",
            "          ...,\n",
            "          [0.7313, 0.0448, 0.6104,  ..., 0.7030, 0.2302, 0.4437],\n",
            "          [0.7674, 0.7163, 0.9198,  ..., 0.6331, 0.9917, 0.6562],\n",
            "          [0.4513, 0.9128, 0.1063,  ..., 0.0084, 0.1289, 0.9979]]]])}\n",
            "Type of inputs: <class 'dict'>\n",
            "Routing to expert for task: Image Segmentation\n",
            "Output for Image Segmentation: tensor([[[2, 2, 2,  ..., 2, 2, 2],\n",
            "         [2, 2, 2,  ..., 3, 2, 2],\n",
            "         [2, 2, 2,  ..., 2, 3, 2],\n",
            "         ...,\n",
            "         [2, 2, 2,  ..., 0, 0, 0],\n",
            "         [2, 2, 2,  ..., 0, 0, 0],\n",
            "         [2, 2, 2,  ..., 0, 0, 0]]], device='cuda:0')\n",
            "\n",
            "Testing for Task: Image Captioning\n",
            "Inputs received: {'prompt': 'Generate a caption for the following image:', 'input': tensor([[[[0.6968, 0.4034, 0.7650,  ..., 0.9688, 0.5397, 0.0190],\n",
            "          [0.7419, 0.9898, 0.5638,  ..., 0.5031, 0.9682, 0.7405],\n",
            "          [0.3767, 0.3363, 0.7342,  ..., 0.3051, 0.1441, 0.2663],\n",
            "          ...,\n",
            "          [0.2113, 0.9513, 0.8031,  ..., 0.6427, 0.8884, 0.1329],\n",
            "          [0.0875, 0.8302, 0.8037,  ..., 0.3331, 0.5521, 0.4900],\n",
            "          [0.4279, 0.2623, 0.0227,  ..., 0.0697, 0.3616, 0.5697]],\n",
            "\n",
            "         [[0.1981, 0.6789, 0.9515,  ..., 0.2735, 0.3269, 0.3603],\n",
            "          [0.8176, 0.3571, 0.9143,  ..., 0.5693, 0.5260, 0.9972],\n",
            "          [0.0823, 0.5110, 0.8405,  ..., 0.1349, 0.8336, 0.8281],\n",
            "          ...,\n",
            "          [0.4502, 0.8310, 0.2522,  ..., 0.2931, 0.4619, 0.8229],\n",
            "          [0.6767, 0.8657, 0.4768,  ..., 0.4564, 0.9114, 0.7873],\n",
            "          [0.6936, 0.3761, 0.7538,  ..., 0.0726, 0.7371, 0.1993]],\n",
            "\n",
            "         [[0.8397, 0.2635, 0.1971,  ..., 0.5712, 0.5005, 0.9883],\n",
            "          [0.4475, 0.3166, 0.1849,  ..., 0.2558, 0.6774, 0.8369],\n",
            "          [0.8790, 0.6262, 0.3958,  ..., 0.3325, 0.1351, 0.0818],\n",
            "          ...,\n",
            "          [0.5589, 0.6781, 0.2842,  ..., 0.2641, 0.8955, 0.5729],\n",
            "          [0.9675, 0.4832, 0.6869,  ..., 0.0632, 0.4785, 0.3724],\n",
            "          [0.9681, 0.6557, 0.1152,  ..., 0.8870, 0.5091, 0.2232]]]])}\n",
            "Type of inputs: <class 'dict'>\n",
            "Routing to expert for task: Image Captioning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output for Image Captioning: a blurry photo of a single bird flying through the air \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkHm349Bj98q"
      },
      "source": [
        "# 6. Combine Components\n",
        "\n",
        "The final model integrates the MoM backbone, Task Classifier, and MoE module. The MoM separate inputs from different modalities, using only what is needed, the Task Classifier predicts the task, and the MoE generates task-specific outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPcHkyB3j_-1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MoMMoE(nn.Module):\n",
        "    def __init__(self, mom, moe):\n",
        "        super(MoMMoE, self).__init__()\n",
        "        self.mom = mom  # Mixture of Modalities\n",
        "        self.moe = moe  # Mixture of Experts\n",
        "\n",
        "    def forward(self, raw_inputs, task_type=None):\n",
        "        \"\"\"\n",
        "        Forward pass for the integrated model.\n",
        "        :param raw_inputs: Raw dictionary of text or image input.\n",
        "        :param task_type: Optional manual override for task type.\n",
        "        :return: Model output from the Mixture of Experts.\n",
        "        \"\"\"\n",
        "        # Process inputs through MoM\n",
        "        processed_inputs = self.mom(raw_inputs)\n",
        "\n",
        "        # Pass processed inputs to MoE\n",
        "        output = self.moe(processed_inputs, task_type=task_type)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_inputs = {\n",
        "    \"Summarization\": {\n",
        "        \"text\": \"Provide a brief summary of the dialogue. Hannah: Hey, do you have Betty's number? Amanda: Lemme check Hannah: <file_gif> Amanda: Sorry, can't find it. Amanda: Ask Larry Amanda: He called her last time we were at the park together Hannah: I don't know him well Hannah: <file_gif> Amanda: Don't be shy, he's very nice Hannah: If you say so.. Hannah: I'd rather you texted him Amanda: Just text him 🙂 Hannah: Urgh.. Alright Hannah: Bye Amanda: Bye bye\"\n",
        "    },\n",
        "    \"Sentiment Analysis\": {\n",
        "        \"text\": \"Analyze the sentiment of the following review. it takes a strange kind of laziness to waste the talents of robert forster , anne meara , eugene levy , and reginald veljohnson all in the same movie .\"\n",
        "    },\n",
        "    \"Image Classification\": {\n",
        "        \"text\": \"Classify the following image.\",\n",
        "        \"image\": torch.rand(1, 3, 224, 224)  # Random image tensor simulating an RGB image\n",
        "    },\n",
        "    \"Image Segmentation\": {\n",
        "        \"text\": \"Segment the objects in the following image.\",\n",
        "        \"image\": torch.rand(1, 3, 224, 224)  # Random image tensor simulating an RGB image\n",
        "    },\n",
        "    \"Image Captioning\": {\n",
        "        \"text\": \"Generate a caption for the following image.\",\n",
        "        \"image\": torch.rand(1, 3, 224, 224)  # Random image tensor simulating an RGB image\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "euJbWxb0ugZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MoM and MoE\n",
        "mom = MixtureOfModalities()\n",
        "moe = MixtureOfExperts(\n",
        "    task_classifier=task_classifier,\n",
        "    experts=experts,\n",
        "    generalist=generalist,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Initialize FinalModel\n",
        "mommoe = MoMMoE(mom, moe)\n",
        "\n"
      ],
      "metadata": {
        "id": "gx4GMxyMWeuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with sample inputs\n",
        "for task_name, raw_input in sample_inputs.items():\n",
        "    print(f\"\\nTesting for Task: {task_name}\")\n",
        "    try:\n",
        "        output = mommoe(raw_inputs=raw_input)\n",
        "        print(f\"Output for {task_name}: {output}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during {task_name} testing: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnPiTMkhb-A8",
        "outputId": "765667cb-4b50-42e1-cfd8-441bbdf03d3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing for Task: Summarization\n",
            "Inputs received: {'prompt': 'Provide a brief summary of the dialogue.', 'input': \"Hannah: Hey, do you have Betty's number? Amanda: Lemme check Hannah: <file_gif> Amanda: Sorry, can't find it. Amanda: Ask Larry Amanda: He called her last time we were at the park together Hannah: I don't know him well Hannah: <file_gif> Amanda: Don't be shy, he's very nice Hannah: If you say so.. Hannah: I'd rather you texted him Amanda: Just text him 🙂 Hannah: Urgh.. Alright Hannah: Bye Amanda: Bye bye\"}\n",
            "Type of inputs: <class 'dict'>\n",
            "Routing to expert for task: Summarization\n",
            "Output for Summarization: Hannah is looking for Betty's number. Amanda suggests Hannah to ask Larry if he called Betty last time they were at the park together. Hannah doesn't know him well, but he's very nice. She should text him.   Hannah and Amanda agree to do that.\n",
            "\n",
            "Testing for Task: Sentiment Analysis\n",
            "Inputs received: {'prompt': 'Analyze the sentiment of the following review.', 'input': 'it takes a strange kind of laziness to waste the talents of robert forster , anne meara , eugene levy , and reginald veljohnson all in the same movie .'}\n",
            "Type of inputs: <class 'dict'>\n",
            "Routing to expert for task: Sentiment Analysis\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5442, -4.8372]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "0\n",
            "Output for Sentiment Analysis: Negative\n",
            "\n",
            "Testing for Task: Image Classification\n",
            "Inputs received: {'prompt': 'Classify the following image.', 'input': tensor([[[[0.8770, 0.0871, 0.1171,  ..., 0.8182, 0.5611, 0.1864],\n",
            "          [0.0296, 0.2350, 0.8645,  ..., 0.5670, 0.9733, 0.4143],\n",
            "          [0.7163, 0.3597, 0.9640,  ..., 0.9610, 0.8489, 0.7027],\n",
            "          ...,\n",
            "          [0.8908, 0.1670, 0.4127,  ..., 0.4062, 0.7259, 0.5802],\n",
            "          [0.4424, 0.0477, 0.1239,  ..., 0.6339, 0.2414, 0.8047],\n",
            "          [0.0133, 0.3751, 0.1852,  ..., 0.2875, 0.3594, 0.8080]],\n",
            "\n",
            "         [[0.3085, 0.8194, 0.0232,  ..., 0.0773, 0.7341, 0.9460],\n",
            "          [0.4417, 0.7469, 0.3153,  ..., 0.1307, 0.2023, 0.3367],\n",
            "          [0.3584, 0.4056, 0.7988,  ..., 0.2558, 0.7738, 0.3326],\n",
            "          ...,\n",
            "          [0.8580, 0.7626, 0.3955,  ..., 0.8379, 0.9228, 0.9593],\n",
            "          [0.5573, 0.7281, 0.1211,  ..., 0.1716, 0.8519, 0.0311],\n",
            "          [0.9437, 0.0439, 0.4441,  ..., 0.3861, 0.5383, 0.9830]],\n",
            "\n",
            "         [[0.3550, 0.9625, 0.9109,  ..., 0.7774, 0.3731, 0.2385],\n",
            "          [0.6113, 0.4035, 0.3848,  ..., 0.7064, 0.1874, 0.6229],\n",
            "          [0.2282, 0.7210, 0.7727,  ..., 0.6752, 0.2771, 0.9392],\n",
            "          ...,\n",
            "          [0.1219, 0.7922, 0.0825,  ..., 0.4260, 0.2763, 0.8394],\n",
            "          [0.0882, 0.2046, 0.3965,  ..., 0.7986, 0.1173, 0.1019],\n",
            "          [0.3789, 0.8505, 0.3523,  ..., 0.7467, 0.6944, 0.8983]]]])}\n",
            "Type of inputs: <class 'dict'>\n",
            "Routing to expert for task: Image Classification\n",
            "Output for Image Classification: kite\n",
            "\n",
            "Testing for Task: Image Segmentation\n",
            "Inputs received: {'prompt': 'Segment the objects in the following image.', 'input': tensor([[[[0.4653, 0.6239, 0.3162,  ..., 0.9801, 0.1342, 0.4407],\n",
            "          [0.6881, 0.6631, 0.4907,  ..., 0.4609, 0.3788, 0.6613],\n",
            "          [0.0281, 0.6185, 0.3766,  ..., 0.3912, 0.7986, 0.2099],\n",
            "          ...,\n",
            "          [0.3130, 0.3567, 0.8382,  ..., 0.3094, 0.9447, 0.0941],\n",
            "          [0.5519, 0.1283, 0.5287,  ..., 0.2968, 0.2039, 0.9705],\n",
            "          [0.7869, 0.5730, 0.4725,  ..., 0.2812, 0.6651, 0.7942]],\n",
            "\n",
            "         [[0.4690, 0.7485, 0.9043,  ..., 0.9067, 0.2207, 0.9338],\n",
            "          [0.4467, 0.0590, 0.1699,  ..., 0.9441, 0.1978, 0.1039],\n",
            "          [0.4479, 0.9287, 0.6859,  ..., 0.8393, 0.1890, 0.3164],\n",
            "          ...,\n",
            "          [0.1734, 0.5768, 0.3718,  ..., 0.7996, 0.1574, 0.3042],\n",
            "          [0.4486, 0.1763, 0.3894,  ..., 0.7532, 0.1859, 0.3458],\n",
            "          [0.9065, 0.6343, 0.1050,  ..., 0.7983, 0.3734, 0.5142]],\n",
            "\n",
            "         [[0.3258, 0.7923, 0.9633,  ..., 0.4556, 0.2836, 0.1387],\n",
            "          [0.0978, 0.1185, 0.4760,  ..., 0.1925, 0.0485, 0.5016],\n",
            "          [0.8369, 0.1844, 0.0628,  ..., 0.3176, 0.8670, 0.3198],\n",
            "          ...,\n",
            "          [0.4816, 0.5316, 0.4627,  ..., 0.8766, 0.5065, 0.9671],\n",
            "          [0.7837, 0.1977, 0.2562,  ..., 0.3087, 0.5916, 0.0800],\n",
            "          [0.0300, 0.0263, 0.4587,  ..., 0.4020, 0.0054, 0.1469]]]])}\n",
            "Type of inputs: <class 'dict'>\n",
            "Routing to expert for task: Image Segmentation\n",
            "Output for Image Segmentation: tensor([[[13, 13, 13,  ..., 13, 13, 13],\n",
            "         [13, 13, 13,  ...,  2, 13, 13],\n",
            "         [13, 13, 13,  ..., 13, 13, 13],\n",
            "         ...,\n",
            "         [ 2,  2,  2,  ...,  0,  0,  0],\n",
            "         [ 2,  2,  2,  ...,  0,  0,  0],\n",
            "         [ 2,  2,  2,  ...,  0,  0,  0]]], device='cuda:0')\n",
            "\n",
            "Testing for Task: Image Captioning\n",
            "Inputs received: {'prompt': 'Generate a caption for the following image.', 'input': tensor([[[[0.1491, 0.1106, 0.0826,  ..., 0.2899, 0.8157, 0.0949],\n",
            "          [0.1280, 0.0075, 0.5285,  ..., 0.6565, 0.4489, 0.3627],\n",
            "          [0.4839, 0.6271, 0.5078,  ..., 0.8532, 0.5813, 0.1361],\n",
            "          ...,\n",
            "          [0.3011, 0.5908, 0.2343,  ..., 0.0152, 0.3995, 0.3244],\n",
            "          [0.7128, 0.9249, 0.2677,  ..., 0.4659, 0.9307, 0.6501],\n",
            "          [0.5213, 0.1765, 0.1549,  ..., 0.7611, 0.1031, 0.4397]],\n",
            "\n",
            "         [[0.5467, 0.6424, 0.3128,  ..., 0.8633, 0.7327, 0.5749],\n",
            "          [0.7952, 0.5505, 0.6537,  ..., 0.0906, 0.0396, 0.9684],\n",
            "          [0.2670, 0.1156, 0.0958,  ..., 0.6766, 0.1068, 0.0039],\n",
            "          ...,\n",
            "          [0.4485, 0.4417, 0.6361,  ..., 0.7362, 0.3892, 0.9006],\n",
            "          [0.9562, 0.6860, 0.7656,  ..., 0.6895, 0.5871, 0.3695],\n",
            "          [0.5491, 0.7975, 0.1077,  ..., 0.0727, 0.6843, 0.3137]],\n",
            "\n",
            "         [[0.2467, 0.8093, 0.4058,  ..., 0.6462, 0.1186, 0.2445],\n",
            "          [0.0169, 0.3584, 0.0762,  ..., 0.2284, 0.2259, 0.7712],\n",
            "          [0.8778, 0.6059, 0.5721,  ..., 0.6940, 0.3869, 0.3233],\n",
            "          ...,\n",
            "          [0.9882, 0.9391, 0.7115,  ..., 0.6640, 0.6838, 0.4816],\n",
            "          [0.7041, 0.5225, 0.5087,  ..., 0.2869, 0.9743, 0.1633],\n",
            "          [0.2652, 0.4345, 0.8288,  ..., 0.6195, 0.1836, 0.4093]]]])}\n",
            "Type of inputs: <class 'dict'>\n",
            "Routing to expert for task: Image Captioning\n",
            "Output for Image Captioning: a blurry photo of a small group of birds \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9e0aa3023094a64995a1d107aa8d836": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2955ea047084bee992fffdcc33ab8fb",
              "IPY_MODEL_e1a2debca94447c5b641c60e52b1d526",
              "IPY_MODEL_6603c948055344ceb82379dd8f5b1934"
            ],
            "layout": "IPY_MODEL_383ae6dcd5414609a794fb2b324292a0"
          }
        },
        "f2955ea047084bee992fffdcc33ab8fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2ee39b9e39741619abce0cdd65b9e35",
            "placeholder": "​",
            "style": "IPY_MODEL_2bff9d641adc48f693d8be9aaf95898e",
            "value": "Map: 100%"
          }
        },
        "e1a2debca94447c5b641c60e52b1d526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baf917ac8c754778a3033dab92066afc",
            "max": 872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9ffa0004a9144c6a71bf0ad6074ebed",
            "value": 872
          }
        },
        "6603c948055344ceb82379dd8f5b1934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e53fe5a08bfd47aa8ca1fc750d793bc8",
            "placeholder": "​",
            "style": "IPY_MODEL_7331c5cdc10d41d093de49c517a9c1e3",
            "value": " 872/872 [00:00&lt;00:00, 1728.36 examples/s]"
          }
        },
        "383ae6dcd5414609a794fb2b324292a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2ee39b9e39741619abce0cdd65b9e35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bff9d641adc48f693d8be9aaf95898e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "baf917ac8c754778a3033dab92066afc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9ffa0004a9144c6a71bf0ad6074ebed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e53fe5a08bfd47aa8ca1fc750d793bc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7331c5cdc10d41d093de49c517a9c1e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7fb23c993a9453584b56f5c690bddaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cba20794471b4d5c83e75b947a8d5289",
              "IPY_MODEL_41d66426fb65492681d932573aacfca4",
              "IPY_MODEL_946986c6d0b44af993151987a77ba127",
              "IPY_MODEL_a8faf2cc5b504364abf01fee55fb9434",
              "IPY_MODEL_e52c7897c0d44abbb5dd7cbeb6e78d43"
            ],
            "layout": "IPY_MODEL_c5233e9db73b427492f557a13e094e63"
          }
        },
        "cba20794471b4d5c83e75b947a8d5289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87a25ac4521446d4bab023f683d9cb98",
            "placeholder": "​",
            "style": "IPY_MODEL_f39a3f9b9f3845189c945d35b53c9587",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "41d66426fb65492681d932573aacfca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_3f83748102b6427a808429b668201933",
            "placeholder": "​",
            "style": "IPY_MODEL_6b3a6ffdd5d044dc996e5b7cc4c3973f",
            "value": ""
          }
        },
        "946986c6d0b44af993151987a77ba127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_24ea97f25b974d92b331012aa0e300cb",
            "style": "IPY_MODEL_f07e410450414859a57200c1a716f600",
            "value": true
          }
        },
        "a8faf2cc5b504364abf01fee55fb9434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_3f3eca17de2e4c7ca9275ac228936d05",
            "style": "IPY_MODEL_a633f43508624d1293dd759ec957b1ac",
            "tooltip": ""
          }
        },
        "e52c7897c0d44abbb5dd7cbeb6e78d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eb139b1630e40fabda1f041ad92faa6",
            "placeholder": "​",
            "style": "IPY_MODEL_c55f5e2c891f4f0ab244deea93e46f8a",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "c5233e9db73b427492f557a13e094e63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "87a25ac4521446d4bab023f683d9cb98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f39a3f9b9f3845189c945d35b53c9587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f83748102b6427a808429b668201933": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b3a6ffdd5d044dc996e5b7cc4c3973f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24ea97f25b974d92b331012aa0e300cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f07e410450414859a57200c1a716f600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f3eca17de2e4c7ca9275ac228936d05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a633f43508624d1293dd759ec957b1ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3eb139b1630e40fabda1f041ad92faa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c55f5e2c891f4f0ab244deea93e46f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}